---
title: Описание кластера для диспетчера кластерных ресурсов | Документация Майкрософт
description: Описание кластера Service Fabric посредством указания доменов сбоя, доменов обновления, свойств узлов и емкости узлов для диспетчера кластерных ресурсов.
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: ''
ms.assetid: 55f8ab37-9399-4c9a-9e6c-d2d859de6766
ms.service: Service-Fabric
ms.devlang: dotnet
ms.topic: conceptual
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 13ee238580d645f3e727090bc0e0275b36bdb225
ms.sourcegitcommit: eb75f177fc59d90b1b667afcfe64ac51936e2638
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/16/2018
ms.locfileid: "34208817"
---
# <a name="describing-a-service-fabric-cluster"></a>Описание кластера Service Fabric
Диспетчер Resource Manager кластера Service Fabric предоставляет несколько механизмов для описания кластера. Во время выполнения диспетчер кластерных ресурсов использует эти сведения, чтобы обеспечить высокую доступность служб, функционирующих в кластере. При применении этих важных правил он также пытается оптимизировать потребление ресурсов в кластере.

## <a name="key-concepts"></a>Основные понятия
Диспетчер кластерных ресурсов поддерживает некоторые функции, описывающие кластер:

* Домены сбоя
* Домены обновления
* Свойства узла
* Емкость узла

## <a name="fault-domains"></a>Домены сбоя
Домен сбоя — это любая область координированного сбоя. В качестве домена сбоя используется отдельный компьютер (так как его работа может остановиться по многим причинам — от сбоев питания и сбоев дисков до проблем встроенного ПО сетевой карты). Компьютеры, подключенные к одному коммутатору Ethernet, находятся в одном домене сбоя, как если бы они использовали один источник питания или находились в одном расположении. Так как сбои оборудования естественным образом пересекаются, домены сбоя по своей сути являются иерархическими и представлены в Service Fabric в виде кодов URI.

Важно, чтобы домены сбоя были настроены правильно, так как Service Fabric использует эти сведения для безопасного размещения служб. Service Fabric размещает службы таким образом, чтобы выход из строя домена сбоя (из-за сбоя отдельного компонента) не приводил к сбою служб. В среде Azure Service Fabric использует данные домена сбоя, предоставляемые средой, для правильной настройки узлов в кластере от вашего имени. Для автономной системы Service Fabric домены сбоя определяются во время настройки кластера. 

> [!WARNING]
> Важно, чтобы сведения о домене сбоя, предоставленные системе Service Fabric, были точными. Например, предположим, что узлы кластера Service Fabric выполняются на 10 виртуальных машинах, работающих на пяти физических узлах. В этом случае, даже если используется 10 виртуальных машин, имеется только 5 разных доменов сбоя (верхнего уровня). Из-за того, что виртуальные машины совместно используют один физический узел, они используют один корневой домен сбоя. И сбой их физического узла приводит к согласованному сбою этих виртуальных машин.  
>
> При этом Service Fabric ожидает, что домен сбоя узла не изменяется. Применение других механизмов обеспечения высокого уровня доступности виртуальных машин, таких как [высокодоступные виртуальные машины (HA-VM)](https://technet.microsoft.com/library/cc967323.aspx), может привести к конфликту с Service Fabric, так как они используют прозрачную миграцию виртуальных машин с одного узла на другой. Эти механизмы не перенастраивают и не уведомляют код, выполняемый на виртуальной машине. Следовательно, использующие их среды **не подходят** для выполнения кластеров Service Fabric. Единственной применяемой технологией обеспечения высокого уровня доступности должна быть система Service Fabric. Необходимости в таких механизмах, как динамическая миграция виртуальных машин, сети SAN или прочие, нет. При использовании совместно с Service Fabric эти механизмы _снижают_ доступность и надежность приложений, так как повышают сложность, добавляют централизованные источники сбоев и используют стратегии обеспечения надежности и доступности, которые конфликтуют с механизмами Service Fabric. 
>
>

На рисунке ниже выделены цветом все сущности, которые могут привести к появлению доменов сбоя, и приведен полный список этих доменов. В этом примере у нас есть центры обработки данных (DC), стойки (R) и колонки (B). Предположительно, если каждая колонка содержит несколько виртуальных машин, в иерархии домена сбоя может быть еще один уровень.

<center>
![Узлы, организованные через домены сбоя][Image1]
</center>

Во время выполнения диспетчер кластерных ресурсов Service Fabric анализирует домены сбоя в кластере и планирует структуру. Реплики с отслеживанием состояния или экземпляры без отслеживания состояния для определенной службы распределяются таким образом, чтобы они находились в разных доменах сбоя. Распределение службы между доменами сбоя гарантирует, что ее доступность не будет нарушена при выходе из строя домена сбоя на любом уровне иерархии.

Для диспетчера кластерных ресурсов Service Fabric не имеет значения количество уровней в иерархии домена сбоя. Однако он пытается сделать так, чтобы потеря какой-либо части иерархии не повлияла на службы, работающие в ней. 

Рекомендуется настраивать одинаковое количество узлов на каждом уровне глубины в иерархии доменов сбоя. Если "дерево" доменов сбоя в кластере остается несбалансированным, диспетчеру кластерных ресурсов сложнее определить, где лучше расположить службы. Несбалансированная структура доменов сбоя означает, что потеря одних доменов сильнее понизит уровень доступности служб, чем потеря других доменов. Поэтому диспетчер кластерных ресурсов разрывается между двумя целями: использовать компьютеры (размещая на них службы) в нагруженном домене и разместить службы в других доменах таким образом, чтобы отказ домена не вызывал проблем. 

Как выглядят несбалансированные домены? На схеме ниже показаны две различные структуры кластера. В первом примере узлы равномерно распределены между доменами сбоя. Во втором примере в одном домене сбоя намного больше узлов, чем в других доменах сбоя. 

<center>
![Две различные структуры кластеров][Image2]
</center>

В Azure выбор домена сбоя для узла осуществляется автоматически. Тем не менее, в зависимости от количества подготавливаемых узлов, в некоторых доменах сбоя по-прежнему может оказаться большее количество узлов, чем в других. Например, предположим, что в кластере есть пять доменов сбоя, но выполняется подготовка семи узлов определенного типа. В этом случае в первых двух доменах сбоя будет размещено больше узлов. Если продолжить развертывание дополнительных типов узлов, используя только несколько экземпляров, проблема усугубится. По этой причине рекомендуется распределять узлы таким образом, чтобы количество узлов каждого типа было кратно количеству доменов сбоя.

## <a name="upgrade-domains"></a>Домены обновления
Домены обновления — это еще одна функция, помогающая диспетчеру кластерных ресурсов Service Fabric понять структуру кластера. Домены обновления определяют наборы узлов, обновляемых одновременно. Домены обновления помогают диспетчеру кластерных ресурсов понимать и координировать операции управления, такие как обновление.

Домены обновления очень похожи на домены сбоя, но имеют ряд ключевых отличий. Домены сбоя определяются областями согласованных сбоев оборудования. С другой стороны, домены обновления определяются политикой. Вы можете решить, сколько требуется доменов обновления. Их количество не зависит от среды. Можно создать любое количество доменов обновления, как и узлов. Еще одно различие между доменами сбоя и доменами обновления заключается в том, что домены обновления не являются иерархическими. Вместо этого они больше напоминают простой тег. 

На схеме ниже показаны три домена обновления, чередующиеся с тремя доменами сбоя. На ней также представлено одно возможное размещение трех разных реплик службы с отслеживанием состояния, где каждая из них находится в разных доменах сбоя и обновления. Такое размещение гарантирует, что если в процессе обновления службы произойдет отказ домена сбоя, то у вас по-прежнему останется одна копия кода и данных.  

<center>
![Размещение с доменами сбоя и обновления][Image3]
</center>

Наличие множества доменов обновления имеет свои преимущества и недостатки. Одно из преимуществ — то, что каждый этап обновления становится более детализированным и поэтому затрагивает меньшее количество узлов или служб. В результате за определенный момент времени необходимо переместить меньшее количество служб, что обеспечивает меньшее число изменений в системе. Это повышает уровень надежности, так как любой сбой в процессе обновления затрагивает меньше служб. Большое количество доменов обновления позволяет снизить размер буфера на остальных узлах, необходимого для обработки последствий обновления. Например, при наличии пяти доменов обновления узлы в каждом из них обрабатывают примерно 20 % вашего трафика. Если в процессе обновления домен обновления необходимо отключить, эту нагрузку, как правило, необходимо перераспределить. Так как у вас есть еще четыре домена обновления, в каждом из них должно быть достаточно емкости для 5 % всего трафика. Дополнительные домены обновления означают, что на узлах в кластере требуется буфер меньшего размера. Например, предположим, что у вас имеется не 5, а 10 доменов обновления. В этом случае каждый домен обновления будет обрабатывать около 10 % всего трафика. При распространении обновления в кластере на каждом домене потребуется емкость всего лишь для 1,1 % всего трафика. Как правило, дополнительные домены обновления позволяют эффективнее использовать узлы, так как в этом случае требуется меньшая зарезервированная емкость. То же самое относится и к доменам сбоя.  

Недостаток наличия большого количества доменов обновления заключается в том, что обновление выполняется дольше. Service Fabric ожидает короткое время после завершения обновления домена обновления, а затем выполняет проверки, прежде чем начать обновление следующего. Эти задержки позволяют обнаружить проблемы, вызванные обновлением, прежде чем обновление будет продолжено. Возникающие при этом негативные последствия допустимы, так как если выбран этот вариант, то неверные изменения не влияют слишком сильно на слишком большую часть службы в определенный момент времени.

Слишком малое количество доменов обновления имеет множество отрицательных побочных эффектов: пока каждый отдельный домен обновления недоступен и обновляется, значительная часть общей емкости ресурсов недоступна. Например, при наличии только трех доменов обновления вы одновременно отключаете примерно треть общих ресурсов службы или кластера. Это нежелательно, так как в кластере должно быть достаточно ресурсов для обработки рабочей нагрузки. Наличие этого буфера означает, что во время обычной работы данные узлы будут менее загружены, чем они были бы в противном случае. Это увеличивает затраты на выполнение службы.

Фактических ограничений на общее количество доменов сбоя или обновления в среде либо же на то, как они перекрываются, не существует. С другой стороны, существует несколько общих шаблонов:

- соотношение 1:1 (каждый домен сбоя сопоставляется с доменом обновления);
- один домен обновления на узел (экземпляр физической или виртуальной ОС);
- модель с "чередованием", или "матричная" модель, в которой домены сбоя и домены обновления образуют матрицу с компьютерами, обычно выполняющимися по диагонали.

<center>
![Структуры доменов сбоя и обновления][Image4]
</center>

Ответа на вопрос о том, какая структура лучше, не существует, так как каждая из них имеет свои преимущества и недостатки. Например, модель "1 домен сбоя — 1 домен обновления" отличается простотой настройки. Более привычной является модель "1 домен обновления на узел". Во время обновления все узлы обновляются независимо друг от друга. Это аналогично тому, как раньше вручную обновлялись небольшие наборы компьютеров. 

Самая распространенная модель — это матрица доменов сбоя и обновления, в которой домены сбоя и домены обновления формируют таблицу и узлы располагаются по диагонали. Это модель, используемая по умолчанию в кластерах Service Fabric в Azure. Конфигурация кластеров со множеством узлов в конечном итоге принимает вид полностью заполненной матрицы, приведенной выше.

## <a name="fault-and-upgrade-domain-constraints-and-resulting-behavior"></a>Ограничения доменов сбоя и обновления и соответствующее поведение
### <a name="default-approach"></a>*Стандартный подход*
По умолчанию диспетчер кластерных ресурсов сбалансированно распределяет службы между доменами сбоя и обновления. Это моделируется как [ограничение](service-fabric-cluster-resource-manager-management-integration.md). В соответствии с ограничениями доменов сбоя и обновления для определенного раздела службы разница в количестве объектов службы (экземпляры службы без отслеживания состояния или реплики службы с отслеживанием состояния) между двумя доменами одного уровня иерархии никогда не должна составлять больше 1. Предположим, что это ограничение предоставляет гарантию "максимальной разницы". Ограничение домена сбоя и обновления предотвращает определенные перемещения или упорядочения, которые нарушают указанное выше правило. 

Давайте рассмотрим один пример. Предположим, что у нас есть кластер с шестью узлами (У), на котором настроено пять доменов сбоя (ДС) и пять доменов обновления (ДО).

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **ДО0** |У1 | | | | |
| **ДО1** |У6 |У2 | | | |
| **ДО2** | | |У3 | | |
| **ДО3** | | | |У4 | |
| **ДО4** | | | | |У5 |

*Конфигурация 1*

Теперь предположим, что мы создаем службу, для которой TargetReplicaSetSize (или InstanceCount для службы без отслеживания состояния) имеет значение 5. Реплики размещаются на узлах У1-У5. Узел У6 фактически никогда не используется, вне зависимости от количества создаваемых служб. Но почему? Давайте рассмотрим разницу между текущей структурой и тем, что произошло бы, если бы мы выбрали У6.

Ниже показана наша текущая структура и общее число реплик (Р) на домен сбоя и обновления.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** | |Р2 | | | |1 |
| **ДО2** | | |Р3 | | |1 |
| **ДО3** | | | |Р4 | |1 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

*Макет 1*


Эта структура сбалансирована в плане распределения узлов на домен сбоя и домен обновления, а также в плане количества реплик на каждый из этих доменов. На каждый домен приходится одинаковое количество узлов и реплик.

Теперь давайте посмотрим, что произошло бы, если бы вместо У2 мы использовали У6. Как бы тогда распределялись реплики?

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** |Р5 | | | | |1 |
| **ДО2** | | |Р2 | | |1 |
| **ДО3** | | | |Р3 | |1 |
| **ДО4** | | | | |Р4 |1 |
| **Всего ДС** |2 |0 |1 |1 |1 |- |

*Макет 2*


Эта структура нарушает определение гарантии "максимальной разницы" ограничения для доменов сбоя. На ДС0 приходится две реплики, а на ДС1 — ноль. То есть разница составляет 2, превышая значение максимальной разницы (1). Так как ограничение нарушено, диспетчер кластерных ресурсов не разрешит использовать такую расстановку. Аналогично, если бы мы выбрали узлы У2 и У6 (вместо У1 и У2), то получили бы следующее.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** | | | | | |0 |
| **ДО1** |Р5 |Р1 | | | |2 |
| **ДО2** | | |Р2 | | |1 |
| **ДО3** | | | |Р3 | |1 |
| **ДО4** | | | | |Р4 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

*Макет 3*


Эта структура сбалансирована с точки зрения доменов сбоя. Однако теперь она нарушает ограничение доменов обновления. Причина этого в том, что у ДО0 ноль реплик, а у ДО1 их две. Следовательно, эта структура также является недопустимой, и она не будет выбрана диспетчером кластерных ресурсов.

Такой подход к распределению реплик состояния или экземпляров без отслеживания состояния обеспечивает наилучшую отказоустойчивость из возможных. В случае, когда один домен выходит из строя, теряется минимальное количество реплик или экземпляров. 

С другой стороны, этот подход может не позволять кластеру использовать все ресурсы. Для некоторых конфигураций кластера нельзя использовать определенные узлы. Поэтому Service Fabric может не размещать ваши службы, в результате чего появятся предупреждающие сообщения. Ранее был рассмотрен пример узла кластера, который нельзя использовать (У6). Даже если вы добавите узлы в этот кластер (У7–У10), из-за ограничений домена сбоя и обновления реплики или экземпляры будут помещены только в У1–У5. 

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **ДО0** |У1 | | | |У10 |
| **ДО1** |У6 |У2 | | | |
| **ДО2** | |У7 |У3 | | |
| **ДО3** | | |У8 |У4 | |
| **ДО4** | | | |У9 |У5 |

*Конфигурация 2*


### <a name="alternative-approach"></a>*Альтернативный подход*

Диспетчер кластерных ресурсов поддерживает другую версию ограничения домена сбоя и обновления, которая позволяет размещения, гарантируя при этом минимальный уровень безопасности. Альтернативное ограничение домена сбоя и обновления можно сформулировать следующим образом: "Для данной секции службы распределение реплик по доменам должно гарантировать, что секция не пострадает от потери кворума". Предположим, что это ограничение предоставляет гарантию "сохранения кворума". 

> [!NOTE]
>Для службы с отслеживанием состояния мы определяем *потерю кворума* в ситуации, когда большинство реплик секций отключаются одновременно. Например, если значение TargetReplicaSetSize равно пяти, набор любых трех реплик представляет собой кворум. Аналогичным образом, если значение TargetReplicaSetSize равно 6, для создания кворума необходимо четыре реплики. В обоих случаях для нормального функционирования секции одновременно может быть отключено не более двух реплик. Для службы без отслеживания состояния не существует понятия *потеря кворума*, так как эти службы обычно функционируют, даже если одновременно отключится большинство экземпляров. Поэтому в остальной части статьи мы сосредоточимся на службах с отслеживанием состояния.
>

Вернемся к предыдущему примеру. При использовании версии ограничения "сохранение кворума" все три заданных макета будут допустимы. Это связано с тем, что даже если произойдет сбой ДС0 во втором макете или ДО1 — в третьем, кворум по-прежнему останется в секции (большинство реплик сохранится). С этой версией ограничения У6 можно было использовать практически всегда.

Подход "сохранения кворума" обеспечивает большую гибкость, чем подход "максимальной разницы", так как легче найти распределения реплик, которые допустимы практически для любой топологии кластера. Однако этот подход не может гарантировать лучшие характеристики отказоустойчивости, так как некоторые сбои хуже других. В худшем случае при сбое одного домена и одной дополнительной реплики большинство реплик может быть потеряно. Например, вместо трех сбоев, необходимых для потери кворума с пятью репликами или экземплярами, теперь вы можете потерять большинство в результате всего лишь двух сбоев. 

### <a name="adaptive-approach"></a>*Адаптивный подход*
Так как оба подхода имеют недостатки и преимущества, мы представляем адаптивный подход, который объединяет эти две стратегии.

> [!NOTE]
>Для Service Fabric версии 6.2 и выше он будет доступен по умолчанию. 
>
Адаптивный подход по умолчанию использует логику "максимальной разницы", а при необходимости переключается на логику "сохранения кворума". Диспетчер кластерных ресурсов автоматически определяет необходимую стратегию на основе настроек кластеров и служб. Для данной службы: *если TargetReplicaSetSize равномерно делится на количество доменов сбоя и количество доменов обновления **, а** количество узлов меньше или равно результату умножения количества доменов сбоя на количество доменов обновления, диспетчер кластерных ресурсов должен использовать для этой службы логику на основе кворума.* Имейте в виду, что диспетчер кластерных ресурсов будет использовать этот подход как для служб без отслеживания состояния, так и для служб с отслеживанием, несмотря на потерю кворума, не имеющую отношения к службам без отслеживания состояния.

Вернемся к предыдущему примеру и предположим, что теперь кластер содержит 8 узлов (кластер по-прежнему настроен с пятью доменами сбоя и пятью доменами обновления, а значение TargetReplicaSetSize службы, размещенной на этом кластере, остается тем же (5)). 

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **ДО0** |У1 | | | | |
| **ДО1** |У6 |У2 | | | |
| **ДО2** | |У7 |У3 | | |
| **ДО3** | | |У8 |У4 | |
| **ДО4** | | | | |У5 |

*Конфигурация 3*

Так как все необходимые условия соблюдены, при распространении службы диспетчер кластерных ресурсов будет использовать логику "на основе кворума". Это позволяет использовать У6–У8. Одно из возможных распределений службы в этом случае может выглядеть следующим образом:

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** |Р2 | | | | |1 |
| **ДО2** | |Р3 |Р4 | | |2 |
| **ДО3** | | | | | |0 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |2 |1 |1 |0 |1 |- |

*Макет 4*

Если значение TargetReplicaSetSize вашей службы сокращено до четырех (например), диспетчер кластерных ресурсов заметит это изменение и возобновит использование логики "максимальной разницы", так как значение TargetReplicaSetSize больше не делится на количество ДС и ДО. В результате некоторые реплики для распределения оставшихся четырех реплик будут перемещаться по узлам У1–У5, чтобы не нарушить значение "максимальной разницы" логики домена сбоя и обновления. 

Вернемся к четвертому макету и значению TargetReplicaSetSize, равному пяти. Если У1 будет удален из кластера, количество доменов обновления становится равным четырем. И снова диспетчер кластерных ресурсов начнет использовать логику "максимальной разницы", так как количество ДО больше не разделяет значение TargetReplicaSetSize службы поровну. В результате при повторном создании реплика Р1 должна попасть на У4, чтобы не нарушить ограничение домена сбоя и обновления.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Недоступно |Недоступно |Недоступно |Недоступно |Недоступно |Недоступно |
| **ДО1** |Р2 | | | | |1 |
| **ДО2** | |Р3 |Р4 | | |2 |
| **ДО3** | | | |Р1 | |1 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

*Макет 5*

## <a name="configuring-fault-and-upgrade-domains"></a>Настройка доменов сбоя и обновления
Определение доменов сбоя и обновления выполняется автоматически в размещенных в Azure развертываниях Service Fabric. Service Fabric просто извлекает и использует сведения о среде из Azure.

Если вы создаете собственный кластер (или хотите попробовать запустить определенную топологию в среде разработки), можно самостоятельно предоставить сведения о доменах сбоя и обновления. В этом примере мы определяем кластер локальной разработки из девяти узлов, охватывающий три центра обработки данных (каждый с тремя стойками). Этот кластер также имеет три домена обновления, чередующиеся с этими тремя центрами обработки данных. Пример конфигурации приведен ниже. 

ClusterManifest.xml

```xml
  <Infrastructure>
    <!-- IsScaleMin indicates that this cluster runs on one-box /one single server -->
    <WindowsServer IsScaleMin="true">
      <NodeList>
        <Node NodeName="Node01" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType01" FaultDomain="fd:/DC01/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node02" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType02" FaultDomain="fd:/DC01/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node03" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType03" FaultDomain="fd:/DC01/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node04" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType04" FaultDomain="fd:/DC02/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node05" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType05" FaultDomain="fd:/DC02/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node06" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType06" FaultDomain="fd:/DC02/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node07" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType07" FaultDomain="fd:/DC03/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node08" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType08" FaultDomain="fd:/DC03/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node09" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType09" FaultDomain="fd:/DC03/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
      </NodeList>
    </WindowsServer>
  </Infrastructure>
```

Использование ClusterConfig.json для автономных развертываний

```json
"nodes": [
  {
    "nodeName": "vm1",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm2",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm3",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD3"
  },
  {
    "nodeName": "vm4",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm5",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm6",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD3"
  },
  {
    "nodeName": "vm7",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm8",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm9",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD3"
  }
],
```

> [!NOTE]
> При определении кластеров с помощью Azure Resource Manager домены сбоя и домены обновления назначаются платформой Azure. Следовательно, определение типов узлов и масштабируемых наборов виртуальных машин в шаблоне Azure Resource Manager не содержит домены сбоя и домены обновления.
>

## <a name="node-properties-and-placement-constraints"></a>Свойства узлов и ограничения размещения
Иногда (фактически большую часть времени) необходимо обеспечить выполнение определенных рабочих нагрузок только на узлах определенных типов в кластере. Например, для одних рабочих нагрузок могут требоваться графические процессоры или накопители SSD, а для других — нет. Хорошим примером использования определенного оборудования в зависимости от рабочих нагрузок является практически каждая n-уровневая архитектура. Одни компьютеры используются в качестве внешнего интерфейса или части обслуживания API приложения, они предоставляются клиентам или доступны в Интернете. Другие компьютеры (часто с разными аппаратными ресурсами) обрабатывают рабочие нагрузки уровней вычислений и хранилища. Как правило, они _не_ предоставляются непосредственно клиентам или в Интернете. Service Fabric ожидает, что существуют случаи, когда определенные рабочие нагрузки должны выполняться на конкретных конфигурациях оборудования. Например: 

* существующее n-уровневое приложение быстро перемещено в среду Service Fabric;
* рабочая нагрузка должна выполняться на конкретном оборудовании для повышения производительности, масштабирования или изоляции;
* рабочая нагрузка должна быть изолирована от других рабочих нагрузок по соображениям политики или потребления ресурсов.

Для поддержки таких типов конфигурации Service Fabric имеет первоклассное понятие тегов, которые можно применить к узлам. Эти теги называются **свойствами узла**. **Ограничения размещения** представляют собой операторы, привязанные к отдельным службам, которые выбирают одно или несколько свойств узла. Ограничения размещения определяют, где должны запускаться службы. Набор ограничений можно расширить, то есть можно использовать любые пары "ключ — значение". 

<center>
![Разные рабочие нагрузки структуры кластера][Image5]
</center>

### <a name="built-in-node-properties"></a>Встроенные свойства узла
Service Fabric определяет некоторые свойства узла по умолчанию, которые могут использоваться автоматически, так что их не нужно задавать. В каждом узле определены свойства по умолчанию **NodeType** и **NodeName**. Например, ограничение на размещение можно записать в таком виде: `"(NodeType == NodeType03)"`. В целом мы пришли к выводу, что NodeType — одно из самых распространенных свойств, Это удобно, так как оно точно соответствует типу компьютера. Каждый тип компьютера соответствует типу рабочей нагрузки в традиционном n-уровневом приложении.

<center>
![Ограничения на размещение и свойства узлов][Image6]
</center>

## <a name="placement-constraint-and-node-property-syntax"></a>Синтаксис ограничений размещения и свойств узлов 
В свойстве узла могут быть указаны значения string, bool или signed long. Оператор в службе называется *ограничением* на размещение, так как этот оператор применяется там, где может выполняться служба в кластере. Ограничением может быть любой логический оператор, который работает с различными свойствами узла в кластере. Ниже приведены допустимые селекторы в этих логических операторах.

1) Условные проверки для создания определенных операторов:

| Инструкция | Синтаксис |
| --- |:---:|
| "равно" | "==" |
| "не равно" | "!=" |
| "больше" | ">" |
| "больше или равно" | ">=" |
| "меньше" | "<" |
| "меньше или равно" | "<=" |

2) Логические операторы для группирования и логических операций:

| Инструкция | Синтаксис |
| --- |:---:|
| "и" | "&&" |
| "или" | "&#124;&#124;" |
| "не" | "!" |
| "группа как отдельный оператор" | "()" |

Ниже приведено несколько примеров основных операторов ограничения.

  * `"Value >= 5"`
  * `"NodeColor != green"`
  * `"((OneProperty < 100) || ((AnotherProperty == false) && (OneProperty >= 100)))"`

Служба может быть размещена только на тех узлах, где оператор ограничения размещения принимает общее значение True. Узлы без определенного свойства не совпадают с какими-либо ограничениями на размещение, содержащими это свойство.

Предположим, что для этого типа узла были определены следующие свойства узла:

ClusterManifest.xml

```xml
    <NodeType Name="NodeType01">
      <PlacementProperties>
        <Property Name="HasSSD" Value="true"/>
        <Property Name="NodeColor" Value="green"/>
        <Property Name="SomeProperty" Value="5"/>
      </PlacementProperties>
    </NodeType>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json. 

> [!NOTE]
> Тип узла в шаблоне Azure Resource Manager обычно параметризуется. Он будет иметь вид "[parameters('vmNodeType1Name')]", а не "NodeType01".
>

```json
"nodeTypes": [
    {
        "name": "NodeType01",
        "placementProperties": {
            "HasSSD": "true",
            "NodeColor": "green",
            "SomeProperty": "5"
        },
    }
],
```

Вы можете создать *ограничения* на размещение службы следующим образом:

C#

```csharp
FabricClient fabricClient = new FabricClient();
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
serviceDescription.PlacementConstraints = "(HasSSD == true && SomeProperty >= 4)";
// add other required servicedescription fields
//...
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

PowerShell:

```posh
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceType -Stateful -MinReplicaSetSize 3 -TargetReplicaSetSize 3 -PartitionSchemeSingleton -PlacementConstraint "HasSSD == true && SomeProperty >= 4"
```

Если все узлы типа NodeType01 являются допустимыми, то этот тип узла можно также выбрать с помощью ограничения "(NodeType == NodeType01)".

Одной замечательной особенностью ограничений на размещение службы является то, что они могут обновляться динамически во время выполнения. Поэтому если необходимо, то можно перемещать службу в кластере, добавлять и удалять требования и т. д. Service Fabric обеспечивает работу и доступность службы даже в том случае, если вносятся такие типы изменений.

C#:

```csharp
StatefulServiceUpdateDescription updateDescription = new StatefulServiceUpdateDescription();
updateDescription.PlacementConstraints = "NodeType == NodeType01";
await fabricClient.ServiceManager.UpdateServiceAsync(new Uri("fabric:/app/service"), updateDescription);
```

PowerShell:

```posh
Update-ServiceFabricService -Stateful -ServiceName $serviceName -PlacementConstraints "NodeType == NodeType01"
```

Ограничения размещения задаются для каждого отдельного именованного экземпляра службы. Обновления всегда заменяют (перезаписывают) свойства, заданные ранее.

Определение кластера задает свойства узла. Чтобы изменить свойства узла, требуется обновить конфигурацию кластера. Для обновления свойств узла нужно перезапустить каждый затронутый узел, чтобы он сообщил о своих новых свойствах. Этими последовательными обновлениям управляет Service Fabric.

## <a name="describing-and-managing-cluster-resources"></a>Описание кластерных ресурсов и управление ими
Одна из важнейших задач любого оркестратора — помощь в управлении потреблением ресурсов в кластере. Управление кластерными ресурсами связано с несколькими аспектами. Во-первых, необходимо гарантировать, что компьютеры не будут перегружены. То есть нужно сделать так, чтобы на компьютерах не было запущено больше служб, чем они могут обрабатывать. Во-вторых, требуются балансировка нагрузки и оптимизация, что очень важно для эффективного выполнения служб. Экономичные или чувствительные к производительности предложения служб не могут позволить, чтобы одни узлы использовались интенсивно, а другие — нет. Наличие узлов с высокой нагрузкой приводит к конфликту ресурсов и снижению производительности, а узлы с небольшой нагрузкой приводят к нецелесообразной растрате ресурсов или увеличению расходов. 

В Service Fabric ресурсы представлены в виде метрик (`Metrics`). Метрики — это любые логические или физические ресурсы, которые нужно описать в Service Fabric. Метриками, например, являются атрибуты WorkQueueDepth или MemoryInMb. Дополнительные сведения о физических ресурсах, которыми может управлять Service Fabric на узлах, см. в разделе [Управление потреблением ресурсов и нагрузкой в Service Fabric с помощью метрик](service-fabric-resource-governance.md). Дополнительные сведения о настройке пользовательских метрик см. в [этой статье](service-fabric-cluster-resource-manager-metrics.md).

Метрики отличаются от ограничений на размещение и свойств узлов. Свойства узлов — это статические дескрипторы самих узлов. Метрики описывают ресурсы на этих узлах, которые потребляются службами, запускаемыми на узле. Свойством может быть, например, HasSSD со значением true или false. Объем дискового пространства, доступного на этом твердотельном накопителе, и объем, используемый службами, например, может быть выражен метрикой DriveSpaceInMb. 

Важно отметить, что диспетчер кластерных ресурсов Service Fabric не понимает, что означают имена метрик (так же как и для ограничений на размещение и свойств узлов). Имена метрик — это просто строки. В случае неоднозначности мы советуем объявлять единицы как часть созданных имен метрик.

## <a name="capacity"></a>Capacity
При отключении *балансировки* всех ресурсов диспетчер кластерных ресурсов Service Fabric по-прежнему будет следить за тем, чтобы емкость ни одного узла не была превышена. Управление превышением емкости возможно в том случае, если кластер не перегружен и рабочая нагрузка не превышает возможности любого узла. Емкость — это другое *ограничение*, используемое диспетчером кластерных ресурсов, чтобы понять, какая часть ресурса используется на узле. Оставшаяся емкость также отслеживается для кластера в целом. На уровне службы и емкость, и потребление выражаются в виде метрик. Например, метрика может называться ClientConnections, а определенный узел может иметь емкость для ClientConnections, равную 32 768. У остальных узлов могут быть другие ограничения. Если на этом узле выполняется какая-либо служба, это может означать, что в данный момент потребляется 32 256 единиц метрики ClientConnections.

Во время выполнения диспетчер кластерных ресурсов отслеживает оставшуюся емкость в кластере и на узлах. Чтобы отслеживать емкость, диспетчер кластерных ресурсов вычитает емкость, используемую каждой службой, из емкости узла, на котором выполняются эти службы. С помощью этих сведений диспетчер кластерных ресурсов Service Fabric может определить, где следует разместить или куда переместить реплики так, чтобы не была превышена емкость узлов.

<center>
![Узлы и емкость кластера][Image7]
</center>

C#:

```csharp
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
ServiceLoadMetricDescription metric = new ServiceLoadMetricDescription();
metric.Name = "ClientConnections";
metric.PrimaryDefaultLoad = 1024;
metric.SecondaryDefaultLoad = 0;
metric.Weight = ServiceLoadMetricWeight.High;
serviceDescription.Metrics.Add(metric);
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

PowerShell:

```posh
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceTypeName –Stateful -MinReplicaSetSize 3 -TargetReplicaSetSize 3 -PartitionSchemeSingleton –Metric @("ClientConnections,High,1024,0)
```

Вы можете увидеть емкость, определенную в манифесте кластера:

ClusterManifest.xml

```xml
    <NodeType Name="NodeType03">
      <Capacities>
        <Capacity Name="ClientConnections" Value="65536"/>
      </Capacities>
    </NodeType>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json. 

```json
"nodeTypes": [
    {
        "name": "NodeType03",
        "capacities": {
            "ClientConnections": "65536",
        }
    }
],
```

Нередко нагрузка службы динамически изменяется. Предположим, что нагрузка реплики (по метрике ClientConnections) изменилась с 1024 на 2048, но оставшаяся емкость узла, на котором она выполнялась, для этой метрики составляла только 512. В этом случае расположение, где в данный момент находится реплика или экземпляр, станет недопустимым, так как на этом узле недостаточно места. Диспетчер кластерных ресурсов должен вмешаться и уменьшить загруженность узла в соответствии с его емкостью. Он уменьшает нагрузку на узел, емкость которого превышена, перемещая с него одну или несколько реплик либо экземпляров на другие узлы. После перемещения реплик диспетчер кластерных ресурсов пытается свести к минимуму стоимость перемещения. Стоимость перемещения рассматривается в [этой статье](service-fabric-cluster-resource-manager-movement-cost.md), а стратегии и правила перераспределения нагрузки диспетчера кластерных ресурсов описываются [здесь](service-fabric-cluster-resource-manager-metrics.md).

## <a name="cluster-capacity"></a>Емкость кластера
Как же диспетчер кластерных ресурсов Service Fabric делает так, что весь кластер не перегружается? Что ж, ввиду динамического характера нагрузки он мало что может сделать. В службах могут наблюдаться всплески нагрузки независимо от действий, предпринятых диспетчером кластерных ресурсов. В результате кластер с большим резервом сегодня может оказаться довольно маломощным при повышении нагрузки завтра. При этом существуют некоторые встроенные элементы управления, которые помогут избежать проблем. Первое, что можно сделать, — это предотвратить создание новых рабочих нагрузок, которые могут привести к переполнению кластера.

Предположим, вы создаете службу без отслеживания состояния, с которой связана некоторая нагрузка. Предположим, что этой службе нужна метрика DiskSpaceInMb и что она будет использовать 5 единиц DiskSpaceInMb для каждого экземпляра службы. Вы хотите создать три экземпляра службы. Отлично! Это значит, что в кластере должно быть 15 единиц метрики DiskSpaceInMb только для того, чтобы можно было создать эти экземпляры службы. Диспетчер кластерных ресурсов постоянно вычисляет емкость и потребление каждой метрики, чтобы можно было определить оставшуюся емкость в кластере. Если емкости недостаточно, диспетчер кластерных ресурсов отклоняет вызов создания службы.

Так как единственное требование заключается в наличии 15 доступных единиц, это пространство может быть распределено несколькими различными путями. Например, это может быть одна оставшаяся единица емкости на 15 различных узлах или три оставшиеся единицы емкости на 5 разных узлах. Если диспетчер кластерных ресурсов может реорганизовать элементы, чтобы на трех узлах были доступны пять единиц, он размещает службу. Реорганизация кластера невозможна, если кластер почти полностью заполнен или существующие службы по какой-либо причине нельзя объединить. В остальных случаях она, как правило, возможна.

## <a name="buffered-capacity"></a>Емкость буфера
Буферизованная емкость — еще одна функция диспетчера кластерных ресурсов. Она позволяет резервировать некоторую часть общей емкости узла. Этот буфер емкости используется только для размещения служб во время обновлений и сбоев узлов. Буферизованная мощность указывается глобально для каждой метрики для всех узлов. Значение, выбираемое для зарезервированной емкости, зависит от количества доменов обновления и доменов сбоя в кластере. Большее количество доменов сбоя и обновления означает, что можно выбрать меньшее значение зарезервированной емкости. Чем больше доменов, тем меньшее количество кластеров будет недоступным во время обновлений и сбоев. Указание буферизованной емкости имеет смысл, только если указана емкость узла для метрики.

Пример указания емкости буфера:

ClusterManifest.xml

```xml
        <Section Name="NodeBufferPercentage">
            <Parameter Name="SomeMetric" Value="0.15" />
            <Parameter Name="SomeOtherMetric" Value="0.20" />
        </Section>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json.

```json
"fabricSettings": [
  {
    "name": "NodeBufferPercentage",
    "parameters": [
      {
          "name": "SomeMetric",
          "value": "0.15"
      },
      {
          "name": "SomeOtherMetric",
          "value": "0.20"
      }
    ]
  }
]
```

При недостаточной емкости буфера для метрики создание служб завершается сбоем. Предотвращение создания новых служб для сохранения буфера гарантирует, что обновления и сбои не приведут к превышению емкости узлов. Емкость буфера указывать необязательно, но мы советуем добавить ее во всех кластерах, определяющих емкость для метрики.

Диспетчер кластерных ресурсов предоставляет эти сведения о нагрузке. Для каждой метрики эти сведения содержат: 
  - параметры буферизованной емкости;
  - общую емкость;
  - текущее потребление;
  - является ли каждая метрика сбалансированной;
  - статистические данные о среднеквадратическом отклонении;
  - наименее и наиболее нагруженные узлы.  
  
Ниже приведен пример таких выходных данных.

```posh
PS C:\Users\user> Get-ServiceFabricClusterLoadInformation
LastBalancingStartTimeUtc : 9/1/2016 12:54:59 AM
LastBalancingEndTimeUtc   : 9/1/2016 12:54:59 AM
LoadMetricInformation     :
                            LoadMetricName        : Metric1
                            IsBalancedBefore      : False
                            IsBalancedAfter       : False
                            DeviationBefore       : 0.192450089729875
                            DeviationAfter        : 0.192450089729875
                            BalancingThreshold    : 1
                            Action                : NoActionNeeded
                            ActivityThreshold     : 0
                            ClusterCapacity       : 189
                            ClusterLoad           : 45
                            ClusterRemainingCapacity : 144
                            NodeBufferPercentage  : 10
                            ClusterBufferedCapacity : 170
                            ClusterRemainingBufferedCapacity : 125
                            ClusterCapacityViolation : False
                            MinNodeLoadValue      : 0
                            MinNodeLoadNodeId     : 3ea71e8e01f4b0999b121abcbf27d74d
                            MaxNodeLoadValue      : 15
                            MaxNodeLoadNodeId     : 2cc648b6770be1bc9824fa995d5b68b1
```

## <a name="next-steps"></a>Дополнительная информация
* Сведения об архитектуре и потоке информации в диспетчере кластерных ресурсов см. в [этой статье](service-fabric-cluster-resource-manager-architecture.md).
* Определение метрик дефрагментации — один из способов объединения нагрузки на узлах вместо ее рассредоточения. Сведения о настройке дефрагментации см. в [этой статье](service-fabric-cluster-resource-manager-defragmentation-metrics.md).
* Начните с самого начала, [изучив общие сведения о диспетчере кластерных ресурсов Service Fabric](service-fabric-cluster-resource-manager-introduction.md)
* Чтобы узнать, как диспетчер кластерных ресурсов управляет нагрузкой кластера и балансирует ее, ознакомьтесь со статьей о [балансировке нагрузки](service-fabric-cluster-resource-manager-balancing.md)

[Image1]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-domains.png
[Image2]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-uneven-fault-domain-layout.png
[Image3]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domains-with-placement.png
[Image4]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domain-layout-strategies.png
[Image5]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-layout-different-workloads.png
[Image6]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-placement-constraints-node-properties.png
[Image7]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-nodes-and-capacity.png
