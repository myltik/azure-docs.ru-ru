---
title: Балансирование нагрузки кластера Azure Service Fabric | Документация Майкрософт
description: Общие сведения о распределении нагрузки в кластере с помощью диспетчера кластерных ресурсов Azure Service Fabric.
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: ''
ms.assetid: 030b1465-6616-4c0b-8bc7-24ed47d054c0
ms.service: Service-Fabric
ms.devlang: dotnet
ms.topic: conceptual
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 5d2f195c50750a5c7685f62c909f77b2960613e6
ms.sourcegitcommit: eb75f177fc59d90b1b667afcfe64ac51936e2638
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/16/2018
ms.locfileid: "34213152"
---
# <a name="balancing-your-service-fabric-cluster"></a>Балансировка кластера Service Fabric
Диспетчер кластерных ресурсов Service Fabric поддерживает динамическое изменение нагрузки, реагируя на добавление и удаление узлов или служб. Он также автоматически исправляет нарушения ограничений и упреждающе перераспределяет нагрузку в кластере. Но как часто выполняются эти действия и что их инициирует?

Диспетчер кластерных ресурсов выполняет три разные категории работы. К ним относятся:

1. Размещение: на этом этапе происходит размещение отсутствующих метрик с отслеживанием состояния или без. Размещаются как новые службы, так и реплики с отслеживанием состояния или экземпляры без отслеживания состояния, которые вызвали сбой. Тут выполняется удаление реплик и экземпляров.
2. Проверка ограничений: на этом этапе проверяются и устраняются нарушения различных ограничений (правил) размещения в системе. Примеры правил — это средства, предотвращающие избыточное использование ресурсов узлов, обеспечивающие соблюдение ограничений на размещение службы и т. п.
3. Балансировка: на этом этапе проверяется необходимость перераспределения нагрузки с учетом требуемого уровня нагрузки для различных метрик. Если это необходимо, предпринимается попытка найти более сбалансированную схему упорядочения в кластере.

## <a name="configuring-cluster-resource-manager-timers"></a>Настройка таймеров диспетчера кластерных ресурсов
Первый набор элементов управления балансировкой — это набор таймеров. Они определяют, как часто диспетчер кластерных ресурсов проверяет состояние кластера и выполняет корректирующие действия.

Каждым из типов исправлений, которые может внести Cluster Resource Manager, управляет соответствующий таймер, который определяет частоту их применения. При каждом срабатывании таймера задача добавляется в расписание. По умолчанию диспетчер ресурсов:

* проверяет состояние и применяет обновления (например, записывает, что узел не работает) каждые 1/10 секунды;
* устанавливает флаг проверки размещения; 
* каждую секунду устанавливает флаг проверки ограничений;
* устанавливает флаг балансировки каждые пять секунд.

Ниже приведены примеры конфигураций, устанавливающих эти таймеры.

ClusterManifest.xml:

``` xml
        <Section Name="PlacementAndLoadBalancing">
            <Parameter Name="PLBRefreshGap" Value="0.1" />
            <Parameter Name="MinPlacementInterval" Value="1.0" />
            <Parameter Name="MinConstraintCheckInterval" Value="1.0" />
            <Parameter Name="MinLoadBalancingInterval" Value="5.0" />
        </Section>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json.

```json
"fabricSettings": [
  {
    "name": "PlacementAndLoadBalancing",
    "parameters": [
      {
          "name": "PLBRefreshGap",
          "value": "0.10"
      },
      {
          "name": "MinPlacementInterval",
          "value": "1.0"
      },
      {
          "name": "MinConstraintCheckInterval",
          "value": "1.0"
      },
      {
          "name": "MinLoadBalancingInterval",
          "value": "5.0"
      }
    ]
  }
]
```

На сегодняшний день диспетчер кластерных ресурсов выполняет эти действия только по очереди. Именно поэтому мы называем таймеры минимальными интервалами, а действия, которые выполняются при срабатывании таймеров, — флагами параметров. Например, Cluster Resource Manager обрабатывает ожидающие запросы, чтобы создать службы перед балансировкой кластера. Как можно видеть, заданные интервалы времени по умолчанию означают, что диспетчер кластерных ресурсов часто проверяет, какие задачи ему необходимо выполнять. Обычно это означает, что набор изменений, вносимых на каждом шаге, мал. Благодаря высокой частоте внесения незначительных изменений диспетчер кластерных ресурсов чутко реагирует на то, что происходит в кластере. Таймеры по умолчанию обеспечивают своего рода пакетную обработку, поскольку обычно множество событий одного типа происходят одновременно. 

Например, когда происходит сбой узлов, то может выполняться одновременная обработка доменов сбоя целиком. Все эти сбои регистрируются во время следующего изменения состояния после *PLBRefreshGap*. Исправления определяются во время следующих циклов размещения, проверки ограничений и балансировки нагрузки. По умолчанию Cluster Resource Manager ищет изменения в кластере, внесенные на протяжении нескольких часов, и не пытается обработать все изменения за один раз. Такой подход может приводить к резкому увеличению оттока.

Чтобы выявить дисбаланс кластера, Cluster Resource Manager необходимы также некоторые дополнительные сведения. Для этого используются два других элемента конфигурации: *пороговые значения балансировки* и *пороговые значения активности*.

## <a name="balancing-thresholds"></a>Пороговые значения балансировки
Пороговое значение балансировки — это основной элемент управления для запуска перераспределения нагрузки. Пороговое значение балансировки для метрики представляет собой _коэффициент_. Если нагрузка для метрики на наиболее загруженном узле, деленная на объем нагрузки на наименее загруженном узле, превышает *пороговое значение балансировки*, то кластер считается несбалансированным. В результате балансировка запускается при очередной проверке службой Cluster Resource Manager. Таймер *MinLoadBalancingInterval* задает частоту, с которой диспетчер кластерных ресурсов проверяет необходимость перераспределения нагрузки. Проверка не означает, что что-то действительно произойдет. 

Пороговые значения балансировки задаются в определении кластера на основе метрик. Дополнительные сведения о метриках см. в [этой статье](service-fabric-cluster-resource-manager-metrics.md).

ClusterManifest.xml

```xml
    <Section Name="MetricBalancingThresholds">
      <Parameter Name="MetricName1" Value="2"/>
      <Parameter Name="MetricName2" Value="3.5"/>
    </Section>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json.

```json
"fabricSettings": [
  {
    "name": "MetricBalancingThresholds",
    "parameters": [
      {
          "name": "MetricName1",
          "value": "2"
      },
      {
          "name": "MetricName2",
          "value": "3.5"
      }
    ]
  }
]
```

<center>
![Пример порогового значения балансировки][Image1]
</center>

В этом примере каждая служба использует одну единицу определенной метрики. В верхнем примере максимальная нагрузка на узле составляет пять, а минимальная — два. Предположим, что пороговое значение балансировки для метрики — три. Так как коэффициент для кластера 5/2 = 2,5, что меньше указанного порогового значения балансировки, равного трем, кластер сбалансирован. При очередной проверке службой Cluster Resource Manager балансировка не запускается.

В примере ниже максимальная нагрузка на узле составляет десять, а минимальная — два (значит, коэффициент будет равен пяти). Пять больше установленного порогового значения балансировки для этой метрики, равного трем. В результате при следующем срабатывании таймера будет запланирован запуск перераспределения кластера. В подобной ситуации часть нагрузки, как правило, распределяется на узел Node3. Так как диспетчер кластерных ресурсов Service Fabric не использует каскадный подход, часть нагрузки может также распределиться на узел Node2. 

<center>
![Действия в примере порогового значения балансировки][Image2]
</center>

> [!NOTE]
> Для управления нагрузкой в кластере применяются две разные стратегии балансировки. Стратегия по умолчанию, которую использует диспетчер кластерных ресурсов, заключается в распределении нагрузки между узлами в кластере. Другой стратегией является [дефрагментация](service-fabric-cluster-resource-manager-defragmentation-metrics.md). Дефрагментация выполняется во время того же цикла балансировки. Стратегии балансировки и дефрагментации могут использоваться для различных метрик в одном и том же кластере. У службы могут быть метрики балансировки и дефрагментации. Для метрик дефрагментации коэффициент нагрузки в кластере активирует перераспределение, когда его значение _ниже_ порогового значения балансировки. 
>

Уменьшение разницы в нагрузке ниже заданного предела — это не главная цель. Пороговые значения балансировки — всего лишь *триггеры*. При выполнении балансировки диспетчер кластерных ресурсов определяет, какие улучшения он может внести, если это возможно. Сам по себе запуск балансировки не означает, что что-либо будет перемещено. Иногда кластер несбалансирован, но слишком ограничен для того, чтобы это можно было исправить. Кроме того, для внесения улучшений требуются перемещения, которые могут быть слишком [дорогостоящими](service-fabric-cluster-resource-manager-movement-cost.md).

## <a name="activity-thresholds"></a>пороговые значения активности
Иногда узлы могут быть относительно несбалансированными, даже если *общий* объем нагрузки в кластере небольшой. Отсутствие нагрузки может быть связано с ее временным снижением или с тем, что кластер только что создан и проходит начальную загрузку. В любом случае в такой ситуации можно не тратить время на балансировку кластера, так как это вряд ли принесет ощутимые результаты. Если кластер прошел балансировку, вы только потратите сетевые и вычислительные ресурсы на перемещение данных, а в результате не ощутите *абсолютно* никакой разницы. Чтобы ненужных перемещений, можно воспользоваться еще одним элементом управления под названием "Пороговые значения активности". С его помощью можно задать абсолютное значение нижней границы активности. Если это пороговое значение не превышено ни для одного узла, балансировка не запускается даже при достижении ее порогового значения.

Предположим, что для этой метрики задано пороговое значение балансировки, равное 3. Также предположим, что пороговое значение активности равно 1536. В первом случае согласно пороговому значению балансировки кластер несбалансирован, однако ни один из узлов не превышает пороговое значение активности, поэтому ничего не происходит. В примере ниже узел Node1 превышает пороговое значение активности. Так как для метрики превышены пороговые значения балансировки и активности, планируется балансировка. Например, рассмотрим схему ниже. 

<center>
![Пример порогового значения активности][Image3]
</center>

Как и пороговые значения балансировки, пороговые значения активности определяются в определении кластера на основе метрик:

ClusterManifest.xml

``` xml
    <Section Name="MetricActivityThresholds">
      <Parameter Name="Memory" Value="1536"/>
    </Section>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json.

```json
"fabricSettings": [
  {
    "name": "MetricActivityThresholds",
    "parameters": [
      {
          "name": "Memory",
          "value": "1536"
      }
    ]
  }
]
```

Пороговые значения балансировки и активности привязаны к определенной метрике. Балансировка запускается только тогда, когда оба эти пороговые значения превышены для одной и той же метрики.

> [!NOTE]
> Если ничего не указывать, пороговое значение балансировки для метрики будет равно 1, а пороговое значение активности — 0. Это означает, что диспетчер кластерных ресурсов будет сохранять оптимальную балансировку этой метрики для любой заданной нагрузки. Если вы используете настраиваемые метрики, рекомендуется явно определять для них свои пороговые значения балансировки и активности. 
>

## <a name="balancing-services-together"></a>Одновременная балансировка служб
Несбалансированность кластера определяется на уровне кластера. Тем не менее она устраняется путем перемещения реплик или экземпляров отдельных служб. Звучит разумно, не правда ли? Если память накапливается в одном узле, это может быть вызвано сразу несколькими репликами или экземплярами. Для устранения дисбаланса может потребоваться перемещение всех реплик с отслеживанием состояния или экземпляров без отслеживания состояния, для которых используется несбалансированная метрика.

В отдельных случаях перемещается служба, которая сама по себе не была несбалансированной (вспомните обсуждение локальных и глобальных весов, приведенное ранее). Почему же служба перемещается, если все ее метрики сбалансированы? Рассмотрим пример.

- Возьмем для примера четыре службы: Service1, Service2, Service3 и Service4. 
- Служба Service1 передает метрики Metric1 и Metric2. 
- Служба Service2 передает метрики Metric2 и Metric3. 
- Служба Service3 передает метрики Metric3 и Metric4.
- Служба Service4 передает метрику Metric99. 

Понимаете, к чему все это ведет? Это же цепочка! Мы имеем дело не с четырьмя независимыми службами, а с тремя связанными службами и одной отдельной службой.

<center>
![Одновременная балансировка служб][Image4]
</center>

Из-за этой цепочки вполне возможно, что дисбаланс в метриках Metric1–Metric4 может привести к перемещению реплик или экземпляров, относящихся к службам Service1–Service3. Кроме того, нам известно, что дисбаланс в метрике Metric1, Metric2 или Metric3 не вызовет перемещения в службе Service4. Это не имеет смысла, так как перемещение реплик или экземпляров службы Service4 никак не повлияет на баланс метрик Metric1–Metric3.

Диспетчер кластерных ресурсов автоматически определяет, какие службы связаны. Добавление, удаление или изменение метрик для служб может влиять на их связи. Например, между двумя циклами балансировки служба Service2 может быть изменена для удаления из нее метрики Metric2. связь между службами S1 и S2 будет разорвана. В этом случае две группы связанных служб превратятся в три.

<center>
![Одновременная балансировка служб][Image5]
</center>

## <a name="next-steps"></a>Дополнительная информация
* Метрики показывают, как диспетчер кластерных ресурсов Service Fabric управляет потреблением и емкостью в кластере. Чтобы узнать больше о метриках и их настройке, ознакомьтесь с [этой статьей](service-fabric-cluster-resource-manager-metrics.md).
* Стоимость перемещения — один из способов сообщить диспетчеру кластерных ресурсов, что некоторые службы перемещать затратнее, чем остальные. Дополнительные сведения о стоимости перемещения см. в [этой статье](service-fabric-cluster-resource-manager-movement-cost.md).
* В диспетчере кластерных ресурсов имеется несколько регулировок, которые можно настроить, чтобы замедлить отток в кластере. Обычно они не требуется, но при необходимости узнать о регулировках можно [здесь](service-fabric-cluster-resource-manager-advanced-throttling.md)

[Image1]:./media/service-fabric-cluster-resource-manager-balancing/cluster-resrouce-manager-balancing-thresholds.png
[Image2]:./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-threshold-triggered-results.png
[Image3]:./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-activity-thresholds.png
[Image4]:./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-services-together1.png
[Image5]:./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-services-together2.png
