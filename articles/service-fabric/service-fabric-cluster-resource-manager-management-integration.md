---
title: Диспетчер кластерных ресурсов Service Fabric — интеграция управления | Документация Майкрософт
description: Обзор точек интеграции между диспетчером ресурсов кластера и управлением Service Fabric.
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: ''
ms.assetid: 956cd0b8-b6e3-4436-a224-8766320e8cd7
ms.service: Service-Fabric
ms.devlang: dotnet
ms.topic: conceptual
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 3f93ca94d5aa3e95637a53a4c8fe3d9d264dd58c
ms.sourcegitcommit: eb75f177fc59d90b1b667afcfe64ac51936e2638
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/16/2018
ms.locfileid: "34208290"
---
# <a name="cluster-resource-manager-integration-with-service-fabric-cluster-management"></a>Интеграция диспетчера кластерных ресурсов с управлением кластерами Service Fabric
Диспетчер кластерных ресурсов Service Fabric не инициирует обновления в Service Fabric, но участвует в них. Первый способ, используемый диспетчером кластерных ресурсов при управлении, заключается в отслеживании требуемого состояния кластера и служб внутри него. Он отправляет отчеты о работоспособности, если не удается перевести кластер в нужное состояние. Например, если емкости недостаточно, то диспетчер кластерных ресурсов отправляет предупреждения о работоспособности и сообщает об ошибках работоспособности, указывающие на проблему. Второе направление интеграции связано с обновлениями. Во время обновлений диспетчер кластерных ресурсов изменяет свое поведение.  

## <a name="health-integration"></a>Интеграция функций работоспособности
Диспетчер кластерных ресурсов постоянно отслеживает правила, которые вы определили для размещения служб. Он также отслеживает оставшуюся емкость для каждой метрики на узлах кластера и в кластере в целом. Он передает предупреждения о работоспособности и сообщает об ошибках работоспособности, если не может выполнить эти правила или выделить достаточную емкость. Например, если узел перегружен и диспетчер кластерных ресурсов попытается исправить ситуацию за счет перемещения служб. Если диспетчер кластерных ресурсов не может исправить ситуацию, то он выведет предупреждение о работоспособности, указывающее, какой узел перегружен и по каким метрикам.

Кроме того, он выводит предупреждения о работоспособности в случае нарушения ограничений размещения. Например, если определено ограничение размещения (например, `“NodeColor == Blue”`) и диспетчер ресурсов обнаруживает его нарушение, то он выводит предупреждение о работоспособности. Это делается для настраиваемых ограничений, а также для ограничений по умолчанию (например, ограничений для доменов сбоя и доменов обновления).

Ниже приведен пример такого отчета о работоспособности. В этом случае отчет о работоспособности формируется для одного из разделов системной службы. Сообщение о работоспособности указывает, что реплики этого раздела временно сгруппированы в слишком малое количество доменов обновления.

```posh
PS C:\Users\User > Get-WindowsFabricPartitionHealth -PartitionId '00000000-0000-0000-0000-000000000001'


PartitionId           : 00000000-0000-0000-0000-000000000001
AggregatedHealthState : Warning
UnhealthyEvaluations  :
                        Unhealthy event: SourceId='System.PLB', Property='ReplicaConstraintViolation_UpgradeDomain', HealthState='Warning', ConsiderWarningAsError=false.

ReplicaHealthStates   :
                        ReplicaId             : 130766528804733380
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528804577821
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528854889931
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528804577822
                        AggregatedHealthState : Ok

                        ReplicaId             : 130837073190680024
                        AggregatedHealthState : Ok

HealthEvents          :
                        SourceId              : System.PLB
                        Property              : ReplicaConstraintViolation_UpgradeDomain
                        HealthState           : Warning
                        SequenceNumber        : 130837100116930204
                        SentAt                : 8/10/2015 7:53:31 PM
                        ReceivedAt            : 8/10/2015 7:53:33 PM
                        TTL                   : 00:01:05
                        Description           : The Load Balancer has detected a Constraint Violation for this Replica: fabric:/System/FailoverManagerService Secondary Partition 00000000-0000-0000-0000-000000000001 is
                        violating the Constraint: UpgradeDomain Details: UpgradeDomain ID -- 4, Replica on NodeName -- Node.8 Currently Upgrading -- false Distribution Policy -- Packing
                        RemoveWhenExpired     : True
                        IsExpired             : False
                        Transitions           : Ok->Warning = 8/10/2015 7:13:02 PM, LastError = 1/1/0001 12:00:00 AM
```

В сообщении о работоспособности содержатся следующие сведения:

1. Все реплики находятся в работоспособном состоянии — их свойство AggregatedHealthState имеет значение Ok.
2. Ограничение распределения в доменах обновления в настоящее время нарушается. Это означает, что в определенном домене обновления больше реплик из данной секции, чем должно быть.
3. Какой из узлов содержит реплику, которая привела к нарушению. В данном случае это узел Node.8.
4. Выполняется ли в данный момент обновление этой секции ("Currently Upgrading -- false").
5. Политика распределения для этой службы ("Distribution Policy -- Packing"). Ею управляет [политика размещения](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md#requiring-replica-distribution-and-disallowing-packing) `RequireDomainDistribution`. Значение "Packing" означает, что в этом случае распределение в доменах _не_ требовалось, поэтому нам известно, что политика размещения не была указана для этой службы. 
6. Время создания отчета (10.08.2015 в 19:13:02).

На основе таких сведений создаются оповещения, которые выводятся в рабочей среде. Они информируют о наличии проблем и позволяют выявлять и останавливать недопустимые обновления. В этом случае мы хотели бы узнать, почему диспетчер ресурсов должен группировать реплики в домен обновления. Как правило, группирование является временным и возникает, например, из-за того, что была нарушена работа узлов в других доменах обновления.

Предположим, диспетчер кластерных ресурсов пытается разместить определенные службы, но приемлемые решения отсутствуют. Если размещение служб невозможно, обычно это происходит по одной из следующих причин:

1. Некое переходное условие сделало невозможным правильное размещение этого экземпляра службы или реплики.
2. Требования к размещению службы невыполнимы.

В этих случаях отчеты о работоспособности от диспетчера кластерных ресурсов помогают определить, почему службу не удается разместить. Этот процесс называется последовательностью устранения ограничений. В его ходе система анализирует настроенные ограничения, влияющие на службу, и записывает, что именно они исключают. Таким образом, когда не удается разместить какие-либо службы, можно узнать, какие узлы были исключены и почему.

## <a name="constraint-types"></a>Типы ограничений
Давайте поговорим о каждом из ограничений в этих отчетах о работоспособности. Сообщения о работоспособности, связанные с этими ограничениями, отображаются, когда не удается разместить реплики.

* **ReplicaExclusionStatic** и **ReplicaExclusionDynamic**. Эти ограничения указывают, решение было отклонено, так как в противном случае пришлось бы разместить на одном узле два объекта службы из одной и той же секции. Это недопустимо, так как сбой данного узла чрезмерно повлиял бы на эту секцию. ReplicaExclusionStatic и ReplicaExclusionDynamic — это практически аналогичные правила, но с незначительными отличиями. Если вы видите последовательность устранения ограничений, содержащую ограничение ReplicaExclusionStatic или ReplicaExclusionDynamic, то диспетчер кластерных ресурсов считает, что узлов недостаточно. Это потребует использовать эти недопустимые размещения в оставшихся решениях, что запрещено. Другие ограничения в последовательности, как правило, позволяют определить, почему вообще исключаются узлы.
* **PlacementConstraint**. Это сообщение означает, что некоторые узлы были устранены из-за несоответствия ограничениям на размещение службы. Мы рассматриваем текущие настроенные ограничения на размещения как часть этого сообщения. Это нормально, если определено ограничение размещения. Однако если в ограничении размещения имеется ошибка, которая приводит к исключению слишком большого числа узлов, то именно так вы заметите это.
* **NodeCapacity**. Это ограничение означает, что диспетчер кластерных ресурсов не может размещать реплики на указанных узлах, так как это приведет к превышению их емкости.
* **Affinity**. Это ограничение означает, что нельзя размещать реплики на затронутых узлах, так как это приведет к нарушению ограничения сходства. Дополнительные сведения о сходстве см. в [этой статье](service-fabric-cluster-resource-manager-advanced-placement-rules-affinity.md).
* **FaultDomain** и **UpgradeDomain**. Эти ограничения исключают узлы, если размещение реплики на указанных узлах привело к упаковке в конкретный домен сбоя или обновления. Несколько примеров, посвященных этому ограничению, представлены в разделе об [ограничениях доменов сбоя и обновления и результирующем поведении](service-fabric-cluster-resource-manager-cluster-description.md).
* **PreferredLocation**. Обычно это ограничение не отображается. Оно приводит к удалению узлов из решения, так как оно выполняется в качестве оптимизации по умолчанию. Предпочтительное ограничение размещения также действует во время обновлений. Оно используется для возврата служб в то расположение, в котором они находились при запуске обновления.

## <a name="blocklisting-nodes"></a>Добавление узлов в список блокировки
Еще одно сообщение о работоспособности, которое передает диспетчер кластерных ресурсов, информирует о добавлении узлов в список блокировки. Список блокировки можно считать временным ограничением, которое применяется автоматически. Узлы добавляются в список блокировки, когда на них повторяются сбои при запуске экземпляров заданного типа службы. Узлы добавляются в список блокировки на основе типа службы. Узел может быть добавлен в список блокировки для одного типа службы, но может отсутствовать в нем для другого типа службы. 

Добавление в список блокировки часто можно видеть во время разработки: некоторые ошибки приводит к сбою при запуске узла службы. Service Fabric несколько раз пытается создать узел службы, но сбои продолжаются. После нескольких попыток узел будет добавлен в список блокировки, и диспетчер кластерных ресурсов попытается создать службу в другом расположении. Если этот сбой будет повторяться на нескольких узлах, может случиться так, что все допустимые узлы в кластере окажутся заблокированы. Добавление в список блокировки может также привести к удалению столь многих узлов, что оставшейся емкости будет недостаточно для успешного запуска службы в требуемом масштабе. Как правило, вы увидите дополнительные ошибки и предупреждения из диспетчера кластерных ресурсов, указывающее, что служба использует меньше требуемого числа реплик или экземпляров, а также сообщения о работоспособности, указывающие на сбой, который изначально приводит к добавлению в список блокировки.

Добавление в список блокировки не является окончательным. Через несколько минут узел удаляется из списка блокировки, и Service Fabric может повторно активировать службы на этом узле. Если запуск служб по-прежнему завершается сбоем, узел снова добавляется в список блокировки для этого типа службы. 

### <a name="constraint-priorities"></a>Приоритеты ограничений

> [!WARNING]
> Изменение приоритетов ограничения не рекомендуется и может иметь значительное отрицательное воздействие на кластер. Ниже приведена справочная информация о приоритетах ограничений по умолчанию и их поведении. 
>

Изучив все эти ограничения, вы можете прийти к выводу, что ограничения для доменов сбоя — это самый важный аспект в вашей системе. Чтобы обеспечить отсутствие нарушений ограничения для доменов сбоя, мне стоит нарушать другие ограничения".

Для ограничений можно настроить различные уровни приоритета, а именно:

   - "жесткий" (0);
   - "мягкий" (1);
   - "оптимизационный" (2);
   - "отключен" (-1). 
   
Большинство ограничений по умолчанию настроено как жесткие ограничения.

Изменение приоритета ограничения происходит редко. Случались ситуации, в которых приоритеты ограничений требовалось изменить, обычно для обхода каких-либо ошибок или поведения, которое влияло на среду. Обычно гибкость инфраструктуры приоритета ограничений давала положительные результаты, но зачастую она не нужна. В большинстве случаев все ограничения сохраняют свои приоритеты. 

Уровни приоритета не означают ни то, что данное ограничение _будет_ нарушено, ни то, что оно всегда будет выполняться. Приоритеты ограничений определяют порядок, в котором накладываются ограничения. Приоритеты определяют компромиссы на случай, если невозможно выполнить все ограничения. Обычно все ограничения могут быть выполнены, если только в среде не выполняется что-то еще. К примерам ситуаций, которые приводят к нарушениям ограничений, можно отнести конфликтующие ограничения или большое число одновременных сбоев.

В сложных ситуациях приоритеты ограничений можно изменить. Например, вы хотите разрешить нарушение ограничений сходства ради устранения проблем с емкостью узла. Для этого вы можете задать для ограничений сходства "мягкий" приоритет (1), а для ограничений емкости оставить "жесткий" приоритет (0).

Ниже перечислены значения приоритета по умолчанию для различных ограничений, указанных в приведенной ниже конфигурации.

ClusterManifest.xml

```xml
        <Section Name="PlacementAndLoadBalancing">
            <Parameter Name="PlacementConstraintPriority" Value="0" />
            <Parameter Name="CapacityConstraintPriority" Value="0" />
            <Parameter Name="AffinityConstraintPriority" Value="0" />
            <Parameter Name="FaultDomainConstraintPriority" Value="0" />
            <Parameter Name="UpgradeDomainConstraintPriority" Value="1" />
            <Parameter Name="PreferredLocationConstraintPriority" Value="2" />
        </Section>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json.

```json
"fabricSettings": [
  {
    "name": "PlacementAndLoadBalancing",
    "parameters": [
      {
          "name": "PlacementConstraintPriority",
          "value": "0"
      },
      {
          "name": "CapacityConstraintPriority",
          "value": "0"
      },
      {
          "name": "AffinityConstraintPriority",
          "value": "0"
      },
      {
          "name": "FaultDomainConstraintPriority",
          "value": "0"
      },
      {
          "name": "UpgradeDomainConstraintPriority",
          "value": "1"
      },
      {
          "name": "PreferredLocationConstraintPriority",
          "value": "2"
      }
    ]
  }
]
```

## <a name="fault-domain-and-upgrade-domain-constraints"></a>Ограничения доменов сбоя и обновления
Диспетчер кластерных ресурсов старается распределять службы между доменами сбоя и доменами обновления. Эта стратегия моделируется как ограничение внутри ядра диспетчера кластерных ресурсов. Дополнительные сведения об использовании ограничений и описание их поведения приведены в разделе [Ограничения доменов сбоя и обновления и соответствующее поведение](service-fabric-cluster-resource-manager-cluster-description.md#fault-and-upgrade-domain-constraints-and-resulting-behavior).

Диспетчеру кластерных ресурсов может потребоваться сгруппировать пару реплик в домен обновления, чтобы устранить проблемы, связанные с обновлениями, сбоями или другими нарушениями ограничений. Группирование в домены сбоя или домены обновления обычно происходит только при наличии нескольких сбоев или других сложностей в системе, мешающих правильному размещению. Если вы хотите предотвратить группирование даже в таких ситуациях, можно использовать [политику размещения](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md#requiring-replica-distribution-and-disallowing-packing) `RequireDomainDistribution`. Обратите внимание, что это может вызвать побочные эффекты, затрагивающие доступность и надежность службы, поэтому следует проявить осторожность.

Если среда настроена правильно, то все ограничения полностью соблюдаются даже во время обновлений. Ключевой момент заключается в том, что диспетчер кластерных ресурсов наблюдает за вашими ограничениями. При обнаружении нарушения он немедленно сообщает об этом и пытается устранить проблему.

## <a name="the-preferred-location-constraint"></a>Ограничение на предпочтительное расположение
Ограничение PreferredLocation немного отличается от остальных, так как имеет два варианта использования. Во-первых, это ограничение применяется во время обновления приложений. Диспетчер кластерных ресурсов автоматически управляет этим ограничением во время обновлений. Оно обеспечивает возврат реплик в исходные расположения после завершения обновлений. Во-вторых, ограничение PreferredLocation используется для [политики размещения](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md) `PreferredPrimaryDomain`. Оба варианта использования являются оптимизацией, поэтому ограничение PreferredLocation — единственное ограничение с "оптимизационным" приоритетом по умолчанию.

## <a name="upgrades"></a>Обновления
Диспетчер кластерных ресурсов также помогает во время обновлений приложения и кластера. Он выполняет следующие задания:

* обеспечивает соблюдение правил кластера;
* обеспечивает плавное выполнение обновления.

### <a name="keep-enforcing-the-rules"></a>Применение правил
В первую очередь следует иметь в виду, что правила (такие строгие ограничения, как ограничения размещения и емкости) по-прежнему действуют во время обновлений. Ограничения на размещение гарантируют, что все ваши рабочие нагрузки выполняются только на определенных узлах даже во время обновления. При высокой ограниченности служб обновления могут занимать больше времени. Если служба или узел, на котором она выполняется, отключается для обновления, доступно несколько вариантов для перемещения этой службы или узла.

### <a name="smart-replacements"></a>Интеллектуальные замены
При запуске обновления диспетчер ресурсов создает моментальный снимок текущего размещения кластера. Когда каждый домен обновления завершает обновление, он пытается вернуть службы, которые были размещены в этом домене обновления, в их исходное расположение. Таким образом во время обновления служба перемещается не более двух раз. Один раз она перемещается с затронутого узла, второй раз — обратно на этот узел. Возврат кластера или службы в исходное расположение до обновления гарантирует также, что обновление не окажет влияния на структуру кластера. 

### <a name="reduced-churn"></a>Сокращение оттока
Во время обновлений происходит еще кое-что — диспетчер кластерных ресурсов отключает балансировку. Отключение балансировки позволяет избежать ненужной реакции системы на само обновление, например избежать переноса служб на узлы, очищенные для обновления. Если рассматривается обновление кластера, то на время обновления приостанавливается балансировка во всем кластере. Проверки ограничений остаются включенными, отключается только перемещение на основе упреждающей балансировки метрик.

### <a name="buffered-capacity--upgrade"></a>Буферизованная емкость и обновление
Обычно возникает необходимость завершить обновление, даже если кластер ограничен или почти заполнен. Управление емкостью кластера во время обновлений даже важнее, чем обычно. При развертывании обновления в кластере требуется перенос от 5 до 20 процентов от общей емкости, в зависимости от числа доменов обновления. Эту нагрузку нужно перенаправить. Именно здесь пригодится понятие [буферизованных емкостей](service-fabric-cluster-resource-manager-cluster-description.md#buffered-capacity). Буферизованная емкость учитывается во время обычной работы. При необходимости во время обновлений диспетчер кластерных ресурсов может заполнить узлы вплоть до их общей емкости (использовав буфер).

## <a name="next-steps"></a>Дополнительная информация
* Начните с самого начала, [изучив общие сведения о диспетчере кластерных ресурсов Service Fabric](service-fabric-cluster-resource-manager-introduction.md)
