---
title: Рекомендации по настройке производительности для MapReduce в Azure Data Lake Store | Документация Майкрософт
description: Рекомендации по настройке производительности для MapReduce в Azure Data Lake Store
services: data-lake-store
documentationcenter: ''
author: stewu
manager: amitkul
editor: stewu
ms.assetid: ebde7b9f-2e51-4d43-b7ab-566417221335
ms.service: data-lake-store
ms.devlang: na
ms.topic: article
ms.date: 12/19/2016
ms.author: stewu
ms.openlocfilehash: f5586e7706d4dad7e3c943b2a661fa296b4d30bf
ms.sourcegitcommit: eb75f177fc59d90b1b667afcfe64ac51936e2638
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/16/2018
ms.locfileid: "34198639"
---
# <a name="performance-tuning-guidance-for-mapreduce-on-hdinsight-and-azure-data-lake-store"></a>Рекомендации по настройке производительности для MapReduce в HDInsight и Azure Data Lake Store

## <a name="prerequisites"></a>предварительным требованиям

* **Подписка Azure**. См. страницу [бесплатной пробной версии Azure](https://azure.microsoft.com/pricing/free-trial/).
* **Учетная запись хранения озера данных Azure**. Инструкции по созданию учетной записи см. в статье [Начало работы с Azure Data Lake Store с помощью портала Azure](data-lake-store-get-started-portal.md).
* **Кластер Azure HDInsight** с доступом к учетной записи Data Lake Store. См. статью [Создание кластера HDInsight с Data Lake Store](data-lake-store-hdinsight-hadoop-use-portal.md). Убедитесь, что вы включили удаленный рабочий стол для кластера.
* **Использование MapReduce в HDInsight**.  См. дополнительные сведения об [использовании MapReduce в Hadoop и HDInsight](https://docs.microsoft.com/azure/hdinsight/hdinsight-use-mapreduce)
* **Рекомендации по настройке производительности в Azure Data Lake Store**.  См. [рекомендации по настройке производительности для Azure Data Lake Store](https://docs.microsoft.com/azure/data-lake-store/data-lake-store-performance-tuning-guidance).

## <a name="parameters"></a>Параметры

Ниже перечислены наиболее важные параметры, которые можно настроить, чтобы оптимизировать производительность ADLS при выполнении заданий MapReduce.

* **Mapreduce.map.memory.mb** — объем памяти, выделяемой для каждого модуля сопоставления.
* **Mapreduce.job.maps** — число задач сопоставления на задание.
* **Mapreduce.reduce.memory.mb** — объем памяти, выделяемой для каждого модуля уменьшения.
* **Mapreduce.job.reduces** — число задач уменьшения на задание.

**Mapreduce.map.memory или Mapreduce.reduce.memory** — это число настраивается в зависимости от объема памяти, требуемой для задачи сопоставления или уменьшения.  Стандартные значения mapreduce.map.memory и mapreduce.reduce.memory можно просмотреть в Ambari в конфигурации YARN.  В Ambari перейдите к YARN и откройте вкладку конфигураций.  Отобразится объем памяти YARN.  

**Mapreduce.job.maps или Mapreduce.job.reduces** — определяет максимальное число создаваемых модулей сопоставления или уменьшения.  Количество разделений определяет число модулей сопоставления, создаваемых для задания MapReduce.  Следовательно, если разделений меньше, чем запрошенных модулей сопоставления, вы можете получить меньше модулей сопоставления, чем планировалось.       

## <a name="guidance"></a>Руководство

**Шаг 1. Определение числа запущенных заданий**. По умолчанию MapReduce будет использовать весь кластер для обработки задания.  Можно выделить меньший объем кластера, используя меньшее модулей сопоставления, чем доступных контейнеров.  В этой статье предполагается, что ваше приложение — единственное, которое выполняется в кластере.      

**Шаг 2. Настройка mapreduce.map.memory/mapreduce.reduce.memory**. Объем памяти для задач сопоставления и уменьшения будет зависеть от конкретного задания.  Можно уменьшить объем памяти, если нужно повысить параллелизм.  Число одновременно выполняемых задач зависит от числа контейнеров.  Сокращая объем памяти для каждого модуля сопоставления или уменьшения, можно создавать дополнительные контейнеры, обеспечивая одновременный запуск большего числа модулей сопоставления или уменьшения.  Слишком сильное снижение объема памяти может вызвать нехватку памяти при выполнении некоторых процессов.  Если при выполнении задания возникает ошибка кучи, следует увеличить объем памяти для модуля сопоставления или уменьшения.  При этом следует учитывать следующее. Добавление нескольких контейнеров ведет к увеличению нагрузки для каждого дополнительного контейнера, из-за которой может снизиться производительность.  Альтернатива — обеспечить больший объем памяти, используя кластер с большим объемом памяти или увеличивая число узлов в кластере.  Больший объем памяти позволит использовать дополнительные контейнеры, обеспечивая возможность параллельной обработки.  

**Шаг 3. Определение общего объема памяти YARN**. Для настройки mapreduce.job.maps/mapreduce.job.reduces следует учитывать общий объем памяти YARN, доступный для использования.  Эта информация доступна в Ambari.  Перейдите к YARN и откройте вкладку конфигураций.  Объем памяти YARN отобразится в этом окне.  Чтобы определить общий объем памяти YARN, умножьте объем памяти YARN для одного узла на число узлов в кластере.

    Total YARN memory = nodes * YARN memory per node
Если используется пустой кластер, это значение может быть представлено общим объемом памяти YARN для кластера.  Если другие приложения используют память, вы можете ограничить выделение памяти кластера, сократив число модулей сопоставления или уменьшения в соответствии с числом контейнеров, которые вы хотите использовать.  

**Шаг 4. Расчет числа контейнеров YARN**. Контейнеры YARN определяют уровень параллелизма для задания.  Разделите общий объем памяти YARN на значение mapreduce.map.memory.  

    # of YARN containers = total YARN memory / mapreduce.map.memory

**Шаг 5. Установка mapreduce.job.maps/mapreduce.job.reduces.** Установите для mapreduce.job.maps/mapreduce.job.reduces по крайней мере число доступных контейнеров.  Вы можете поэкспериментировать, увеличивая число модулей сопоставления и уменьшения, чтобы понять, помогает ли это оптимизировать производительность.  Имейте в виду, что большое число модулей сопоставления может увеличить нагрузку, что, в свою очередь, может привести к снижению производительности.  

Возможности изоляции и планирования использования ресурсов ЦП отключены по умолчанию, следовательно, число контейнеров YARN ограничено объемом памяти.

## <a name="example-calculation"></a>Пример вычисления

Предположим, у вас есть кластер, который состоит из 8 узлов D14 и вы хотите запустить задание с большим количеством операций ввода-вывода.  Ниже представлены примеры вычислений, которые нужно выполнить.

**Шаг 1. Определение числа запущенных заданий**. В нашем примере предполагается, что выполняется только одно задание.  

**Шаг 2: Настройка mapreduce.map.memory/mapreduce.reduce.memory**. В нашем примере выполняется задание с большим объемом операций ввода-вывода. Следовательно, выделить 3 ГБ памяти для задач сопоставления будет достаточно.

    mapreduce.map.memory = 3GB
**Шаг 3. Определение общей памяти YARN**

    total memory from the cluster is 8 nodes * 96GB of YARN memory for a D14 = 768GB
**Шаг 4. Расчет числа контейнеров YARN**

    # of YARN containers = 768GB of available memory / 3 GB of memory =   256

**Шаг 5. Установка mapreduce.job.maps/mapreduce.job.reduces**

    mapreduce.map.jobs = 256

## <a name="limitations"></a>Ограничения

**Регулирование Azure Data Lake Store**

Как мультитенантная служба, ADLS определяет ограничения пропускной способности на уровне учетной записи.  При превышении этих ограничений происходит сбой задач. Это можно определить, отслеживая ошибки регулирования в журналах задач.  Если для обработки задания требуется большая пропускная способность, свяжитесь с нами.   

Чтобы проверить, применяется ли для вас регулирование, включите ведение журнала отладки на стороне клиента. Вот как это сделать.

1. Включите следующее свойство в библиотеку Log4j в Ambari, последовательно выбрав YARN > Config > Advanced yarn-log4j: log4j.logger.com.microsoft.azure.datalake.store=DEBUG.

2. Перезапустите все узлы и службы, чтобы изменения конфигурации вступили в силу.

3. Если регулирование выполняется, вы увидите код ошибки HTTP 429 в файле журнала YARN. Файл журнала YARN расположен здесь: /tmp/&lt;пользователь&gt;/yarn.log

## <a name="examples-to-run"></a>Примеры выполнения кода

Чтобы показать, как MapReduce выполняется в Azure Data Lake Store, ниже приведен пример кода, выполняемого в кластере со следующими параметрами:

* 16 узлов D14v2;
* кластер Hadoop под управлением HDI 3.6.

Для начала ознакомьтесь с примерами команд для запуска MapReduce Teragen, Terasort и Teravalidate.  Эти команды можно настроить в соответствии с имеющимися ресурсами.

**Teragen**

    yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar teragen -Dmapreduce.job.maps=2048 -Dmapreduce.map.memory.mb=3072 10000000000 adl://example/data/1TB-sort-input

**Terasort**

    yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar terasort -Dmapreduce.job.maps=2048 -Dmapreduce.map.memory.mb=3072 -Dmapreduce.job.reduces=512 -Dmapreduce.reduce.memory.mb=3072 adl://example/data/1TB-sort-input adl://example/data/1TB-sort-output

**Teravalidate**

    yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar teravalidate -Dmapreduce.job.maps=512 -Dmapreduce.map.memory.mb=3072 adl://example/data/1TB-sort-output adl://example/data/1TB-sort-validate
