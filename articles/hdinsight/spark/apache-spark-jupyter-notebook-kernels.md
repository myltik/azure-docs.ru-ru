---
title: Ядра для записной книжки Jupyter в кластерах Spark в Azure HDInsight | Документация Майкрософт
description: Сведения о ядрах PySpark, PySpark3 и Spark для записной книжки Jupyter, доступной с кластерами Spark в Azure HDInsight.
keywords: записная книжка jupyter в spark, jupyter в spark
services: hdinsight
documentationcenter: ''
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.devlang: na
ms.topic: conceptual
ms.date: 02/22/2018
ms.author: nitinme
ms.openlocfilehash: 58a0bf27109af3131bd102fd43e9367d267525f3
ms.sourcegitcommit: 1362e3d6961bdeaebed7fb342c7b0b34f6f6417a
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/18/2018
ms.locfileid: "31521540"
---
# <a name="kernels-for-jupyter-notebook-on-spark-clusters-in-azure-hdinsight"></a>Ядра для записной книжки Jupyter в кластерах Spark в Azure HDInsight 

Кластеры HDInsight Spark предоставляют ядра, которые можно использовать с записной книжкой Jupyter в Spark для тестирования приложений. Ядра — это программа, которая выполняет и интерпретирует ваш код. Вот эти ядра:

- **PySpark** (для приложений, написанных на языке Python2).
- **PySpark3** (для приложений, написанных на языке Python3).
- **Spark** (для приложений, написанных на языке Scala).

В этой статье вы узнаете, как использовать эти ядра, а также преимущества их использования.

## <a name="prerequisites"></a>предварительным требованиям

* Кластер Apache Spark в HDInsight. Инструкции см. в статье [Начало работы. Создание кластера Apache Spark в HDInsight на платформе Linux и выполнение интерактивных запросов с помощью SQL Spark](apache-spark-jupyter-spark-sql.md).

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a>Создание записной книжки Jupyter в Spark HDInsight

1. Откройте кластер на [портале Azure](https://portal.azure.com/).  Инструкции см. в разделе [Отображение кластеров](../hdinsight-administer-use-portal-linux.md#list-and-show-clusters). Кластер откроется в новой колонке портала.

2. В разделе **Быстрые ссылки** щелкните **Панели мониторинга кластера**, чтобы открыть колонку **Панели мониторинга кластера**.  Если раздел **Быстрые ссылки** не отображается, в колонке в меню слева щелкните **Обзор**.

    ![Записная книжка Jupyter в Spark](./media/apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Записная книжка Jupyter в Spark") 

3. Щелкните **Записная книжка Jupyter**. При появлении запроса введите учетные данные администратора для кластера.
   
   > [!NOTE]
   > Вы также можете получить доступ к записной книжке Jupyter в кластере Spark, открыв следующий URL-адрес в браузере. Замените **CLUSTERNAME** именем кластера:
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. Щелкните **Создать**, а затем — **Pyspark**, **PySpark3** или **Spark**, чтобы создать объект Notebook. Для приложений Scala используйте ядро Spark, для приложений Python2 — ядро PySpark, а для приложений Python3 — ядро PySpark3.
   
    ![Ядра для записной книжки Jupyter в Spark](./media/apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Ядра для записной книжки Jupyter в Spark") 

4. Объект Notebook должен открыться с помощью выбранного ядра.

## <a name="benefits-of-using-the-kernels"></a>Преимущества использования ядер

Ниже приведены некоторые преимущества использования новых ядер для записной книжки Jupyter в кластерах Spark HDInsight.

- **Предустановленные контексты**. Благодаря ядрам **PySpark**, **PySpark3** и **Spark** вам не требуется явно настраивать контексты Spark или Hive перед началом работы с приложением. Они доступны по умолчанию. а именно:
   
   * **sc** для контекста Spark;
   * **sqlContext** для контекста Hive.
   
   Это значит, что для настройки этих контекстов вам не придется выполнять операторы следующего вида:
   
          sc = SparkContext('yarn-client')
          sqlContext = HiveContext(sc)
   
   Вместо этого вы сможете сразу использовать в своем приложении предустановленные контексты.

- **Волшебные команды.** Ядро PySpark предоставляет несколько "магических команд". Это специальные команды, которые можно вызывать с помощью `%%` (например, `%%MAGIC` <args>). Волшебная команда должна быть первым словом в ячейке кода и может состоять из нескольких строк содержимого. Волшебное слово должно быть первым словом в ячейке. Любые другие слова перед магической командой, даже комментарии, приведут к ошибке.     Дополнительные сведения о волшебных командах см. [здесь](http://ipython.readthedocs.org/en/stable/interactive/magics.html).
   
    В следующей таблице перечислены различные магические команды, доступные для ядер.

   | Волшебная команда | Пример | ОПИСАНИЕ |
   | --- | --- | --- |
   | help |`%%help` |Формирует таблицу из всех доступных волшебных слов с примерами и описанием. |
   | info |`%%info` |Выводит сведения о сеансе для текущей конечной точки Livy. |
   | Настройка |`%%configure -f`<br>`{"executorMemory": "1000M"`,<br>`"executorCores": 4`} |Настраивает параметры для создания сеанса. Флаг force (-f) является обязательным, если сеанс уже был создан, иначе сеанс будет удален и создан заново. Список допустимых параметров приведен в разделе, посвященном [тексту запроса сеансов POST Livy](https://github.com/cloudera/livy#request-body) . Параметры должны передаваться в виде строки JSON, следующей после волшебной команды, как показано в столбце примера. |
   | sql |`%%sql -o <variable name>`<br> `SHOW TABLES` |Выполняет запрос Hive к sqlContext. Если передан параметр `-o` , результат запроса сохраняется в контексте Python %%local в качестве таблицы данных [Pandas](http://pandas.pydata.org/) . |
   | local |`%%local`<br>`a=1` |Весь код в последующих строках выполняется локально. В качестве кода должен быть указан допустимый код на языке Python2 (вне зависимости от используемого ядра). Таким образом, даже если при создании объекта Notebook было выбрано ядро **PySpark3** или **Spark**, то при использовании в ячейке магической команды `%%local` в этой ячейке должен содержаться только допустимый код Python2. |
   | журналы |`%%logs` |Выводит журналы для текущего сеанса Livy. |
   | удалить |`%%delete -f -s <session number>` |Удаляет указанный сеанс для текущей конечной точки Livy. Удалить сеанс, который был инициирован самим ядром, невозможно. |
   | cleanup |`%%cleanup -f` |Удаляет все сеансы для текущей конечной точки Livy, включая сеанс этой записной книжки. Флаг -f является обязательным. |

   > [!NOTE]
   > Помимо магических команд, добавленных ядром PySpark, можно также использовать [встроенные магические команды](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics) IPython, в том числе `%%sh`. Можно использовать магическую команду `%%sh` для выполнения сценариев и блоков кода на головном узле кластера.
   >
   >
2. **Автоматическая визуализация**. Ядро **Pyspark** автоматически визуализирует выходные данные запросов Hive и SQL. Вы можете выбрать различные типы средства визуализации, включая таблицы, круговые диаграммы, графики, диаграммы с областями и линейчатые диаграммы.

## <a name="parameters-supported-with-the-sql-magic"></a>Параметры, поддерживаемые волшебной командой %%sql
Магическая команда `%%sql` поддерживает различные параметры, позволяющие управлять результатом выполнения запросов. Возможные результаты показаны в следующей таблице.

| Параметр | Пример | ОПИСАНИЕ |
| --- | --- | --- |
| -o |`-o <VARIABLE NAME>` |При использовании этого параметра результат запроса сохраняется в контексте Python %%local в качестве таблицы данных [Pandas](http://pandas.pydata.org/) . Именем переменной таблицы данных служит указанное вами имя переменной. |
| -q |`-q` |Позволяет отключить визуализации для ячейки. Если вам не нужна автоматическая визуализация содержимого ячейки и вы хотите только записать ее как таблицу данных, используйте параметр `-q -o <VARIABLE>`. Если вы хотите отключить визуализацию, не записывая результаты (например, для выполнения запроса SQL, такого как инструкция `CREATE TABLE`), то используйте параметр `-q` без аргумента `-o`. |
| -m |`-m <METHOD>` |Параметр **METHOD** имеет значение **take** или **sample** (по умолчанию используется значение **take**). Если используется метод **take**, то ядро выбирает элементы из верхней части результирующего набора данных, который определяется параметром MAXROWS (описывается далее в этой таблице). Если используется метод **sample**, то ядро выбирает элементы из набора данных случайным образом в соответствии с параметром `-r`, описанным далее в этой таблице. |
| -r |`-r <FRACTION>` |Здесь **FRACTION** — это число с плавающей запятой от 0,0 до 1,0. Если для SQL-запроса используется метод выборки `sample`, то ядро выбирает заданную долю элементов из результирующего набора случайным образом. Например, при выполнении SQL-запроса с аргументами `-m sample -r 0.01` из результирующего набора данных случайным образом отбирается 1 % строк. |
| -n |`-n <MAXROWS>` |**MAXROWS** должно быть выражено целым числом. Число выходных строк для параметра **MAXROWS** ограничивается ядром. Если значение параметра **MAXROWS** выражено отрицательным числом, например **-1**, то число строк в результирующем наборе не ограничивается. |

**Пример.**

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

Приведенная выше инструкция делает следующее:

* Выбирает все записи из таблицы **hivesampletable**.
* Отключает автоматическую визуализацию, так как включает параметр -q.
* Случайным образом выбирает 10 % строк из таблицы hivesampletable и ограничивает размер результирующего набора 500 строками, так как включает параметр `-m sample -r 0.1 -n 500` .
* И наконец, сохраняет выходные данные в таблицу данных **query2**, так как включает параметр `-o query2`.

## <a name="considerations-while-using-the-new-kernels"></a>Рекомендации по использованию новых ядер

Какое бы ядро вы ни использовали, работающие объекты Notebook потребляют ресурсы кластера.  Используя эти ядра (так как контексты заданы предварительно), при простом выходе из объектов Notebook контекст не завершается, а значит ресурсы кластера продолжают использоваться. После завершения работы с объектом Notebook рекомендуется выбрать **Close and Halt** (Закрыть и остановить) в меню **File** (Файл) объекта Notebook. Это действие завершит контекст и закроет объект Notebook.     

## <a name="show-me-some-examples"></a>Примеры

Открыв объект Jupyter Notebook, вы увидите в корневом каталоге две папки.

* Папка **PySpark** содержит примеры записных книжек, в которых используется новое ядро **Python**.
* Папка **Scala** содержит примеры записных книжек, в которых используется новое ядро **Spark**.

Чтобы получить представление о различных волшебных командах, вы можете открыть записную книжку **00 - [READ ME FIRST] Spark Magic Kernel Features** из каталога **PySpark** или **Spark**. Также можно использовать другие примеры записных книжек в этих каталогах, чтобы узнать, как реализовать различные сценарии с помощью записных книжек Jupyter с кластерами HDInsight Spark.

## <a name="where-are-the-notebooks-stored"></a>Где хранятся записные книжки?

Если кластер HDInsight использует службу хранилища Azure в качестве учетной записи хранения по умолчанию, записные книжки Jupyter сохраняются в папке **/HdiNotebooks** в учетной записи хранения.  Доступ к объектам Notebook, текстовым файлам и папкам, создаваемым в Jupyter, можно получить через учетную запись хранения.  Например, если Jupyter используется для создания папки **myfolder** и объекта Notebook **myfolder/mynotebook.ipynb**, то доступ к этому объекту можно получить в расположении `/HdiNotebooks/myfolder/mynotebook.ipynb` в учетной записи хранения.  Верно и обратное: если вы передаете объект Notebook непосредственно в свою учетную запись хранения в `/HdiNotebooks/mynotebook1.ipynb`, то этот объект также отображается в Jupyter.  Объекты Notebook хранятся в учетной записи хранения даже после удаления кластера.

> [!NOTE]
> Для кластеров HDInsight, использующих Azure Data Lake Store в качестве хранилища по умолчанию, записные книжки не сохраняются в связанном хранилище.
>

Записные книжки сохраняются в учетной записи хранения как в HDFS. Таким образом, подключаясь к кластеру по протоколу SSH, вы можете использовать команды управления файлами, как показано в следующем фрагменте:

    hdfs dfs -ls /HdiNotebooks                               # List everything at the root directory – everything in this directory is visible to Jupyter from the home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download the contents of the HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb to the root folder so it’s visible from Jupyter

Вне зависимости от того, использует кластер службу хранилища Azure или Azure Data Lake Store в качестве учетной записи хранения по умолчанию, записные книжки также сохраняются на головном узле кластера в `/var/lib/jupyter`.

## <a name="supported-browser"></a>Поддерживаемый браузер

Записные книжки Jupyter, выполняемые в кластерах HDInsight Spark, поддерживаются только браузером Google Chrome.

## <a name="feedback"></a>Отзыв
Новые ядра находятся в стадии развития и будут улучшаться со временем. Кроме того, это может означать, что по мере развития этих ядер API могут измениться. Мы будем признательны вам за любые отзывы о работе с новыми ядрами. Ваши комментарии помогут нам оформить финальную версию этих ядер. Отзывы и замечания оставляйте в разделе **Комментарии** под данной статьей.

## <a name="seealso"></a>Дополнительные материалы
* [Обзор: Apache Spark в Azure HDInsight](apache-spark-overview.md)

### <a name="scenarios"></a>Сценарии
* [Использование Spark со средствами бизнес-аналитики. Выполнение интерактивного анализа данных с использованием Spark в HDInsight с помощью средств бизнес-аналитики](apache-spark-use-bi-tools.md)
* [Использование Spark с машинным обучением. Использование Spark в HDInsight для анализа температуры в здании на основе данных системы кондиционирования](apache-spark-ipython-notebook-machine-learning.md)
* [Использование Spark с машинным обучением. Использование Spark в HDInsight для прогнозирования результатов контроля качества пищевых продуктов](apache-spark-machine-learning-mllib-ipython.md)
* [Анализ журнала веб-сайта с использованием Spark в HDInsight](apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a>Создание и запуск приложений
* [Создание автономного приложения с использованием Scala](apache-spark-create-standalone-application.md)
* [Удаленный запуск заданий с помощью Livy в кластере Spark](apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a>Средства и расширения
* [Использование подключаемого модуля средств HDInsight для IntelliJ IDEA для создания и отправки приложений Spark Scala](apache-spark-intellij-tool-plugin.md)
* [Удаленная отладка приложений Spark в кластере HDInsight Spark Linux с помощью подключаемого модуля средств HDInsight для IntelliJ IDEA](apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [Использование записных книжек Zeppelin с кластером Spark в HDInsight](apache-spark-zeppelin-notebook.md)
* [Использование внешних пакетов с записными книжками Jupyter](apache-spark-jupyter-notebook-use-external-packages.md)
* [Установка записной книжки Jupyter на компьютере и ее подключение к кластеру Apache Spark в Azure HDInsight (предварительная версия)](apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a>Управление ресурсами
* [Управление ресурсами кластера Apache Spark в Azure HDInsight](apache-spark-resource-manager.md)
* [Отслеживание и отладка заданий в кластере Apache Spark в HDInsight на платформе Linux](apache-spark-job-debugging.md)
