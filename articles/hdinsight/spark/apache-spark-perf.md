---
title: Оптимизация заданий Spark для повышения производительности в Azure HDInsight | Документация Майкрософт
description: В этой статье описываются распространенные стратегии повышения производительности кластеров Spark.
services: hdinsight
documentationcenter: ''
author: maxluk
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.service: hdinsight
ms.custom: hdinsightactive
ms.devlang: na
ms.topic: article
ms.date: 01/11/2018
ms.author: maxluk
ms.openlocfilehash: f35ed98efb26dfa0d75a57ca3646f567a7949dae
ms.sourcegitcommit: d78bcecd983ca2a7473fff23371c8cfed0d89627
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/14/2018
ms.locfileid: "34164372"
---
# <a name="optimize-spark-jobs"></a>Оптимизация заданий Spark

Узнайте об оптимизации конфигурации кластера Spark для определенной рабочей нагрузки.  Наиболее распространенной проблемой является нехватка памяти. Причинами этой проблемы можно назвать неправильные конфигурации (в частности неправильные размеры исполнителей), длительные операции и задачи, которые приводят к декартовым операциям. Ускорить выполнение заданий можно с помощью соответствующего кэширования и разрешения [неравномерного распределения данных](#optimize-joins-and-shuffles). Для повышения производительности мы рекомендуем отслеживать и просматривать выполнения длительных и ресурсоемких заданий Spark.

В разделах ниже описаны общие рекомендации и распространенные методы оптимизации задания Spark.

## <a name="choose-the-data-abstraction"></a>Выбор абстракции данных

Кластер Spark 1.x использует устойчивые распределенные наборы данных для абстракции данных, а Spark 2.x представляет кадры и наборы данных. Рассмотрим следующие относительные характеристики:

* **Кадры данных**
    * Оптимальный вариант в большинстве случаев.
    * Оптимизация запросов через Catalyst.
    * Комплексное создание кода.
    * Прямой доступ к памяти.
    * Низкий лимит переполнения памяти при сборке мусора.
    * Не настолько удобны для разработчиков, как наборы данных, так как отсутствуют проверки со временем компиляции или программирование на основе объекта домена.
* **Наборы данных**
    * Подходят для использования в сложных конвейерах ETL, где допустимо влияние производительности.
    * Не подходят для использования в статистических функциях, где весомо влияние производительности.
    * Оптимизация запросов через Catalyst.
    * Удобны для разработчиков, так как обеспечивают программирование на основе объекта домена и проверки со временем компиляции.
    * Увеличивают нагрузку при десериализации и сериализации.
    * Высокий лимит переполнения памяти при сборке мусора.
    * Разбивают комплексное создание кода на этапы.
* **Устойчивые распределенные наборы данных (RDD)**
    * В кластере Spark 2.x необязательно использовать наборы RDD, если только вам не нужно создать пользовательский RDD.
    * Отсутствует оптимизация запросов через Catalyst.
    * Отсутствует комплексное создание кода.
    * Высокий лимит переполнения памяти при сборке мусора.
    * Необходимо использовать устаревшие API-интерфейсы Spark 1.x.

## <a name="use-optimal-data-format"></a>Использование оптимального формата данных

Spark поддерживает многие форматы, такие как CSV, JSON, XML, PARQUET, ORC и AVRO. С помощью внешних источников данных его можно расширить для поддержки большего количества форматов. Дополнительные сведения см. на странице [пакетов Spark](https://spark-packages.org).

Лучший формат для повышения производительности — PARQUET со *сжатием Snappy*, который является стандартным форматом в кластере Spark 2.x. В формате PARQUET данные хранятся в столбцах. Этот формат высоко оптимизирован в Spark.

## <a name="select-default-storage"></a>Выбор хранилища по умолчанию

При создании кластера Spark вы можете выбрать хранилище BLOB-объектов Azure или Azure Data Lake Store как хранилище кластера по умолчанию. Оба варианта предоставляют возможность долговременного хранения промежуточных кластеров, что означает, что данные не будут автоматически удалены при удалении кластера. Вы можете повторно создать промежуточный кластер и по-прежнему иметь доступ к данным.

| Тип хранилища данных | Файловая система | Speed | Промежуточный | Варианты использования |
| --- | --- | --- | --- | --- |
| Хранилище больших двоичных объектов Azure | **wasb:**//url/ | **Стандартный** | Yes | Промежуточный кластер |
| Хранилище озера данных Azure | **adl:**//url/ | **Более быстрая** | Yes | Промежуточный кластер |
| Локальная система HDFS | **hdfs:**//url/ | **Самая быстрая** | Нет  | Интерактивный постоянно доступный кластер |

## <a name="use-the-cache"></a>Использование кэша

Spark обеспечивает собственные механизмы кэширования, которые можно использовать с помощью различных методов, например `.persist()`, `.cache()` и `CACHE TABLE`. Такое встроенное кэширование эффективно при работе с небольшими наборами данных, а также в конвейерах ETL, где требуется кэшировать промежуточные результаты. Однако встроенное кэширование Spark в настоящее время не подходит для работы с секционированием, так как в кэшированнной таблице не хранятся секционированные данные. Более универсальным и надежным способом кэширования является *кэширование на уровне хранилища*.

* Встроенное кэширование Spark (не рекомендуется)
    * Подходит для небольших наборов данных.
    * Не подходит для работы с секционированием, но в следующих выпусках Spark эта проблема может быть устранена.

* Кэширование на уровне хранилища (рекомендуется)
    * Можно реализовать с помощью [Alluxio](http://www.alluxio.org/).
    * Использует кэширование SSD и в памяти.

* Локальная система HDFS (рекомендуется)
    * Путь `hdfs://mycluster`.
    * Использует кэширование SSD.
    * Кэшированные данные будут потеряны при удалении кластера, что требует перестроения кэша.

## <a name="use-memory-efficiently"></a>Эффективное использование памяти

Кластер Spark работает путем размещения данных в памяти, поэтому управление ресурсами памяти является ключевым аспектом оптимизации выполнения заданий Spark.  Есть несколько методов, которые можно применить для эффективного использования памяти кластера.

* В рамках стратегии секционирования рекомендуется выбирать небольшие секции данных и учитывать размер данных, типы и распределение.
* Лучше ознакомиться с более новой и эффективной [сериализацией данных Kryo](https://github.com/EsotericSoftware/kryo), а не использовать стандартную сериализацию Java.
* Рекомендуется использовать YARN, так как можно разделить `spark-submit` по пакету.
* Отслеживайте и настраивайте параметры конфигурации Spark.

Для справки структура памяти Spark и некоторые основные параметры памяти исполнителя показаны на рисунке ниже.

### <a name="spark-memory-considerations"></a>Рекомендации по использованию памяти Spark

При использовании YARN эта платформа управляет максимальным объемом памяти, используемой всеми контейнерами на всех узлах Spark.  На схеме ниже показаны ключевые объекты и их связи.

![Управление памятью Spark в YARN](./media/apache-spark-perf/yarn-spark-memory.png)

При получении сообщений о нехватке памяти сделайте следующее:

* Просмотрите операции перемешивания при управлении группами обеспечения доступности баз данных. Ограничьте их путем снижения на стороне сопоставления, выполните предварительное секционирование (или разбиение на группы) исходных данных, увеличьте объем операций перемешивания для отдельных процессов и сократите объем отправляемых данных.
* Выберите `ReduceByKey` с фиксированным объемом памяти, а не `GroupByKey`, который обеспечивает статистические функции, управление окнами и другие возможности, но включает неограниченный объем памяти.
* Выберите `TreeReduce`, который в основном обрабатывает исполнителей или секции, а не `Reduce`, который в основном обрабатывает драйвер.
* Используйте кадры данных, а не объекты устойчивого распределенного набора данных более низкого уровня.
* Создайте типы ComplexTypes, инкапсулирующие действия, такие как "Первые N", различные статистические функции или операции управления окнами.

## <a name="optimize-data-serialization"></a>Оптимизация сериализации данных

Так как задания кластера Spark можно распределить, соответствующая сериализация данных представляет собой важный шаг для повышения производительности.  Есть два варианта сериализации данных Spark:

* Сериализация Java, используемая по умолчанию.
* Сериализация Kryo (новый формат), которая может ускорить сериализацию и сделать ее компактнее по сравнению с сериализацией Java.  При использовании сериализации Kryo необходимо зарегистрировать классы в программе. Пока что она поддерживает не все сериализуемые типы.

## <a name="use-bucketing"></a>Использование группирования

Группирование аналогично секционированию данных, но каждый контейнер может содержать не одно значение столбца, а набор. Группирование подходит для секционирования большого количества значений (миллионов и более), например идентификаторов продукта. Контейнер определяется хэшированием ключа контейнера строки. Таблицы в контейнерах предлагают уникальную оптимизацию, так как в них хранятся метаданные о способах группирования и сортировки.

Ниже приведены некоторые расширенные функции группирования.

* Оптимизация запросов на основе группирования метасведений.
* Оптимизированные статистические функции.
* Оптимизированные соединения.

Вы можете одновременно использовать секционирование и группирование.

## <a name="optimize-joins-and-shuffles"></a>Оптимизация операций соединения и перемешивания

При медленном выполнении заданий в операциях соединения или перемешивания причиной скорее всего является *неравномерное распределение данных*, представляющее собой асимметрию в данных задания. Например, выполнение задания сопоставления может занять 20 секунд, но выполнение задания с объединением или перемешиванием данных может занять несколько часов.   Для устранения неравномерного распределения данных необходимо использовать строку случайных данных для целого ключа или *изолированную строку случайных данных* только для некоторого подмножества ключей.  При использовании изолированной строки случайных данных необходимо далее использовать фильтрацию для изоляции подмножества соответствующих ключей в соединениях сопоставлений. Другой вариант — создать столбец группы и выполнить предварительную статистическую обработку сначала в группах.

На скорость выполнения операции соединения также влияет тип этой операции. По умолчанию кластер Spark использует тип соединения `SortMerge`. Этот тип соединения лучше всего подходит для больших наборов данных, но с точки зрения объема расчетов он достаточно дорогостоящий, так как используется сначала для сортировки левой и правой частей данных, а затем для их объединения.

Соединение `Broadcast` лучше всего подходит для небольших наборов данных, или в случаях, когда одна сторона соединения значительно меньше другой. Этот тип соединения оповещает все исполнители, поэтому в целом требует большего объема памяти для такой операции передачи.

Тип соединения в конфигурации можно изменить, задав `spark.sql.autoBroadcastJoinThreshold`, или можно задать подсказку по соединению с помощью API-интерфейсов DataFrame (`dataframe.join(broadcast(df2))`).

```scala
// Option 1
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 1*1024*1024*1024)

// Option 2
val df1 = spark.table("FactTableA")
val df2 = spark.table("dimMP")
df1.join(broadcast(df2), Seq("PK")).
    createOrReplaceTempView("V_JOIN")
sql("SELECT col1, col2 FROM V_JOIN")
```

Если вы используете таблицы в группах, вам доступен третий тип соединения — соединение `Merge`. В соединении `SortMerge` правильно предварительно секционированный и отсортированный набор данных пропустит дорогостоящий этап сортировки.

Порядок соединений имеет значение, особенно в более сложных запросах. Начните с наиболее часто используемых соединений. Кроме того, по возможности перемещайте соединения, увеличивающие количество строк после статистической обработки.

Чтобы управлять параллелизмом (особенно в случае декартовых соединений), можно добавить вложенные структуры, управление окнами и, вероятно, пропустить один или несколько шагов в задании Spark.

## <a name="customize-cluster-configuration"></a>Настройка конфигурации кластера

В зависимости от рабочей нагрузки кластера Spark может выясниться, что нестандартная конфигурация Spark оптимизирует выполнение задания.  Выполните тестирование производительности примеров рабочих нагрузок, чтобы проверить все нестандартные конфигурации кластера.

Ниже приведены некоторые общие параметры, которые можно изменить:

* `--num-executors` задает необходимое количество исполнителей.
* `--executor-cores` задает количество ядер для каждого исполнителя. Обычно используются исполнители среднего размера, так как некоторые процессы используют доступную память.
* `--executor-memory` задает объем памяти каждого исполнителя, который управляет размером кучи в YARN. Необходимо предусмотреть лимит переполнения памяти при выполнения заданий.

### <a name="select-the-correct-executor-size"></a>Выбор правильного размера исполнителя

При выборе конфигурации исполнителя предусмотрите лимит переполнения памяти при сборке мусора Java.

* Факторы, которые стоит учесть, чтобы выбрать исполнитель меньшего размера:
    1. Уменьшите размер кучи до 32 ГБ, чтобы лимит переполнения памяти при сборке мусора не превышал 10 %.
    2. Уменьшите количество ядер, чтобы лимит переполнения памяти при сборке мусора не превышал 10 %.

* Факторы, которые стоит учесть, чтобы выбрать исполнитель большего размера:
    1. Уменьшите лимит переполнения памяти при обмене данными между исполнителями.
    2. Уменьшите количество открытых подключений между исполнителями (N2) в больших кластерах (более 100 исполнителей).
    3. Увеличьте размер кучи для обработки задач с интенсивным потреблением ресурсов памяти.
    4. (Необязательно.) Уменьшите лимит переполнения памяти каждого исполнителя.
    5. (Необязательно.) Увеличьте использование и параллелизм, увеличив число назначенных ЦП.

Как показывает опыт при выборе размера исполнителя стоит руководствоваться следующим:
    
1. Начните с 30 ГБ на каждый исполнитель и распределите доступные ядра компьютера.
2. Увеличьте количество ядер исполнителя для больших кластеров (более 100 исполнителей).
3. Увеличьте или уменьшите размеры в зависимости от пробных запусков и предшествующих факторов, например лимита переполнения памяти при сборке мусора.

При выполнении параллельных запросов учтите следующее:

1. Начните с 30 ГБ на каждый исполнитель и для всех ядер компьютера.
2. Создайте несколько параллельных приложений Spark, увеличив число назначенных ЦП (уменьшение задержки приблизительно на 30 %).
3. Распределите запросы между параллельными приложениями.
4. Увеличьте или уменьшите размеры в зависимости от пробных запусков и предшествующих факторов, например лимита переполнения памяти при сборке мусора.

Отслеживайте производительность запросов для выбросов или других проблем с производительностью за счет поиска в представлении временной шкалы, SQL Graph, статистике задания и т. д. Иногда один или несколько исполнителей работают медленнее по сравнению с другими, и выполнение задач занимает гораздо больше времени. Это часто происходит в больших кластерах (более 30 узлов). В этом случае разделите работу на большое число задач, чтобы планировщик мог компенсировать их медленное выполнение. Например, создайте как минимум в два раза больше задач по сравнению с количеством ядер исполнителя в приложении. Можно также включить упреждающее выполнение задач с помощью `conf: spark.speculation = true`.

## <a name="optimize-job-execution"></a>Оптимизация выполнения задания

* При необходимости выполните кэширование, например при повторном использовании данных.
* Передайте переменные во все исполнители. Переменные сериализуются только один раз, за счет чего поиск ускоряется.
* Используйте пул потока в драйвере, что ускорит выполнение нескольких задач.

Регулярно отслеживайте выполняющиеся задания для обнаружения проблем с производительностью. Если необходимо получить дополнительные сведения об определенных проблемах, ознакомьтесь с одним из следующих инструментов профилирования.

* [Инструмент Intel PAL](https://github.com/intel-hadoop/PAT) — отслеживает использование пропускной способности ЦП, хранилища и сети.
* [Oracle Java 8 Mission Control](http://www.oracle.com/technetwork/java/javaseproducts/mission-control/java-mission-control-1998576.html) — формирует профили Spark и кода исполнителя.

Ключевым аспектом производительности запроса Spark 2.x является механизм Tungsten, который зависит от комплексного создания кода. В некоторых случаях комплексное создание кода можно отключить. Например, если в статистическом выражении используется неизменяемый тип (`string`), вместо `HashAggregate` появится `SortAggregate`. Например, для повышения производительности запустите команду ниже, а затем повторно включите создание кода:

```sql
MAX(AMOUNT) -> MAX(cast(AMOUNT as DOUBLE))
```

## <a name="next-steps"></a>Дополнительная информация

* [Debug Apache Spark jobs running on Azure HDInsight](apache-spark-job-debugging.md) (Отладка заданий Apache Spark, запущенных в Azure HDInsight)
* [Управление ресурсами для кластера Apache Spark в Azure HDInsight](apache-spark-resource-manager.md)
* [Use Apache Spark REST API to submit remote jobs to an HDInsight Spark cluster](apache-spark-livy-rest-interface.md) (Использование REST API Apache Spark для отправки удаленных заданий в кластер Spark HDInsight)
* [Tuning Spark](https://spark.apache.org/docs/latest/tuning.html) (Настройка Spark)
* [How to Actually Tune Your Spark Jobs So They Work](https://www.slideshare.net/ilganeli/how-to-actually-tune-your-spark-jobs-so-they-work) (О настройке заданий Spark для их оптимальной работы)
* [Kryo Serialization](https://github.com/EsotericSoftware/kryo) (Сериализация Kryo)
