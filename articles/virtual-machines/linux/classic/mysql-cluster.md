---
title: Кластеризация MySQL с помощью наборов балансировки нагрузки | Документация Майкрософт
description: Настройка высокодоступного кластера Linux MySQL с балансировкой нагрузки, созданного с помощью классической модели развертывания в Azure
services: virtual-machines-linux
documentationcenter: ''
author: bureado
manager: jeconnoc
editor: ''
tags: azure-service-management
ms.assetid: 6c413a16-e9b5-4ffe-a8a3-ae67046bbdf3
ms.service: virtual-machines-linux
ms.workload: infrastructure-services
ms.tgt_pltfrm: vm-linux
ms.devlang: na
ms.topic: article
ms.date: 04/14/2015
ms.author: jparrel
ms.openlocfilehash: e2671def47879e3d4eae000c9084cd458e29b933
ms.sourcegitcommit: 6fcd9e220b9cd4cb2d4365de0299bf48fbb18c17
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/05/2018
ms.locfileid: "30841270"
---
# <a name="use-load-balanced-sets-to-clusterize-mysql-on-linux"></a>Использование наборов балансировки нагрузки для кластеризации MySQL в Linux
> [!IMPORTANT]
> В Azure предлагаются две модели развертывания для создания ресурсов и работы с ними: [модель Azure Resource Manager](../../../resource-manager-deployment-model.md) и классическая модель. В этой статье рассматривается использование классической модели развертывания. Для большинства новых развертываний Майкрософт рекомендует использовать модель диспетчера ресурсов. Если вам необходимо развернуть кластер MySQL, можно использовать [шаблон Resource Manager](https://azure.microsoft.com/documentation/templates/mysql-replication/).
> [!INCLUDE [virtual-machines-common-classic-createportal](../../../../includes/virtual-machines-classic-portal.md)]

В этой статье исследуются и демонстрируются разные возможные подходы к развертыванию высокодоступных служб на основе Linux в Microsoft Azure на примере MySQL Server. Видеоролик, демонстрирующий этот подход, см. на [канале 9](http://channel9.msdn.com/Blogs/Open/Load-balancing-highly-available-Linux-services-on-Windows-Azure-OpenLDAP-and-MySQL).

Мы рассмотрим не использующее общие ресурсы высокодоступное решение MySQL с двумя узлами и одним владельцем на основе DRBD, Corosync и Pacemaker. Одновременно MySQL работает только в одном узле. Операции чтения и записи из ресурса DRBD в каждый момент времени также возможны только в одном узле.

В данном случае не требуется решение с применением виртуального IP-адреса, например LVS, так как будут использоваться наборы балансировки нагрузки в Microsoft Azure, чтобы обеспечить функциональность виртуального IP-адреса для циклического перебора, обнаружения конечной точки, удаления и восстановления после ошибки. Виртуальный IP-адрес — это IPv4-адрес с поддержкой глобальной маршрутизации, назначаемый Microsoft Azure при первом создании облачной службы.

Существуют и другие доступные архитектуры для MySQL, в том числе NBD Cluster, Percona и Galera, а также несколько решений ПО промежуточного слоя, включая по крайней мере одно, доступное в качестве виртуальной машины на сайте [VM Depot](http://vmdepot.msopentech.com). Поскольку эти решения могут реплицироваться в одноадресной (в отличие от многоадресной и широковещательной) передаче и не зависят от общего хранилища и множества сетевых интерфейсов, такие сценарии должны легко развертываться в Microsoft Azure.

Эти архитектуры кластеризации можно аналогичным образом распространить и на другие продукты, такие как PostgreSQL и OpenLDAP. Например, данная процедура балансировки нагрузки без использования общих ресурсов была успешно протестирована с OpenLDAP с несколькими хозяевами, и это можно увидеть в нашем блоге "Канал 9".

## <a name="get-ready"></a>Подготовка
Необходимы следующие ресурсы и возможности:

  - учетная запись Microsoft Azure с действующей подпиской, позволяющая создать по крайней мере две виртуальные машины (в данном примере использовался тип XS);
  - сеть и подсеть;
  - территориальная группа;
  - группа доступности;
  - возможность создания виртуальных жестких дисков в одном регионе с облачной службой и присоединения их к виртуальным машинам с Linux.

### <a name="tested-environment"></a>Тестовая среда
* Ubuntu 13.10
  * DRBD
  * MySQL Server
  * Corosync и Pacemaker

### <a name="affinity-group"></a>Территориальная группа
Создайте территориальную группу для этого решения. Для этого войдите на портал Azure, перейдите в раздел **настроек** и создайте в нем территориальную группу. Этой территориальной группе будут назначаться выделенные ресурсы, созданные позднее.

### <a name="networks"></a>Сети
Создается новая сеть, и в ней создается подсеть. В этом примере используется сеть 10.10.10.0/24 только с одной подсетью /24.

### <a name="virtual-machines"></a>Виртуальные машины
Первая виртуальная машина Ubuntu 13.10 создается с помощью коллекции рекомендованных образов Ubuntu и получает имя `hadb01`. В процессе создается новая облачная служба с именем hadb. Это имя дает понять, что после добавления дополнительных ресурсов эта служба станет общедоступной службой с балансировкой нагрузки. Виртуальная машина `hadb01` создается обычным образом с помощью портала. Автоматически создается конечная точка для SSH и выбирается новая сеть. Теперь вы можете создать группу доступности для виртуальных машин.

После создания первой виртуальной машины (технически при создании облачной службы) создайте вторую виртуальную машину с именем `hadb02`. Вторую виртуальную машину также создайте на портале с помощью образа Ubuntu 13.10 из коллекции, но выберите не создание новой облачной службы, а использование уже существующей `hadb.cloudapp.net`. Сеть и группа доступности должны выбираться автоматически. Также будет создана конечная точка SSH.

После создания обеих виртуальных машин обратите внимание на порт SSH для `hadb01` (TCP 22) и `hadb02` (автоматически назначенный Azure).

### <a name="attached-storage"></a>Подключаемое хранилище
Подключите новый диск к обеим виртуальным машинам и создайте в процессе новые диски объемом в 5 ГБ. Эти диски размещаются в контейнере виртуальных жестких дисков, который используется для главных дисков операционной системы. После создания и подключения дисков не требуется перезапускать Linux, так как ядро увидит новое устройство. Обычно это `/dev/sdc`. Результат можно проверить в `dmesg`.

Затем создайте новые разделы с помощью `cfdisk` (в первую очередь раздел Linux) на каждой виртуальной машине и запишите новую таблицу разделов. Файловую систему в этом разделе создавать не нужно.

## <a name="set-up-the-cluster"></a>Настройка кластера
Для установки Corosync, Pacemaker и DRBD на обеих виртуальных машинах Ubuntu используйте APT. Чтобы воспользоваться командой `apt-get`, выполните следующий код:

    sudo apt-get install corosync pacemaker drbd8-utils.

Откажитесь от установки MySQL в это время. Скрипты установки Debian и Ubuntu будут инициализировать каталог данных MySQL в `/var/lib/mysql`, но так как этот каталог будет заменен файловой системой DRBD, вам необходимо установить MySQL позже.

Убедитесь (с помощью `/sbin/ifconfig`), что обе виртуальные машины используют адреса в подсети 10.10.10.0/24 и могут проверять связь друг с другом по имени. Можно также убедиться (с помощью `ssh-keygen` и `ssh-copy-id`), что обе виртуальные машины могут взаимодействовать через SSH, не требуя пароля.

### <a name="set-up-drbd"></a>Настройка DRBD
Создайте ресурс DRBD, использующий базовый раздел `/dev/sdc1` для создания ресурса `/dev/drbd1`, который можно форматировать с помощью ext3 и использовать как в основном, так и в дополнительном узлах.

1. На обеих виртуальных машинах откройте `/etc/drbd.d/r0.res` и скопируйте туда следующее определение ресурса.

        resource r0 {
          on `hadb01` {
            device  /dev/drbd1;
            disk   /dev/sdc1;
            address  10.10.10.4:7789;
            meta-disk internal;
          }
          on `hadb02` {
            device  /dev/drbd1;
            disk   /dev/sdc1;
            address  10.10.10.5:7789;
            meta-disk internal;
          }
        }

2. На обеих виртуальных машинах инициализируйте ресурсы с помощью `drbdadm`.

        sudo drbdadm -c /etc/drbd.conf role r0
        sudo drbdadm up r0

3. На основной виртуальной машине (`hadb01`) установите владение этим ресурсом DRBD.

        sudo drbdadm primary --force r0

Если проверить содержимое /proc/drbd (`sudo cat /proc/drbd`) на обеих виртуальных машинах, то там должно быть указано `Primary/Secondary` на `hadb01` и `Secondary/Primary` на `hadb02`, в соответствии с решением на этот момент. Диск в 5 ГБ синхронизируется в сети 10.10.10.0/24 бесплатно.

После синхронизации диска можно создать файловую систему на `hadb01`. При тестировании мы использовали файловую систему ext2, но следующий код создает файловую систему ext3.

    mkfs.ext3 /dev/drbd1

### <a name="mount-the-drbd-resource"></a>Подключение ресурса DRBD
Теперь на `hadb01` все готово для подключения ресурсов DRBD. В Debian и производных дистрибутивах в качестве каталога данных MySQL используется `/var/lib/mysql` . Так как вы еще не установили MySQL, создайте этот каталог и подключите ресурс DRBD. Для этого на `hadb01` выполните следующий код:

    sudo mkdir /var/lib/mysql
    sudo mount /dev/drbd1 /var/lib/mysql

## <a name="set-up-mysql"></a>Настройка MySQL
Теперь можно приступить к установке MySQL в `hadb01`:

    sudo apt-get install mysql-server

Для `hadb02`существует два варианта. Вы можете установить MySQL Server, который создаст /var/lib/mysql и включит туда новый каталог данных, а затем удалить содержимое. Для этого на `hadb02` выполните следующий код:

    sudo apt-get install mysql-server
    sudo service mysql stop
    sudo rm –rf /var/lib/mysql/*

Второй вариант заключается в том, чтобы выполнить переход на `hadb02` и установить там MySQL Server. Скрипты установки заметят существующую установку и не будут ее затрагивать.

На `hadb01` выполните следующий код:

    sudo drbdadm secondary –force r0

На `hadb02` выполните следующий код:

    sudo drbdadm primary –force r0
    sudo apt-get install mysql-server

Если в текущий момент переход на другой ресурс не планируется, то первый вариант более простой, хотя, возможно, менее красивый. После выполнения этой установки можно начинать работу с базой данных MySQL. На `hadb02` (или на виртуальной машине, где действует один из серверов в соответствии с DRBD) выполните следующий код:

    mysql –u root –p
    CREATE DATABASE azureha;
    CREATE TABLE things ( id SERIAL, name VARCHAR(255) );
    INSERT INTO things VALUES (1, "Yet another entity");
    GRANT ALL ON things.\* TO root;

> [!WARNING]
> Последняя инструкция фактически отключает аутентификацию привилегированного пользователя в этой таблице. Она включена только для иллюстрации, и в инструкциях GRANT для рабочей среды это необходимо изменить.

Если планируется выполнять запросы извне виртуальных машин (что является целью данного руководства), вам также необходимо включить сеть для MySQL. На обеих виртуальных машинах откройте `/etc/mysql/my.cnf` и перейдите к свойству `bind-address`. Измените адрес с 127.0.0.1 на 0.0.0.0. Сохранив этот файл, вызовите `sudo service mysql restart` в текущей основной виртуальной машине.

### <a name="create-the-mysql-load-balanced-set"></a>Создание набора балансировки нагрузки MySQL
Вернитесь на портал, перейдите на `hadb01` и выберите **Конечные точки**. Чтобы создать конечную точку, выберите в раскрывающемся списке MySQL (TCP 3306) и выберите **Create new load balanced set** (Создать новый набор балансировки нагрузки). Присвойте конечной точке балансировки нагрузки имя `lb-mysql`. Установите для параметра **Время** 5 секунд (не менее 5 секунд).

После создания конечной точки перейдите на `hadb02`, выберите **Конечные точки** и создайте конечную точку. Выберите `lb-mysql`, а затем в раскрывающемся списке выберите MySQL. Для этого действия можно также использовать Azure CLI.

Теперь у вас есть все, что требуется для работы с кластером вручную.

### <a name="test-the-load-balanced-set"></a>Тестирование набора балансировки нагрузки
Тестирование можно выполнять с внешнего компьютера с помощью клиента MySQL или с использованием приложений, например приложения phpMyAdmin, работающего в качестве веб-сайта Azure. В этом случае использовалась программа командной строки MySQL, запущенная на другом компьютере Linux.

    mysql azureha –u root –h hadb.cloudapp.net –e "select * from things;"

### <a name="manually-failing-over"></a>Переход на другой ресурс вручную
Вы можете смоделировать отработку отказа, завершив работу MySQL, переключившись на владельца DRBD и запустив MySQL снова.

Для этого на hadb01 выполните следующий код:

    service mysql stop && umount /var/lib/mysql ; drbdadm secondary r0

Затем на hadb02:

    drbdadm primary r0 ; mount /dev/drbd1 /var/lib/mysql && service mysql start

После выполнения перехода на другой ресурс вручную вы можете повторить удаленный запрос, и он должен работать без проблем.

## <a name="set-up-corosync"></a>Установка Corosync
Corosync — это базовая кластерная инфраструктура, необходимая для работы Pacemaker. Для пользователей Heartbeat (а также других методологий, например Ultramonkey) Corosync — это ответвление функциональных возможностей CRM, в то время как функциональность Pacemaker остается более похожей на Heartbeat.

Основное ограничение для Corosync в Azure заключается в том, что в Corosync многоадресные взаимодействия предпочтительнее одноадресных и широковещательных однако в сети Microsoft Azure предусмотрена поддержка только одноадресных взаимодействий.

К счастью, в Corosync имеется рабочий одноадресный режим. Единственное фактическое ограничение заключается в том, что, так как все узлы не взаимодействуют между собой, необходимо задавать узлы в файлах конфигурации вместе с их IP-адресами. Вы можете воспользоваться примерами файлов Corosync для Unicast и просто изменить адрес привязки, списки узлов и каталоги для ведения журнала (в Ubuntu используется `/var/log/corosync`, а в примерах файлов — `/var/log/cluster`), а также инструменты включения кворума.

> [!NOTE]
> Используйте директиву `transport: udpu`, указанную ниже, и заданные вручную IP-адреса для двух узлов.

На `/etc/corosync/corosync.conf` выполните следующий код для двух узлов:

    totem {
      version: 2
      crypto_cipher: none
      crypto_hash: none
      interface {
        ringnumber: 0
        bindnetaddr: 10.10.10.0
        mcastport: 5405
        ttl: 1
      }
      transport: udpu
    }

    logging {
      fileline: off
      to_logfile: yes
      to_syslog: yes
      logfile: /var/log/corosync/corosync.log
      debug: off
      timestamp: on
      logger_subsys {
        subsys: QUORUM
        debug: off
        }
      }

    nodelist {
      node {
        ring0_addr: 10.10.10.4
        nodeid: 1
      }

      node {
        ring0_addr: 10.10.10.5
        nodeid: 2
      }
    }

    quorum {
      provider: corosync_votequorum
    }

Скопируйте этот файл конфигурации на обе виртуальные машины и запустите Corosync в обоих узлах.

    sudo service start corosync

Вскоре после запуска службы должен быть установлен кластер в текущем кольце и построен кворум. Эту функциональность можно проверить, просмотрев журналы или выполнив следующий код:

    sudo corosync-quorumtool –l

Должен появиться результат, аналогичный приведенному ниже.

![corosync-quorumtool -l sample output](./media/mysql-cluster/image001.png)

## <a name="set-up-pacemaker"></a>Установка Pacemaker
Pacemaker использует кластер для мониторинга ресурсов, обнаружения выхода из строя основных ресурсов и переключения на дополнительные ресурсы. Ресурсы можно задавать с помощью ряда доступных скриптов или с помощью скриптов LSB (подобных init), а также другими способами.

Нам нужно, чтобы Pacemaker "владел" ресурсом DRBD, точкой подключения и службой MySQL. Если Pacemaker может включать и выключать, подключать и отключать DRBD, а также запускать и останавливать MySQL в правильном порядке при возникновении аварийной ситуации, настройка завершена.

При первой установке Pacemaker конфигурация должна быть довольно простой, как показано ниже:

    node $id="1" hadb01
      attributes standby="off"
    node $id="2" hadb02
      attributes standby="off"

1. Проверьте конфигурацию, выполнив команду `sudo crm configure show`.
2. Затем создайте файл (например, `/tmp/cluster.conf`) со следующими ресурсами:

        primitive drbd_mysql ocf:linbit:drbd \
              params drbd_resource="r0" \
              op monitor interval="29s" role="Master" \
              op monitor interval="31s" role="Slave"

        ms ms_drbd_mysql drbd_mysql \
              meta master-max="1" master-node-max="1" \
                clone-max="2" clone-node-max="1" \
                notify="true"

        primitive fs_mysql ocf:heartbeat:Filesystem \
              params device="/dev/drbd/by-res/r0" \
              directory="/var/lib/mysql" fstype="ext3"

        primitive mysqld lsb:mysql

        group mysql fs_mysql mysqld

        colocation mysql_on_drbd \
               inf: mysql ms_drbd_mysql:Master

        order mysql_after_drbd \
               inf: ms_drbd_mysql:promote mysql:start

        property stonith-enabled=false

        property no-quorum-policy=ignore

3. Загрузите файл в конфигурацию. Необходимо сделать это только в одном узле.

        sudo crm configure
          load update /tmp/cluster.conf
          commit
          exit

4. Убедитесь, что Pacemaker запускается при загрузке в обоих узлах.

        sudo update-rc.d pacemaker defaults

5. Используя `sudo crm_mon –L`, убедитесь, что один из узлов стал основным в кластере и на нем запущены все ресурсы. Для проверки запущенных ресурсов можно использовать mount и ps.

На следующем снимке экрана показан `crm_mon` с одним остановленным узлом (выйдите с помощью клавиш CTRL+C).

![crm_mon node stopped](./media/mysql-cluster/image002.png)

На следующем снимке экрана показаны оба узла, один из которых главный, а второй подчиненный.

![crm_mon operational master/slave](./media/mysql-cluster/image003.png)

## <a name="testing"></a>Тестирование
Теперь все готово для моделирования автоматической отработки отказа. Это можно осуществить двумя способами: мягким и жестким.

Суть мягкого способа заключается в завершении работы кластера с помощью ``crm_standby -U `uname -n` -v on``. Если этот процесс осуществляется на главном узле, подчиненный узел становится главным. Не забудьте восстановить его прежнее состояние с помощью параметра off. Если вы этого не сделаете, crm_mon будет сообщать, что один из узлов находится в резервном режиме.

Жесткий способ заключается в остановке работы основной ВМ (hadb01) на портале или в изменении уровня запуска (runlevel) на ВМ (например, halt, shutdown). Таким образом, в Corosync и Pacemaker поступает сигнал о том, что уровень главного узла понижен. Это можно протестировать (что имеет смысл для периодов обслуживания), но можно также принудительно вызвать этот сценарий, заморозив виртуальную машину.

## <a name="stonith"></a>STONITH
Должна присутствовать возможность остановки работы виртуальной машины с помощью Azure CLI вместо сценария STONITH, который управляет физическим устройством. Можно взять `/usr/lib/stonith/plugins/external/ssh` за основу и включить STONITH в конфигурации кластера. Необходима установка Azure CLI на глобальном уровне и загрузка параметров и профиля публикации для пользователя кластера.

Образец кода для ресурса можно найти на сайте [GitHub](https://github.com/bureado/aztonith). Измените конфигурацию кластера, добавив в `sudo crm configure` следующее:

    primitive st-azure stonith:external/azure \
      params hostlist="hadb01 hadb02" \
      clone fencing st-azure \
      property stonith-enabled=true \
      commit

> [!NOTE]
> Этот сценарий не выполняет проверки состояния работоспособности. Исходный ресурс SSH имел 15 проверок связи, но время восстановления для виртуальных машин Azure может варьироваться в более широких пределах.

## <a name="limitations"></a>Ограничения
Действительны следующие ограничения.

* Скрипт linbit ресурса DRBD, управляющий DRBD как ресурсом в Pacemaker, использует `drbdadm down` при остановке работы узла, даже если узел просто переходит в режим ожидания. Это не лучший вариант, так как подчиненный узел не будет синхронизировать ресурс DRBD при выполнении операций записи в главном узле. Если вовремя не произойдет сбой главного узла, подчиненный может принять на себя более старое состояние файловой системы. Имеется два способа разрешения этой проблемы:
  * принудительное выполнение `drbdadm up r0` во всех узлах кластера с помощью локальной (не кластерной) программы наблюдения watchdog;
  * изменение скрипта linbit DRBD таким образом, чтобы в `/usr/lib/ocf/resource.d/linbit/drbd` не вызывалась команда `down`.
* Балансировщику нагрузки требуется не менее пяти секунд, чтобы ответить, поэтому приложения должны быть кластерными и более устойчивыми к времени ожидания. Другие архитектуры, например очереди в приложении и ПО промежуточного слоя запроса, также могут быть полезными.
* Настройка MySQL необходима, чтобы гарантировать выполнение операций записи в разумном темпе и запись кэша на диск с максимально возможной частотой для сокращения потерь памяти.
* Производительность операций записи зависит от межсоединения виртуальной машины в виртуальном коммутаторе, так как это механизм, используемый DRBD для репликации устройства.
