---
title: 'Выходные данные Stream Analytics: возможности хранения и анализа | Документация Майкрософт'
description: Узнайте о вариантах вывода данных Stream Analytics, включая данные Power BI, для получения результатов анализа.
keywords: преобразование данных, результаты анализа, параметры хранилища данных
services: stream-analytics,documentdb,sql-database,event-hubs,service-bus,storage
documentationcenter: ''
author: SnehaGunda
manager: kfile
ms.assetid: ba6697ac-e90f-4be3-bafd-5cfcf4bd8f1f
ms.service: stream-analytics
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: data-services
ms.date: 02/18/2017
ms.author: sngun
ms.openlocfilehash: a641c7e5e792b020be54a2ebc4bac63b545ce71e
ms.sourcegitcommit: 34e0b4a7427f9d2a74164a18c3063c8be967b194
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/30/2018
---
# <a name="stream-analytics-outputs-options-for-storage-analysis"></a>Выходные данные Stream Analytics: возможности хранения и анализа
Создавая задание Stream Analytics, необходимо учитывать, каким образом будут использоваться полученные данные. Каким образом будут просматриваться результаты задания Stream Analytics и где они будут храниться

Для работы с различными шаблонами приложений служба Azure Stream Analytics предлагает различные параметры хранения выходных данных и просмотра результатов анализа. Это позволяет легко просматривать выходные данные задания, гибко их применять, отправлять в хранилище и использовать в других целях. Любой выход, настраиваемый в задании, должен существовать до момента запуска задания и начала потока событий. Например, при выводе данных в хранилище BLOB-объектов задание не создаст учетную запись хранения автоматически. Перед запуском задания Stream Analytics создайте учетную запись хранения.

## <a name="azure-data-lake-store"></a>Хранилище озера данных Azure
Stream Analytics поддерживает [хранилище озера данных Azure](https://azure.microsoft.com/services/data-lake-store/). Это хранилище позволяет сохранять данные с любым размером, типом и скоростью приема в одном месте для эксплуатационной и исследовательской аналитики. Кроме того, Stream Analytics необходимо разрешение на доступ к хранилищу данных озера. Сведения об авторизации и о том, как зарегистрироваться для получения Data Lake Store (при необходимости), приведены в [статье о выходных данных Data Lake](stream-analytics-data-lake-output.md).

### <a name="authorize-an-azure-data-lake-store"></a>Авторизация Azure Data Lake Store
Если для хранения выходных данных на портале Azure выбрано Data Lake Store, вам будет предложено авторизовать подключение к существующей службе Data Lake Store.  

![Авторизация хранилища озера данных](./media/stream-analytics-define-outputs/06-stream-analytics-define-outputs.png)  

Затем заполните свойства выходных данных Data Lake Store, как показано ниже.

![Авторизация хранилища озера данных](./media/stream-analytics-define-outputs/07-stream-analytics-define-outputs.png)  

В таблице ниже приведены имена и описание свойств, необходимых для создания выходных данных хранилища озера данных.

<table>
<tbody>
<tr>
<td><B>Имя свойства</B></td>
<td><B>Описание</B></td>
</tr>
<tr>
<td>Псевдоним выходных данных</td>
<td>Понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующее хранилище озера данных.</td>
</tr>
<tr>
<td>Имя учетной записи</td>
<td>Имя учетной записи хранения Data Lake, в которую отправляются выходные данные. Появится раскрывающийся список учетных записей хранилища озера данных, к которым имеет доступ вошедший на портал пользователь.</td>
</tr>
<tr>
<td>Шаблон префикса пути</td>
<td>Имя файла подчиняется следующим правилам: <BR>{Шаблон префикса пути}/schemaHashcode_Guid_Number.extension <BR> <BR>Выходные файлы примера:<BR>Myoutput/20170901/00/45434_gguid_1.csv <BR>Myoutput/20170901/01/45434_gguid_1.csv <BR> <BR>Кроме того ниже приведены ситуации, в которых создается новый файл:<BR>1.Изменения в схеме выходных данных <BR>2.Внешний или внутренний перезапуск задания<BR><BR>Кроме того, если шаблон пути к файлу не содержит символ "/", последний шаблон в пути к файлу будет рассматриваться в качестве префикса имени файла.<BR><BR>Пример:<BR>Для шаблона пути folder1/logs/HH созданный файл может иметь вид folder1/logs/02_134343_gguid_1.csv.</td>
</tr>
<tr>
<td>Формат даты [<I>необязательное свойство</I>]</td>
<td>Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример: ГГГГ/ММ/ДД</td>
</tr>
<tr>
<td>Формат времени [<I>необязательное свойство</I>]</td>
<td>Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ.</td>
</tr>
<tr>
<td>Формат сериализации событий</td>
<td>Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro.</td>
</tr>
<tr>
<td>Кодирование</td>
<td>Если используется формат CSV или JSON, необходимо указать формат кодирования. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8.</td>
</tr>
<tr>
<td>Разделитель</td>
<td>Применяется только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта.</td>
</tr>
<tr>
<td>Формат</td>
<td>Применяется только для сериализации JSON. Вариант "строки-разделители" предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Вариант "массив" означает, что выходные данные будут отформатированы как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. В общем рекомендуется использовать JSON-файл со строками-разделителями, так как для него не требуется никакой специальной обработки. При этом по-прежнему выполняется запись в выходной файл.</td>
</tr>
</tbody>
</table>

### <a name="renew-data-lake-store-authorization"></a>Обновление авторизации хранилища озера данных
Необходимо будет повторно аутентифицировать учетную запись Data Lake Store, если с момента создания задания или последней аутентификации пароль был изменен.

![Авторизация хранилища озера данных](./media/stream-analytics-define-outputs/08-stream-analytics-define-outputs.png)  

## <a name="sql-database"></a>База данных SQL
[База данных SQL Azure](https://azure.microsoft.com/services/sql-database/) может служить местом назначения для выходных реляционных данных, а также для выходных данных приложений, которые зависят от содержимого, размещенного в реляционной базе данных. Задания Stream Analytics будут записывать данные в существующую таблицу в Базе данных SQL Azure.  Схема таблицы должна в точности соответствовать полям и их типам в выходных данных задания. [Хранилище данных SQL Azure](https://azure.microsoft.com/documentation/services/sql-data-warehouse/) также можно задать для выходных данных с помощью параметра вывода базы данных SQL (это функция из предварительной версии). В таблице ниже приведены имена и описание свойств для создания выходных данных Базы данных SQL.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующую базу данных. |
| База данных |Имя базы данных, в которую отправляются выходные данные. |
| Имя сервера |Имя сервера базы данных SQL. |
| Имя пользователя |Имя пользователя, который имеет право на запись в базу данных. |
| Пароль |Пароль для подключения к базе данных. |
| Таблица |Имя таблицы, в которую записываются выходные данные. В имени таблицы учитывается регистр символов, а схема этой таблицы должна точно соответствовать количеству и типам полей в выходных данных задания. |

> [!NOTE]
> В настоящее время для выходных данных задания в Stream Analytics поддерживается предложение базы данных SQL Azure. Однако менее виртуальная машина Azure с SQL Server, к которой подключена база данных, не поддерживается. Это может быть изменено в будущих выпусках.
> 
> 

## <a name="blob-storage"></a>Хранилище BLOB-объектов
Хранилище BLOB-объектов предоставляет экономичное и масштабируемое решение для хранения в облаке больших объемов неструктурированных данных.  Общие сведения о хранилище BLOB-объектов Azure и его использовании см. в статье [Приступая к работе с хранилищем BLOB-объектов Azure с помощью .NET](../storage/blobs/storage-dotnet-how-to-use-blobs.md).

В таблице ниже приведены имена и описание свойств для создания выходных данных в хранилище BLOB-объектов.

<table>
<tbody>
<tr>
<td>Имя свойства</td>
<td>Описание</td>
</tr>
<tr>
<td>Псевдоним выходных данных</td>
<td>Понятное имя, которое используется в запросах для направления выходных данных запроса в хранилище BLOB-объектов.</td>
</tr>
<tr>
<td>Учетная запись хранения</td>
<td>Имя учетной записи хранения, в которую отправляются выходные данные.</td>
</tr>
<tr>
<td>Ключ учетной записи хранения</td>
<td>Секретный ключ, связанный с учетной записью хранения.</td>
</tr>
<tr>
<td>Контейнер хранилища</td>
<td>Контейнеры обеспечивают логическую группировку BLOB-объектов, хранящихся в службе BLOB-объектов Microsoft Azure. При передаче BLOB-объекта в службу BLOB-объектов для него необходимо указать контейнер.</td>
</tr>
<tr>
<td>Шаблон префикса пути (необязательное свойство)</td>
<td>Шаблон пути к файлу, используемый для записи BLOB-объектов в указанном контейнере. <BR> Чтобы указать периодичность записи BLOB-объектов, в шаблоне пути можно использовать один или несколько экземпляров следующих двух переменных: <BR> {date}, {time} <BR> Пример 1: cluster1/logs/{date}/{time} <BR> Пример 2: cluster1/logs/{date} <BR> <BR> Имя файла подчиняется следующим правилам: <BR> {Шаблон префикса пути}/schemaHashcode_Guid_Number.extension <BR> <BR> Выходные файлы примера: <BR> Myoutput/20170901/00/45434_gguid_1.csv <BR> Myoutput/20170901/01/45434_gguid_1.csv <BR> <BR> Кроме того ниже приведены ситуации, в которых создается новый файл: <BR> 1.Размер текущего файла превышает максимально допустимое количество блоков (в настоящее время 50 000) <BR> 2.Изменения в схеме выходных данных <BR> 3.Внешний или внутренний перезапуск задания  </td>
</tr>
<tr>
<td>Формат даты (необязательное свойство)</td>
<td>Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример: ГГГГ/ММ/ДД</td>
</tr>
<tr>
<td>Формат времени (необязательное свойство)</td>
<td>Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ.</td>
</tr>
<tr>
<td>Формат сериализации событий</td>
<td>Формат сериализации для выходных данных.  Поддерживаются форматы JSON, CSV и Avro.</td>
</tr>
<tr>
<td>Кодирование</td>
<td>Если используется формат CSV или JSON, необходимо указать формат кодирования. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8.</td>
</tr>
<tr>
<td>Разделитель</td>
<td>Применяется только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта.</td>
</tr>
<tr>
<td>Формат</td>
<td>Применяется только для сериализации JSON. Вариант "строки-разделители" предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Вариант "массив" означает, что выходные данные будут отформатированы как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. В общем рекомендуется использовать JSON-файл со строками-разделителями, так как для него не требуется никакой специальной обработки. При этом по-прежнему выполняется запись в выходной файл.</td>
</tr>
</tbody>
</table>

При использовании хранилища BLOB-объектов для выходных данных в большом двоичном объекте создается файл в следующих случаях:

* Если файл превышает максимальное число допустимых блоков. Обратите внимание, что предельного числа блоков можно достигнуть, не превышая максимально допустимого размера большого двоичного объекта. Например, при высокой скорости вывода данных в блоке будет большее число байтов, значит, и размер файла будет большим. В случае низкой скорости вывода данных в каждом блоке будет меньше данных, а значит, и размер файла будет меньшим.  
* Если схема изменена в выходных данных, а для формата выходных данных требуется фиксированная схема (CSV или Avro).  
* Задание перезапускается либо извне, либо изнутри.  
* Если запрос полностью секционирован, для каждой секции выходных данных создается файл.  
* Если файл или контейнер учетной записи хранения удален пользователем.  
* Если выходные данные разделены по времени с использованием шаблона префикса пути, новый блок применяется, когда запрос переходит к следующему часу.

## <a name="event-hub"></a>Концентратор событий
[Концентраторы событий](https://azure.microsoft.com/services/event-hubs/) — это высокомасштабируемая служба приема данных о событиях публикации и подписки. Она может принимать миллионы событий в секунду. Концентратор событий может использоваться в качестве места назначения выходных данных, например в случае, когда выходные данные задания Stream Analytics становятся входными данными для другого задания потоковой передачи.

Чтобы настроить потоки данных концентраторов событий как выходные данные, требуется ряд параметров.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в концентратор событий. |
| Пространство имен служебной шины |Пространство имен Service Bus — это контейнер для набора сущностей обмена сообщениями. При создании нового концентратора событий создается также пространство имен служебной шины. |
| Концентратор событий |Имя выходных данных концентратора событий. |
| Имя политики концентратора событий |Политика общего доступа, которую можно создать на вкладке с настройками концентратора событий. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики концентратора событий |Ключ общего доступа, используемый для проверки подлинности при получении доступа к пространству имен служебной шины. |
| Столбец ключа раздела (необязательное свойство) |Этот столбец содержит ключ раздела для выходных данных концентратора событий. |
| Формат сериализации событий |Формат сериализации для выходных данных.  Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование |В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| Разделитель |Применяется только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат |Применяется только для сериализации JSON. Вариант "строки-разделители" предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Вариант "массив" означает, что выходные данные будут отформатированы как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. В общем рекомендуется использовать JSON-файл со строками-разделителями, так как для него не требуется никакой специальной обработки. При этом по-прежнему выполняется запись в выходной файл. |

## <a name="power-bi"></a>Power BI
[Power BI](https://powerbi.microsoft.com/) . Визуализация позволяет создавать панели мониторинга оперативных данных, формировать отчеты и составлять отчетности на основе метрик.

### <a name="authorize-a-power-bi-account"></a>Авторизация учетной записи Power BI
1. Если служба Power BI выбрана в качестве места назначения для выходных данных на портале Azure, вам будет предложено авторизовать существующего пользователя Power BI или создать новую учетную запись Power BI.  
   
   ![Авторизация пользователя Power BI](./media/stream-analytics-define-outputs/01-stream-analytics-define-outputs.png)  
2. Создайте учетную запись (если она еще не создана), а затем щелкните «Авторизовать сейчас».  Появится следующий экран:  
   
   ![Power BI в учетной записи Azure](./media/stream-analytics-define-outputs/02-stream-analytics-define-outputs.png)  
3. На этом этапе укажите рабочую или учебную учетную запись для авторизации выходных данных Power BI. Если вы еще не зарегистрировались в Power BI, выберите параметр «Зарегистрироваться сейчас». Рабочая или учебная учетная запись, которая используется для Power BI, может отличаться от учетной записи в подписке Azure, с помощью которой вы вошли в систему.

### <a name="configure-the-power-bi-output-properties"></a>Настройка свойств выходных данных Power BI
После авторизации учетной записи Power BI можно настроить свойства для выходных данных Power BI. В таблице ниже приведены имена и описание свойств для настройки выходных данных в Power BI.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующее место назначения в Power BI. |
| Рабочая область группы |Чтобы иметь возможность обмениваться данными с другими пользователями Power BI, вы можете выбрать группы в своей учетной записи Power BI или щелкнуть "Моя рабочая область", если не хотите записывать данные в группу.  Для обновления существующей группы требуется повторно выполнить проверку подлинности в службе Power BI. |
| Имя набора данных |Имя набора данных для использования в выходных данных Power BI. |
| Имя таблицы |Имя таблицы в наборе выходных данных Power BI. В настоящее время для вывода выходных данных из заданий Stream Analytics в Power BI можно использовать только одну таблицу в наборе данных. |

Пошаговые инструкции по настройке выходных данных и панели мониторинга Power BI см. в статье [Azure Stream Analytics и Power BI: панель оперативного мониторинга аналитики для визуализации потоковой передачи данных в режиме реального времени](stream-analytics-power-bi-dashboard.md).

> [!NOTE]
> Не создавайте вручную набор данных и таблицу на панели мониторинга Power BI. Они будут автоматически созданы при запуске задания, когда задание начнет вносить выходные данные в Power BI. Обратите внимание: если запрос задания не создает никаких результатов, набор данных и таблица не создаются. Обратите внимание, что если в Power BI уже есть набор данных и таблица с именем, аналогичным указанному в этом задании Stream Analytics, существующие данные будут перезаписаны.
> 
> 

### <a name="schema-creation"></a>Создание схемы
Azure Stream Analytics создает набор данных и таблицу Power BI от имени пользователя, если он еще не создан. Во всех остальных случаях таблица обновляется с использованием новых значений. В настоящее время существует ограничение: в наборе данных может существовать только одна таблица.

### <a name="data-type-conversion-from-stream-analytics-to-power-bi"></a>Преобразование типов данных из Stream Analytics в Power BI
Azure Stream Analytics обновляет модель данных динамически во время выполнения, если меняется схема вывода. Изменяются имена и типы столбцов, а добавление или удаление столбцов отслеживается.

В этой таблице представлено преобразование [типов данных Stream Analytics](https://msdn.microsoft.com/library/azure/dn835065.aspx) в [типы данных Entity Data Model (EDM)](https://powerbi.microsoft.com/documentation/powerbi-developer-walkthrough-push-data/), используемые в Power BI, если набор данных и таблица POWER BI не созданы.


Из Stream Analytics | В Power BI
-----|-----|------------
bigint | Int64
nvarchar(max) | Строка
Datetime | DateTime
float; | Double
Record array | Тип String, постоянное значение IRecord или IArray

### <a name="schema-update"></a>Обновление схемы
Stream Analytics определяет схему модели данных на основе первого набора событий в выходных данных. Затем (при необходимости) схема модели данных обновляется для размещения входящих событий, которые могут не соответствовать исходной схеме.

Следует избегать запроса `SELECT *`, чтобы не произошло динамическое обновление схемы по строкам. Помимо потенциального влияния на производительность, это также может привести к неопределенности значения времени, затраченного на результаты. Необходимо точно выбрать те поля, которые должны отображаться на панели мониторинга Power BI. Кроме того, значения данных должны соответствовать выбранному типу данных.


Предыдущий или текущий | Int64 | Строка | DateTime | Double
-----------------|-------|--------|----------|-------
Int64 | Int64 | Строка | Строка | Double
Double | Double | Строка | Строка | Double
Строка | Строка | Строка | Строка |  | Строка | 
DateTime | Строка | Строка |  DateTime | Строка


### <a name="renew-power-bi-authorization"></a>Повторная авторизация в Power BI
Необходимо повторно выполнить аутентификацию учетной записи Power BI, если с момента создания задания или последней аутентификации пароль был изменен. Если в клиенте Azure Active Directory (AAD) настроена Многофакторная идентификация (MFA), вам также потребуется каждые две недели обновлять авторизацию Power BI. Признаком этой проблемы является отсутствие выходных данных задания и наличие записи «Ошибка проверки подлинности пользователя» в журналах операций:

  ![Ошибка маркера обновления Power BI](./media/stream-analytics-define-outputs/03-stream-analytics-define-outputs.png)  

Чтобы устранить эту проблему, остановите выполнение задания и перейдите к выходным данным Power BI.  Щелкните ссылку "Обновить авторизацию" и перезапустите задание с момента его последней остановки во избежание потери данных.

  ![Обновление авторизации в Power BI](./media/stream-analytics-define-outputs/04-stream-analytics-define-outputs.png)  

## <a name="table-storage"></a>Хранилище таблиц
[Табличное хранилище Azure](../storage/common/storage-introduction.md) отличается высокой степенью доступности и масштабируемости, позволяя приложению автоматически осуществлять масштабирование в соответствии с нуждами пользователя. Табличное хранилище является хранилищем ключей и атрибутов NoSQL корпорации Майкрософт и позволяет работать со структурированными данными с менее жесткими ограничениями схемы. Хранилище таблиц Azure можно использовать для постоянного хранения данных и эффективного их извлечения.

В таблице ниже приведены имена и описание свойств для создания выходных данных в табличном хранилище.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в табличное хранилище. |
| Учетная запись хранения |Имя учетной записи хранения, в которую отправляются выходные данные. |
| Ключ учетной записи хранения |Ключ доступа, связанный с учетной записью хранения. |
| Имя таблицы |Это имя таблицы. Создается таблица, если она еще существует. |
| Ключ раздела |Имя выходного столбца, содержащего ключ раздела. Ключ раздела — это уникальный идентификатор раздела в пределах конкретной таблицы, являющийся первой частью первичного ключа сущности. Это строковое значение размером до 1 КБ. |
| Ключ строки |Имя выходного столбца, содержащего ключ строки. Ключ строки — это уникальный идентификатор сущности внутри конкретного раздела. Он является второй частью первичного ключа сущности. Ключ строки — это строковое значение размером до 1 КБ. |
| Размер пакета |Количество записей в пакетной операции. Для большинства заданий достаточно значения по умолчанию. Дополнительные сведения об изменении этого параметра см. в статье о [спецификациях пакетных операций с таблицами](https://msdn.microsoft.com/library/microsoft.windowsazure.storage.table.tablebatchoperation.aspx). |
 
## <a name="service-bus-queues"></a>Очереди служебной шины
[Очереди служебной шины](https://msdn.microsoft.com/library/azure/hh367516.aspx) доставляют сообщения конкурирующим потребителям по типу FIFO (первым пришел, первым вышел). Обычно получатели принимают и обрабатывают сообщения в порядке их добавления в очередь, и каждое сообщение принимается и обрабатывается только одним потребителем сообщений.

В таблице ниже приведены имена и описание свойств для создания выходных данных в очереди.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в очередь служебной шины. |
| Пространство имен служебной шины |Пространство имен Service Bus — это контейнер для набора сущностей обмена сообщениями. |
| Имя очереди |Имя очереди служебной шины. |
| Имя политики очереди |При создании очереди можно также создать политики общего доступа на вкладке с настройками очереди. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики очереди |Ключ общего доступа, используемый для проверки подлинности при получении доступа к пространству имен служебной шины. |
| Формат сериализации событий |Формат сериализации для выходных данных.  Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование |В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| Разделитель |Применяется только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат |Применяется только для типа JSON. Вариант "строки-разделители" предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Вариант "массив" означает, что выходные данные будут отформатированы как массив объектов JSON. |

## <a name="service-bus-topics"></a>Разделы шины обслуживания
Если очереди служебной шины предоставляют принцип взаимодействия "один к одному" (отправитель с получателем), то [разделы служебной шины](https://msdn.microsoft.com/library/azure/hh367516.aspx) позволяют использовать принцип взаимодействия "один ко многим".

В таблице ниже приведены имена и описание свойств для создания выходных данных в табличном хранилище.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в раздел служебной шины. |
| Пространство имен служебной шины |Пространство имен Service Bus — это контейнер для набора сущностей обмена сообщениями. При создании нового концентратора событий создается также пространство имен служебной шины. |
| Имя раздела |Разделы являются сущностями обмена сообщениями, как концентраторы событий и очереди. Они предназначены для сбора потоков событий с нескольких различных устройств и служб. Созданному разделу присваивается определенное имя. Сообщения, отправленные в раздел, будут недоступны, пока не создана подписка. Поэтому убедитесь, что раздел содержит одну или несколько подписок. |
| Имя политики раздела |При создании раздела можно также создать политики общего доступа на вкладке настройки раздела. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики раздела |Ключ общего доступа, используемый для проверки подлинности при получении доступа к пространству имен служебной шины. |
| Формат сериализации событий |Формат сериализации для выходных данных.  Поддерживаются форматы JSON, CSV и Avro. |
 | Кодирование |Если используется формат CSV или JSON, необходимо указать формат кодирования. В настоящее время единственным поддерживаемым форматом кодирования является UTF-8. |
| Разделитель |Применяется только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |

## <a name="azure-cosmos-db"></a>Azure Cosmos DB
[Azure Cosmos DB](https://azure.microsoft.com/services/documentdb/) — это глобально распределенная многомодельная служба базы данных. Она предоставляет функции неограниченного гибкого масштабирования по всему миру, расширенные возможности выполнения запросов и автоматического индексирования с помощью моделей данных без использования схем, гарантированную низкую задержку, а также ведущие в отрасли полные соглашения об уровне обслуживания.

Ниже приведены имена и описание свойств для создания выходных данных Azure Cosmos DB.

* **Псевдоним выходных данных** — псевдоним для ссылки на эти выходные данные в запросе Stream Analytics.  
* **Имя учетной записи** — имя или универсальный код ресурса (URI) конечной точки учетной записи Cosmos DB.  
* **Ключ учетной записи** — общедоступный ключ доступа к учетной записи Cosmos DB.  
* **База данных** — имя базы данных Cosmos DB.  
* **Шаблон имени коллекции** — имя или шаблон имени для используемых коллекций. Формат имени коллекции можно составить с помощью необязательного маркера {partition}, где разделы начинаются с 0. Далее приведены примеры допустимых входных данных:  
  1\) MyCollection — должна существовать одна коллекция с именем MyCollection;  
  2\) MyCollection{partition} — должны существовать такие коллекции: MyCollection0, MyCollection1, MyCollection2 и т. д.  
* **Partition Key** — необязательно. Требуется, только если в шаблоне имени коллекции используется маркер раздела. Имя поля в выходных событиях, указывающее ключ для разделения выходных данных между коллекциями. Для выходных данных одной коллекции можно использовать любой столбец произвольных выходных данных, например PartitionId.  
* **Document ID** — необязательно. Имя поля в выходных событиях, используемое для указания первичного ключа, на котором основываются операции вставки или обновления.  

## <a name="azure-functions-in-preview"></a>Функции Azure (в предварительной версии)
Функции Azure — это независимая от сервера служба вычислений, которая позволяет выполнять код по требованию без необходимости явно подготавливать или администрировать инфраструктуру. Они позволяют реализовать код, который запускается событиями, возникающими в Azure или сторонних службах.  Эта возможность решения "Функции Azure" реагировать на триггеры упрощает вывод данных Azure Stream Analytics. Этот выходной адаптер позволяет пользователям подключать Stream Analytics к Функциям Azure и запускать сценарий или часть кода в ответ на ряд событий.

Azure Stream Analytics вызывает Функции Azure через триггеры HTTP. Новый адаптер выходных данных функции Azure доступен со следующими настраиваемыми свойствами:

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Приложение-функция |Имя приложения-функции Azure |
| Функция |Имя функции в приложении-функции Azure |
| Максимальный размер пакета |Это свойство может использоваться для задания максимального размера для каждого пакета выходных данных, которые будут отправляться в Функции Azure. По умолчанию это значение равно 256 КБ. |
| Максимальное количество пакетов  |Как видно из названия, это свойство позволяет указать максимальное число событий в каждом пакете, отправляются в службу "Функции Azure". Значение максимального количества пакетов по умолчанию — 100. |
| Ключ |Если нужно использовать службу "Функции Azure" из другой подписки это можно сделать, предоставив ключ для доступа к функции. |

Обратите внимание, когда служба Azure Stream Analytics получает исключение 413 (сущность запроса HTTP слишком большая) из службы "Функции Azure", размер пакетов, отправляемых в службу "Функции Azure", уменьшается. В коде функции Azure это исключение позволяет убедится, что Azure Stream Analytics не отправляет пакеты слишком большого размера. Также убедитесь, что максимальное количество пакетов и размеры значений, используемые в функции, соответствуют значениям, введенным на портале Stream Analytics. 

Кроме того, в ситуации, когда во временном окне не происходит целевое событие, никакой выход не генерируется. В результате функция computeResult не вызывается. Такое поведение согласуется со встроенными оконными агрегатными функциями.


## <a name="get-help"></a>Получение справки
Дополнительную помощь и поддержку вы можете получить на нашем [форуме Azure Stream Analytics](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics)

## <a name="next-steps"></a>Дополнительная информация
Вы получили основные сведения о Stream Analytics, управляемой службе аналитики потоковой передачи данных из Интернета вещей. Дополнительные сведения об этой службе см. на следующих ресурсах:

* [Приступая к работе с Azure Stream Analytics](stream-analytics-real-time-fraud-detection.md)
* [Масштабирование заданий в службе Azure Stream Analytics](stream-analytics-scale-jobs.md)
* [Справочник по языку запросов Azure Stream Analytics](https://msdn.microsoft.com/library/azure/dn834998.aspx)
* [Справочник по API-интерфейсу REST управления Stream Analytics](https://msdn.microsoft.com/library/azure/dn835031.aspx)

<!--Link references-->
[stream.analytics.developer.guide]: ../stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-real-time-fraud-detection.md
[stream.analytics.query.language.reference]: http://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.rest.api.reference]: http://go.microsoft.com/fwlink/?LinkId=517301
