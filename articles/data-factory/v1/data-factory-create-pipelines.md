---
title: Создание конвейеров, цепочки действий и расписаний для них в фабрике данных | Документация Майкрософт
description: Узнайте, как создать конвейер данных в фабрике данных Azure для перемещения и преобразования данных. Создание управляемого данными рабочего процесса, который предоставит готовую к использованию информацию.
services: data-factory
documentationcenter: ''
author: sharonlo101
manager: craigg
ms.assetid: 13b137c7-1033-406f-aea7-b66f25b313c0
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: conceptual
ms.date: 01/10/2018
ms.author: shlo
robots: noindex
ms.openlocfilehash: f80a22c39608a9d9c67977f2d0493af7300f475b
ms.sourcegitcommit: 266fe4c2216c0420e415d733cd3abbf94994533d
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 06/01/2018
ms.locfileid: "34622712"
---
# <a name="pipelines-and-activities-in-azure-data-factory"></a>Конвейеры и действия в фабрике данных Azure
> [!div class="op_single_selector" title1="Select the version of Data Factory service you are using:"]
> * [Версия 1 — общедоступная](data-factory-create-pipelines.md)
> * [Версия 2 — предварительная](../concepts-pipelines-activities.md)

> [!NOTE]
> Статья относится к версии 1 фабрики данных, которая является общедоступной версией. Если вы используете версию 2 службы фабрики данных, которая находится на этапе предварительной версии, см. статью [Конвейеры и действия в фабрике данных Azure](../concepts-pipelines-activities.md).

В этой статье вы ознакомитесь с принципом работы конвейеров и действий в фабрике данных Azure и узнаете, как с их помощью создавать комплексные рабочие процессы, управляемые данными, для сценариев перемещения и обработки данных.  

> [!NOTE]
> Предполагается, что вы уже изучили [общие сведения фабрике данных Azure](data-factory-introduction.md). Если у вас нет практического опыта создания фабрик данных, рекомендуем для лучшего понимания статьи изучить [руководства по преобразованию ](data-factory-build-your-first-pipeline.md) и (или) [по перемещению данных](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).  

## <a name="overview"></a>Обзор
Фабрика данных может иметь один или несколько конвейеров. Конвейеры — это логические группы действий, которые вместе отвечают за выполнение задачи. Действия в конвейере определяют операции с данными. Например, действие копирования позволяет скопировать данные из локальной базы данных SQL Server в хранилище BLOB-объектов Azure. Затем можно использовать действие Hive для запуска сценария Hive в кластере Azure HDInsight, чтобы обработать данные из хранилища BLOB-объектов и получить выходные данные. Наконец, используйте второе действие копирования, чтобы скопировать выходные данные в хранилище данных SQL Azure, на основе которого строятся решения по отчетам бизнес-аналитики. 

У каждого действия может быть несколько входных [наборов данных](data-factory-create-datasets.md) или же ни одного, и каждое действие может производить один или несколько выходных [наборов данных](data-factory-create-datasets.md). На следующей схеме показана связь между конвейером, действием и набором данных в фабрике данных: 

![Связь между конвейером, действием и набором данных](media/data-factory-create-pipelines/relationship-pipeline-activity-dataset.png)

Конвейер позволяет управлять группами действий, а не каждым отдельным действием. Например, вы можете развернуть конвейер, а также запланировать, приостановить и возобновить его работу, а не выполнять действия отдельно.

Фабрика данных поддерживает два типа действий: действия перемещения данных и действия преобразования данных. У каждого действия может быть несколько входных [наборов данных](data-factory-create-datasets.md) или же ни одного, и каждое действие может производить один или несколько выходных наборов данных.

Входной набор данных представляет входные данные для действия в конвейере, а выходной набор данных — выходные данные для действия. Наборы данных представляют данные в разных хранилищах, например в таблицах, файлах, папках и документах. Созданные наборы данных можно использовать для действий в конвейере. Например, можно указать входной или выходной набор данных для действия HDInsightHive или действия копирования. Дополнительные сведения о наборах данных см. в статье [Наборы данных в фабрике данных Azure](data-factory-create-datasets.md).

### <a name="data-movement-activities"></a>Действия перемещения данных
Действие копирования в фабрике данных копирует данные из хранилища-источника в хранилище-приемник. Фабрика данных поддерживает приведенные ниже хранилища данных. Данные из любого источника можно записывать в любой приемник. Щелкните название хранилища, чтобы узнать, как скопировать данные из него или в него.

[!INCLUDE [data-factory-supported-data-stores](../../../includes/data-factory-supported-data-stores.md)]

> [!NOTE]
> Хранилища данных, отмеченные звездочкой (*), могут находиться в локальном расположении или в IaaS Azure и требовать установки [шлюза управления данными](data-factory-data-management-gateway.md) на локальном компьютере или компьютере IaaS Azure.

Дополнительные сведения см. в статье [Перемещение данных с помощью действия копирования](data-factory-data-movement-activities.md).

### <a name="data-transformation-activities"></a>Действия преобразования данных
[!INCLUDE [data-factory-transformation-activities](../../../includes/data-factory-transformation-activities.md)]

Дополнительные сведения см. в статье [Преобразование данных в фабрике данных Azure](data-factory-data-transformation-activities.md).

### <a name="custom-net-activities"></a>Пользовательские действия .NET 
Если требуется переместить данные в хранилище данных, которое не поддерживается действием копирования, или из такого хранилища либо преобразовать данные с использованием собственной логики, вы можете создать **настраиваемое действие .NET**. Сведения о создании и использовании настраиваемого действия см. в статье [Использование настраиваемых действий в конвейере фабрики данных Azure](data-factory-use-custom-activities.md).

## <a name="schedule-pipelines"></a>Планирование конвейеров
Конвейер работает только в период активности, то есть между временем **начала** и **окончания**. Он не работает до времени начала и после времени окончания. Если конвейер приостановлен, он не будет работать независимо от значений времени начала и окончания. Запустить конвейер можно только в том случае, если он не находится в приостановленном состоянии. Сведения о планировании и выполнении в фабрике данных Azure см. в разделе [Планирование и выполнение](data-factory-scheduling-and-execution.md).

## <a name="pipeline-json"></a>Конвейер JSON
Рассмотрим подробнее определение конвейера в формате JSON. В общем виде структуру конвейера можно представить приведенным ниже образом.

```json
{
    "name": "PipelineName",
    "properties": 
    {
        "description" : "pipeline description",
        "activities":
        [

        ],
        "start": "<start date-time>",
        "end": "<end date-time>",
        "isPaused": true/false,
        "pipelineMode": "scheduled/onetime",
        "expirationTime": "15.00:00:00",
        "datasets": 
        [
        ]
    }
}
```

| Тег | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| name |Имя конвейера. Укажите имя, представляющее действие, которое выполняет конвейер. <br/><ul><li>Максимальное количество знаков: 260.</li><li>Должно начинаться с буквы, цифры или символа подчеркивания (_).</li><li>Следующие знаки не допускаются: ".", "+", "?", "/", "<", ">", "*", "%", "&", ":", "\\".</li></ul> |Yes |
| description | Укажите текст, описывающий, для чего используется конвейер. |Yes |
| Действия | В разделе **activities** можно определить одно или несколько действий. Дополнительные сведения об элементе JSON действий см. в следующем разделе. | Yes |  
| start | Дата и время начала работы конвейера. Задается в [формате ISO](http://en.wikipedia.org/wiki/ISO_8601). Например, `2016-10-14T16:32:41Z`. <br/><br/>Можно указать местное время, например восточное поясное время (EST). Пример: `2016-02-27T06:00:00-05:00`". Это 6:00 по восточному стандартному времени.<br/><br/>Свойства start и end определяют активный период работы конвейера. Срезы выходных данных создаются только в этот активный период. |Нет <br/><br/>Если вы указываете значение свойства end, вы также должны указать значение свойства start.<br/><br/>Для создания конвейера значения времени начала и времени окончания могут быть пустыми. Если требуется задать активный период работы конвейера, следует указать оба значения. Если вы не указали время начала и окончания при создании конвейера, их можно установить позже с помощью командлета Set-AzureRmDataFactoryPipelineActivePeriod. |
| end | Дата и время завершения работы конвейера. Не является обязательным и задается в формате ISO. Например: `2016-10-14T17:32:41Z` <br/><br/>Можно указать местное время, например восточное поясное время (EST). Пример: `2016-02-27T06:00:00-05:00`. Это 6:00 по восточному стандартному времени.<br/><br/>Чтобы работа конвейера не была ограничена во времени, укажите для свойства end значение 9999-09-09. <br/><br/> Конвейер работает только в период активности, то есть между временем начала и окончания. Он не работает до времени начала и после времени окончания. Если конвейер приостановлен, он не будет работать независимо от значений времени начала и окончания. Запустить конвейер можно только в том случае, если он не находится в приостановленном состоянии. Сведения о планировании и выполнении в фабрике данных Azure см. в разделе [Планирование и выполнение](data-factory-scheduling-and-execution.md). |Нет  <br/><br/>Если вы указываете значение свойства end, вы также должны указать значение свойства start.<br/><br/>Ознакомьтесь с примечаниями к свойству **start**. |
| isPaused | Если задано значение true, конвейер не запускается. Он находится в приостановленном состоянии. Значение по умолчанию — false. Это свойство можно использовать для включения или отключения конвейера. |Нет  |
| pipelineMode | Определяет метод планирования работы конвейера. Допустимые значения: scheduled (по умолчанию), onetime.<br/><br/>Значение scheduled означает, что конвейер будет запускаться с указанной периодичностью в соответствии с его активным периодом (временем начала и окончания). Значение onetime означает, что конвейер будет запускаться только один раз. В настоящее время изменить или обновить однократные конвейеры после их создания нельзя. Подробные сведения об однократном запуске см. в разделе [Однократный конвейер](#onetime-pipeline). |Нет  |
| expirationTime; | Период времени после создания, в течение которого [однократный конвейер](#onetime-pipeline) является допустимым и должен оставаться подготовленным. Если на момент завершения этого периода у конвейера не будет активных, невыполненных или ожидающих выполнения запусков, конвейер будет автоматически удален. Значение по умолчанию — `"expirationTime": "3.00:00:00"`.|Нет  |
| datasets |Список наборов данных для использования действиями, определенными в конвейере. Это свойство можно использовать для определения наборов данных, характерных для этого конвейера и не определенных в фабрике данных. Наборы данных, определенные в этом конвейере, могут использоваться только этим конвейером и не предназначены для совместного доступа. Дополнительные сведения см. в разделе [Контекст наборов данных](data-factory-create-datasets.md#scoped-datasets). |Нет  |

## <a name="activity-json"></a>Действие JSON
В разделе **activities** можно определить одно или несколько действий. Каждое действие имеет следующую структуру верхнего уровня.

```json
{
    "name": "ActivityName",
    "description": "description", 
    "type": "<ActivityType>",
    "inputs":  "[]",
    "outputs":  "[]",
    "linkedServiceName": "MyLinkedService",
    "typeProperties":
    {

    },
    "policy":
    {
    },
    "scheduler":
    {
    }
}
```

В приведенной ниже таблице описаны свойства, используемые в определениях JSON действия:

| Тег | ОПИСАНИЕ | Обязательно |
| --- | --- | --- |
| name | Имя действия. Укажите имя, представляющее операцию, которую выполняет действие. <br/><ul><li>Максимальное количество знаков: 260.</li><li>Должно начинаться с буквы, цифры или символа подчеркивания (_).</li><li>Следующие знаки не допускаются: ".", "+", "?", "/", "<", ">", "*", "%", "&", ":", "\\".</li></ul> |Yes |
| description | Текст, описывающий, для чего используется действие |Yes |
| Тип | Тип действия. Разные типы действий описаны в разделах, посвященных [действиям перемещения](#data-movement-activities) и [преобразования данных](#data-transformation-activities). |Yes |
| inputs |Входные таблицы, используемые действием:<br/><br/>`// one input table`<br/>`"inputs":  [ { "name": "inputtable1"  } ],`<br/><br/>`// two input tables` <br/>`"inputs":  [ { "name": "inputtable1"  }, { "name": "inputtable2"  } ],` |Yes |
| outputs |Выходные таблицы, используемые действием.<br/><br/>`// one output table`<br/>`"outputs":  [ { "name": "outputtable1" } ],`<br/><br/>`//two output tables`<br/>`"outputs":  [ { "name": "outputtable1" }, { "name": "outputtable2" }  ],` |Yes |
| linkedServiceName |Имя связанной службы, используемой действием. <br/><br/>Для действия может потребоваться указать службу, связанную с обязательной вычислительной средой. |Да — для действия HDInsight и действия пакетной оценки показателей машинного обучения Azure;  <br/><br/>Нет — для всех остальных |
| typeProperties |Свойства в разделе **typeProperties** зависят от типа действия. Чтобы просмотреть свойства типа для действия, щелкните ссылки на действие в предыдущем разделе. | Нет  |
| policy |Политики, которые влияют на поведение во время выполнения действия. Если для этого свойства не задано значение, используются стандартные политики. |Нет  |
| scheduler | Свойство scheduler позволяет задать расписание выполнения действия. Для него предусмотрен такой же набор подсвойств, что и для [свойства availability в наборе данных](data-factory-create-datasets.md#dataset-availability). |Нет  |


### <a name="policies"></a>Политики
Политики влияют на поведение во время выполнения действия, особенно при обработке среза таблицы. В следующей таблице приведено несколько примеров.

| Свойство | Допустимые значения | По умолчанию | ОПИСАНИЕ |
| --- | --- | --- | --- |
| concurrency |Целое число  <br/><br/>Максимальное значение — 10 |1 |Число одновременных выполнений действия.<br/><br/>Определяет количество параллельных выполнений одного действия для обработки разных срезов. Например, высокое значение этого свойства ускорит обработку большого набора доступных данных. |
| executionPriorityOrder |NewestFirst<br/><br/>OldestFirst |OldestFirst |Определяет порядок обработки срезов данных.<br/><br/>Предположим, есть два ожидающих обработки среза (от 16:00 и от 17:00). Если для свойства executionPriorityOrder задано значение NewestFirst, срез от 17:00 будет обработан первым. Точно так же, если для executionPriorityORder задано значение OldestFIrst, первым будет обработан срез от 16:00. |
| retry |Целое число <br/><br/>Максимальное значение — 10 |0 |Число повторных попыток обработки данных до того, как срез перейдет в состояние Failure (сбой). Выполнение действия со срезом данных повторяется указанное количество раз. Повторная попытка выполняется сразу после неудачной. |
| timeout |Интервал времени |00:00:00 |Время ожидания для действия. Пример: 00:10:00 (время ожидания — 10 минут).<br/><br/>Если значение не указано или равно 0, то время ожидания не ограничено.<br/><br/>Если время обработки среза превышает время ожидания, система отменяет текущую обработку и начинает новую. Количество повторов зависит от значения свойства retry. Когда время ожидания истекает, состояние среза меняется на TimedOut. |
| delay |Интервал времени |00:00:00 |Задайте задержку перед обработкой данных после начала выполнения среза.<br/><br/>Действие для среза данных запускается в ожидаемое время выполнения с указанной задержкой.<br/><br/>Пример: 00:10:00 (означает задержку в 10 минут). |
| longRetry |Целое число <br/><br/>Максимальное значение — 10 |1 |Количество длительных повторных попыток перед завершением сбоем выполнения среза.<br/><br/>Интервал между этими попытками задается свойством longRetryInterval. Используйте свойство longRetry, если повторные попытки необходимо выполнять с паузами. Если указаны свойства Retry и longRetry, то каждая попытка longRetry включает в себя попытки Retry, и максимальное число попыток равно Retry * longRetry.<br/><br/>Например, в политике действия указаны следующие параметры:<br/>Retry: 3<br/>longRetry: 2<br/>longRetryInterval: 01:00:00<br/><br/>Предположим, что существует только один выполняемый срез (в состоянии Waiting), и каждый раз при выполнении действия происходит сбой. Первые три попытки будут выполнены подряд. После каждой повторной попытки срез будет находиться в состоянии Retry. После выполнения первых трех попыток состоянием среза станет LongRetry.<br/><br/>Через час (значение свойства longRetryInterval) будут выполнены еще три попытки подряд. После этого состояние среза изменится на Failed и дальнейшие попытки предприниматься не будут. Поэтому всего было предпринято 6 попыток.<br/><br/>Если какое-либо выполнение завершится успешно, то состоянием среза станет Ready и дальнейшие попытки выполняться не будут.<br/><br/>Свойство longRetry можно использовать в ситуациях, когда зависимые данные поступают в неопределенное время или вся среда, в которой происходит обработка данных, непредсказуема. В таких случаях последовательные повторные попытки могут оказаться бесполезными, а выполненные через некоторое время, напротив, могут привести к желаемому результату.<br/><br/>Предупреждение. Не задавайте высокие значения для свойств longRetry и longRetryInterval. Как правило, более высокие значения приводят к появлению других системных проблем. |
| longRetryInterval |Интервал времени |00:00:00 |Период времени между длительными повторными попытками. |

## <a name="sample-copy-pipeline"></a>Пример конвейера копирования
В следующем примере конвейера содержится одно действие типа **Copy** in the **действий** . В этом примере [действие копирования](data-factory-data-movement-activities.md) копирует данные из хранилища BLOB-объектов Azure в базу данных SQL Azure. 

```json
{
  "name": "CopyPipeline",
  "properties": {
    "description": "Copy data from a blob to Azure SQL table",
    "activities": [
      {
        "name": "CopyFromBlobToSQL",
        "type": "Copy",
        "inputs": [
          {
            "name": "InputDataset"
          }
        ],
        "outputs": [
          {
            "name": "OutputDataset"
          }
        ],
        "typeProperties": {
          "source": {
            "type": "BlobSource"
          },
          "sink": {
            "type": "SqlSink",
            "writeBatchSize": 10000,
            "writeBatchTimeout": "60:00:00"
          }
        },
        "Policy": {
          "concurrency": 1,
          "executionPriorityOrder": "NewestFirst",
          "retry": 0,
          "timeout": "01:00:00"
        }
      }
    ],
    "start": "2016-07-12T00:00:00Z",
    "end": "2016-07-13T00:00:00Z"
  }
} 
```

Обратите внимание на следующие моменты.

* В разделе действий доступно только одно действие, параметр **type** которого имеет значение **Copy**.
* Для этого действия параметру input присвоено значение **InputDataset**, а параметру output — значение **OutputDataset**. Сведения об определении наборов данных в JSON см. в статье [Наборы данных](data-factory-create-datasets.md). 
* В разделе **typeProperties** в качестве типа источника указано **BlobSource**, а в качестве типа приемника — **SqlSink**. В разделе [Действия перемещения данных](#data-movement-activities) щелкните хранилище данных, которое хотите использовать как источник или приемник, чтобы подробнее узнать о перемещении данных из этого хранилища или в него. 

Полное пошаговое руководство по созданию этого конвейера см. в разделе [Копирование данных из хранилища BLOB-объектов Azure в базу данных SQL с помощью фабрики данных](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md). 

## <a name="sample-transformation-pipeline"></a>Пример конвейера преобразования
В следующем примере конвейера содержится одно действие типа **HDInsightHive** in the **действий** . В этом примере [действие HDInsight Hive](data-factory-hive-activity.md) преобразовывает данные из хранилища BLOB-объектов Azure, запуская файл сценария Hive в кластере Azure HDInsight Hadoop. 

```json
{
    "name": "TransformPipeline",
    "properties": {
        "description": "My first Azure Data Factory pipeline",
        "activities": [
            {
                "type": "HDInsightHive",
                "typeProperties": {
                    "scriptPath": "adfgetstarted/script/partitionweblogs.hql",
                    "scriptLinkedService": "AzureStorageLinkedService",
                    "defines": {
                        "inputtable": "wasb://adfgetstarted@<storageaccountname>.blob.core.windows.net/inputdata",
                        "partitionedtable": "wasb://adfgetstarted@<storageaccountname>.blob.core.windows.net/partitioneddata"
                    }
                },
                "inputs": [
                    {
                        "name": "AzureBlobInput"
                    }
                ],
                "outputs": [
                    {
                        "name": "AzureBlobOutput"
                    }
                ],
                "policy": {
                    "concurrency": 1,
                    "retry": 3
                },
                "scheduler": {
                    "frequency": "Month",
                    "interval": 1
                },
                "name": "RunSampleHiveActivity",
                "linkedServiceName": "HDInsightOnDemandLinkedService"
            }
        ],
        "start": "2016-04-01T00:00:00Z",
        "end": "2016-04-02T00:00:00Z",
        "isPaused": false
    }
}
```

Обратите внимание на следующие моменты. 

* В разделе activities есть только одно действие, параметр **type** которого имеет значение **HDInsightHive**.
* Файл сценария Hive, **partitionweblogs.hql**, хранится в учетной записи хранения Azure (указывается с помощью свойства scriptLinkedService, имеющего значение **AzureStorageLinkedService**) в папке **script** в контейнере **adfgetstarted**.
* В разделе `defines` указываются параметры среды выполнения, передаваемые сценарию Hive в качестве значений конфигурации Hive (например, `${hiveconf:inputtable}`, `${hiveconf:partitionedtable}`).

Разделы **typeProperties** для каждого действия преобразования отличаются. Для получения дополнительных сведений о свойствах типа, поддерживаемых для действия преобразования, щелкните такое действие в таблице [Действия по преобразованию данных](#data-transformation-activities). 

Полное пошаговое руководство по созданию данного конвейера см. в разделе [Учебник. Создание первого конвейера для обработки данных с помощью кластера Hadoop](data-factory-build-your-first-pipeline.md). 

## <a name="multiple-activities-in-a-pipeline"></a>Несколько действий в конвейере
В двух предыдущих примерах конвейеров присутствует только одно действие. Конвейер может содержать сразу несколько действий.  

Если в конвейере несколько действий и выходные данные одного действия не являются входными данными другого действия, такие действия могут выполняться параллельно, при условии, что срезы входных данных для действий готовы. 

Можно объединить в цепочку два действия, используя выходной набор данных одного действия как входной набор данных другого действия. Второе действие выполняется только после успешного завершения первого.

![Построение цепочки действий в одном конвейере](./media/data-factory-create-pipelines/chaining-one-pipeline.png)

В этом примере конвейер содержит два действия: Activity1 и Activity2. Действие Activity1 принимает в качестве входных данных набор Dataset1 и выводит набор Dataset2. Действие Activity2 принимает в качестве входных данных набор Dataset2 и выводит набор Dataset3. Так как выходные данные действия Activity1 (Dataset2) являются входными для Activity2, Activity2 выполняется только после успешного завершения Activity1 и создания среза Dataset2. Если Activity1 по какой-либо причине завершается неудачно и не создает срез Dataset2, Activity2 для этого среза не выполняется (например, с 9:00 до 10:00). 

Вы можете объединять в цепочку действия, находящиеся в разных конвейерах.

![Построение цепочки действий в двух конвейерах](./media/data-factory-create-pipelines/chaining-two-pipelines.png)

В этом примере конвейер Pipeline1 имеет только одно действие, принимающее входной набор данных Dataset1 и выводящее Dataset2. Конвейер Pipeline2 также имеет только действие, принимающее Dataset2 и выводящее Dataset3. 

Чтобы узнать больше, ознакомьтесь с [планированием и выполнением](data-factory-scheduling-and-execution.md#multiple-activities-in-a-pipeline). 

## <a name="create-and-monitor-pipelines"></a>Создание и мониторинг конвейеров
Конвейеры можно создать с помощью одного из указанных ниже средств или пакетов SDK. 

- Мастер копирования 
- Портал Azure
- Visual Studio
- Azure PowerShell
- Шаблон диспетчера ресурсов Azure
- ИНТЕРФЕЙС REST API
- .NET API

Пошаговые инструкции по созданию конвейеров с помощью одного из указанных ниже средств или пакетов SDK приведены в следующих руководствах:
 
- [Создание конвейера с помощью действия преобразования данных](data-factory-build-your-first-pipeline.md)
- [Руководство. Копирование данных из хранилища BLOB-объектов Azure в базу данных SQL с помощью фабрики данных](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md)

После создания и развертывания конвейера вы можете управлять конвейерами и отслеживать их с помощью колонок на портале Azure или приложения для мониторинга и управления. Пошаговые инструкции представлены в указанных ниже статьях. 

- [Мониторинг конвейеров фабрики данных Azure и управление ими с помощью портала Azure и PowerShell](data-factory-monitor-manage-pipelines.md)
- [Мониторинг конвейеров фабрики данных Azure и управление ими с помощью приложения для мониторинга и управления](data-factory-monitor-manage-app.md)


## <a name="onetime-pipeline"></a>Однократный конвейер
Вы можете создать конвейер и настроить для него периодическое выполнение (например, ежечасно и ежедневно) в пределах между временем начала и окончания, заданными в определении конвейера. Дополнительные сведения см. в разделе [Планирование действий](#scheduling-and-execution). Вы также можете создать конвейер, выполняемый однократно. Для этого свойству **pipelineMode** в определении конвейера необходимо присвоить значение **onetime** (однократный), как показано в следующем примере файла JSON. По умолчанию для этого свойства используется значение **scheduled**.

```json
{
    "name": "CopyPipeline",
    "properties": {
        "activities": [
            {
                "type": "Copy",
                "typeProperties": {
                    "source": {
                        "type": "BlobSource",
                        "recursive": false
                    },
                    "sink": {
                        "type": "BlobSink",
                        "writeBatchSize": 0,
                        "writeBatchTimeout": "00:00:00"
                    }
                },
                "inputs": [
                    {
                        "name": "InputDataset"
                    }
                ],
                "outputs": [
                    {
                        "name": "OutputDataset"
                    }
                ]
                "name": "CopyActivity-0"
            }
        ]
        "pipelineMode": "OneTime"
    }
}
```

Обратите внимание на следующее.

* Время **начала** и **окончания** для конвейера не указывается.
* При этом нужно указывать **доступность** входных и выходных наборов данных (**периодичность** и **интервал**), несмотря на то что фабрика данных эти значения не использует.  
* В представлении диаграммы однократные конвейеры не отображаются. В этом весь замысел.
* Однократные конвейеры не обновляются. Однократный конвейер можно клонировать, переименовать, обновить его свойства и развернуть, чтобы создать другой конвейер.


## <a name="next-steps"></a>Дальнейшие действия
- Дополнительные сведения о наборах данных см. в статье [Создание наборов данных](data-factory-create-datasets.md). 
- Дополнительные сведения о планировании и выполнении конвейеров см. в статье [Планирование и выполнение в фабрике данных Azure](data-factory-scheduling-and-execution.md). 
  

