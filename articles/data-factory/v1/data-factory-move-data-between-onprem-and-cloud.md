---
title: 'Перемещение данных: шлюз управления данными | Документация Майкрософт'
description: Настройка шлюза данных для перемещения данных между локальными узлами и облаком. Используйте шлюз управления данными в фабрике данных Azure для перемещения данных.
services: data-factory
documentationcenter: ''
author: nabhishek
manager: craigg
ms.assetid: 7bf6d8fd-04b5-499d-bd19-eff217aa4a9c
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: conceptual
ms.date: 01/10/2018
ms.author: abnarain
robots: noindex
ms.openlocfilehash: 958b495b2328a981c573447c44585bc7430c1095
ms.sourcegitcommit: 266fe4c2216c0420e415d733cd3abbf94994533d
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 06/01/2018
ms.locfileid: "34621172"
---
# <a name="move-data-between-on-premises-sources-and-the-cloud-with-data-management-gateway"></a>Перемещение данных между локальными источниками и облаком с помощью шлюза управления данными
> [!NOTE]
> Статья относится к версии 1 фабрики данных, которая является общедоступной версией. Если вы используете версию 2 службы фабрики данных, которая находится на этапе предварительной версии, ознакомьтесь с [копированием данных между локальной средой и облаком с помощью фабрики данных версии 2](../tutorial-hybrid-copy-powershell.md).

Эта статья содержит общие сведения об интеграции данных, хранящихся в локальных и облачных хранилищах данных, с помощью фабрики данных. В статье используются понятия, описанные в статье [Действия по перемещению данных](data-factory-data-movement-activities.md) и других основополагающих статьях о фабрике данных: [наборы данных](data-factory-create-datasets.md) и [конвейеры](data-factory-create-pipelines.md).

## <a name="data-management-gateway"></a>Шлюз управления данными
Шлюз управления данными необходимо установить на локальный компьютер, чтобы обеспечить перемещение данных из локального хранилища данных и в него. Шлюз можно установить на том же компьютере, на котором размещается хранилище, или на другом компьютере. Важно, чтобы шлюз мог подключиться к хранилищу данных.

> [!IMPORTANT]
> Дополнительные сведения о шлюзе управления данными см. в статье [Шлюз управления данными](data-factory-data-management-gateway.md). 

Приведенное ниже пошаговое руководство поможет вам создать фабрику данных с конвейером, перемещающим данные из локальной базы данных **SQL Server** в хранилище BLOB-объектов Azure. В рамках этого пошагового руководства вы установите и настроите шлюз управления данными на своем компьютере.

## <a name="walkthrough-copy-on-premises-data-to-cloud"></a>Пошаговое руководство: копирование локальных данных в облако
В этом пошаговом руководстве вы сделаете следующее: 

1. Создадите фабрику данных.
2. Создадите шлюз управления данными. 
3. Создадите связанные службы для хранилища данных-источника и приемника.
4. Создадите наборы данных, которые представляют входные и выходные данные.
5. Создадите конвейер с действием копирования для перемещения данных.

## <a name="prerequisites-for-the-tutorial"></a>Предварительные требования для прохождения этого учебника
Для работы с этим пошаговым руководством необходимо следующее:

* **Подписка Azure**.  Если у вас нет подписки, вы можете создать бесплатную пробную версию учетной записи всего за несколько минут. Дополнительные сведения см. в статье [Бесплатная пробная версия](http://azure.microsoft.com/pricing/free-trial/).
* **исходного**хранилища данных. В этом руководстве в качестве **назначения и приемника** будет использоваться хранилище BLOB-объектов. в статье [Об учетных записях хранения Azure](../../storage/common/storage-create-storage-account.md#create-a-storage-account) .
* **SQL Server.** В этом руководстве используйте локальную базу данных SQL Server в качестве **исходного** хранилища данных. 

## <a name="create-data-factory"></a>Создание фабрики данных
На этом этапе вы с помощью портала Azure создадите экземпляр фабрики данных Azure с именем **ADFTutorialOnPremDF**.

1. Войдите на [портал Azure](https://portal.azure.com).
2. Щелкните **Создать ресурс**, **Аналитика**, а затем — **Фабрика данных**.

   ![Создать -> Фабрика данных](./media/data-factory-move-data-between-onprem-and-cloud/NewDataFactoryMenu.png)  
3. На странице **Новая фабрика данных** введите **ADFTutorialOnPremDF** в поле "Имя".

    ![Добавить на начальную панель](./media/data-factory-move-data-between-onprem-and-cloud/OnPremNewDataFactoryAddToStartboard.png)

   > [!IMPORTANT]
   > Имя фабрики данных Azure должно быть глобально уникальным. Получив сообщение об ошибке **Имя фабрики данных "ADFTutorialOnPremDF" недоступно**, измените имя фабрики данных (например, на yournameADFTutorialOnPremDF) и попробуйте создать ее еще раз. Выполняя оставшиеся действия, описанные в этом руководстве, вместо ADFTutorialOnPremDF используйте именно это имя.
   >
   > В будущем имя фабрики данных может быть зарегистрировано в качестве **DNS-имени** и, следовательно, стать отображаемым.
   >
   >
4. Выберите **подписку Azure** , в рамках которой вы хотите создать фабрику данных.
5. Выберите имеющуюся **группу ресурсов** или создайте новую. Для примера в этом руководстве создайте группу ресурсов с именем **ADFTutorialResourceGroup**.
6. На странице **Новая фабрика данных** нажмите кнопку **Создать**.

   > [!IMPORTANT]
   > Создавать экземпляры фабрики данных может пользователь с ролью [Участник фабрики данных](../../role-based-access-control/built-in-roles.md#data-factory-contributor) на уровне подписки или группы ресурсов.
   >
   >
7. После завершения процедуры создания фабрики данных появится страница **Фабрика данных**, как показано на рисунке ниже.

   ![Домашняя страница фабрики данных](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDataFactoryHomePage.png)

## <a name="create-gateway"></a>Создание шлюза
1. На странице **Фабрика данных** щелкните плитку **Создать и развернуть**, чтобы запустить **редактор** для фабрики данных.

    ![Плитка "Создание и развертывание"](./media/data-factory-move-data-between-onprem-and-cloud/author-deploy-tile.png)
2. В редакторе фабрики данных нажмите кнопку **... More** (...Дополнительно) на панели инструментов, а затем — **Новый шлюз данных**. Кроме того, в представлении в виде дерева можно щелкнуть правой кнопкой мыши **Шлюзы данных** и выбрать пункт **Новый шлюз данных**.

   ![«Новый шлюз данных» на панели инструментов](./media/data-factory-move-data-between-onprem-and-cloud/NewDataGateway.png)
3. На странице **Создать** в поле **Имя** введите **adftutorialgateway** и нажмите кнопку **ОК**.     

    ![Страница "Создать шлюз"](./media/data-factory-move-data-between-onprem-and-cloud/OnPremCreateGatewayBlade.png)

    > [!NOTE]
    > В этом пошаговом руководстве вы создадите логический шлюз с одним узлом (локальный компьютер с Windows). Шлюз управления данными можно развернуть, сопоставив с ним несколько локальных компьютеров. Чтобы увеличить масштаб, увеличьте число заданий перемещения данных, которые могут выполняться одновременно на узле. Эта функция также доступна для логического шлюза с одним узлом. Дополнительные сведения см. в статье [Шлюз управления данными: высокий уровень доступности и масштабируемость (предварительная версия)](data-factory-data-management-gateway-high-availability-scalability.md).  
4. На странице **Настройка** щелкните **Установка непосредственно на этот компьютер**. Это позволит скачать пакет установки для шлюза, а также установить, настроить и зарегистрировать шлюз на компьютере.  

   > [!NOTE]
   > Используйте Internet Explorer или другой веб-браузер, совместимый с Microsoft ClickOnce.
   >
   > Если вы используете браузер Chrome, перейдите в [интернет-магазин Chrome](https://chrome.google.com/webstore/), введите ClickOnce в строке поиска, а затем выберите и установите одно из расширений ClickOnce.
   >
   > То же самое (установку надстройки) сделайте и в случае с браузером Firefox. Нажмите кнопку **Открыть меню** на панели инструментов (**три горизонтальные линии** в правом верхнем углу), щелкните **Надстройки**, введите "ClickOnce" в строку поиска, выберите одно из расширений ClickOnce и установите его.    
   >
   >

    ![Шлюз — страница "Настройка"](./media/data-factory-move-data-between-onprem-and-cloud/OnPremGatewayConfigureBlade.png)

    Это самый простой способ (одним щелчком) скачать, установить, настроить и зарегистрировать шлюз в один прием. Вы увидите, что на компьютере установлено приложение **Microsoft Data Management Gateway Configuration Manager** . Вы также можете найти исполняемый файл **ConfigManager.exe** в папке по следующему пути: **C:\Program Files\Microsoft Data Management Gateway\2.0\Shared**.

    Шлюз также можно скачать и установить вручную, используя ссылки на этой странице. Затем вы можете зарегистрировать его с помощью ключа, указанного в текстовом поле **Создать ключ**.

    Все дополнительные сведения о шлюзе см. в статье [Шлюз управления данными](data-factory-data-management-gateway.md).

   > [!NOTE]
   > Для успешной установки шлюза управления данными и его настройки вы должны обладать правами администратора на локальном компьютере. В локальную группу Windows **Пользователи шлюза управления данными** можно добавить дополнительных пользователей. Участники этой группы могут использовать диспетчер конфигурации шлюза управления данными для настройки шлюза.
   >
   >
5. Подождите несколько минут или пока не появится следующее уведомление:

    ![Установка шлюза успешно выполнена](./media/data-factory-move-data-between-onprem-and-cloud/gateway-install-success.png)
6. Запустите на компьютере приложение **Диспетчер конфигурации шлюза управления данными**. Для этого введите текст **шлюз управления данными** в окне **Поиск**. Вы также можете найти исполняемый файл **ConfigManager.exe** в папке по следующему пути: **C:\Program Files\Microsoft Data Management Gateway\2.0\Shared**.

    ![Диспетчер конфигурации шлюза](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDMGConfigurationManager.png)
7. Убедитесь, что отображается сообщение `adftutorialgateway is connected to the cloud service`. В строке состояния внизу отображается надпись **Подключение к облачной службе установлено** и **зеленая галочка**.

    На вкладке **Главная** можно также выполнить следующие операции:

   * **зарегистрировать** шлюз, используя ключ с портала Azure, с помощью кнопки "Зарегистрировать";
   * **остановить** службу узла шлюза управления данными на компьютере шлюза;
   * **запланировать** установку обновлений на определенное время суток;
   * просмотреть, когда шлюз был **обновлен последний раз**;
   * указать время для установки обновления в шлюзе.
8. Переключитесь на вкладку **Параметры** . Сертификат, указанный в разделе **Сертификат** , используется для шифрования и расшифровки учетных данных локального хранилища данных, указанного на портале. Щелкните **Изменить** , чтобы использовать собственный сертификат (необязательно). По умолчанию шлюз использует сертификат, который автоматически создан службой фабрики данных.

    ![Конфигурация сертификата шлюза](./media/data-factory-move-data-between-onprem-and-cloud/gateway-certificate.png)

    Кроме того, на вкладке **Параметры** можно:

   * просмотреть или экспортировать сертификат, используемый шлюзом;
   * изменить конечную точку HTTPS, используемую шлюзом;    
   * задать прокси-сервер HTTP, который будет использовать шлюз.     
9. (Необязательно.) Для устранения проблем в работе шлюза вы можете использовать подробное ведение журнала. Чтобы включить эту функцию, перейдите на вкладку **Диагностика** и установите флажок **Включить запись подробных сведений в журнал**. Данные журналов можно изучить в средстве **Просмотр событий**, открыв узел **Журналы приложений и служб** -> **Шлюз управления данными**.

    ![Вкладка «Диагностика»](./media/data-factory-move-data-between-onprem-and-cloud/diagnostics-tab.png)

    На вкладке **Диагностика** можно также выполнить следующие действия.

   * Щелкните **Проверить подключение** , чтобы проверить подключение к локальному источнику данных через шлюз.
   * Щелкните **Просмотреть журналы** , чтобы просмотреть журнал шлюза управления данными в окне просмотра событий.
   * Щелкните **Оправить журналы**, чтобы передать ZIP-файл с журналами за последние семь дней в корпорацию Майкрософт. Это поможет упростить устранение неполадок, с которыми вы сталкиваетесь.
10. На вкладке **Диагностика** в разделе **Проверка подключения** выберите **SqlServer** в качестве типа хранилища данных, введите имя сервера базы данных и имя базы данных, укажите тип проверки подлинности, введите имя пользователя и пароль и нажмите кнопку **Test** (Проверка), чтобы проверить возможность подключения шлюза к базе данных.
11. Перейдите в браузер и на **портале Azure** нажмите кнопку **ОК** на странице **Настройка**, а затем — на странице **Новый шлюз данных**.
12. В иерархической структуре слева найдите элемент **adftutorialgateway** в узле **Шлюзы данных**.  Щелкните его, чтобы увидеть связанные файлы JSON.

## <a name="create-linked-services"></a>Создание связанных служб
На этом этапе вы создадите две связанные службы: **AzureStorageLinkedService** и **SqlServerLinkedService**. Служба **SqlServerLinkedService** связывает с фабрикой данных локальную базу данных SQL Server, а служба **AzureStorageLinkedService** — хранилище BLOB-объектов Azure. Далее, это руководство поможет вам создать конвейер, который копирует данные из локальной базы данных SQL Server в службу хранилища BLOB-объектов Azure.

#### <a name="add-a-linked-service-to-an-on-premises-sql-server-database"></a>Добавление связанной службы в локальную базу данных SQL Server
1. В **редакторе фабрики данных** щелкните **Создать хранилище данных** на панели инструментов и выберите **SQL Server**.

   ![Создать связанную службу SQL Server](./media/data-factory-move-data-between-onprem-and-cloud/NewSQLServer.png)
2. В **редакторе JSON** справа сделайте следующее:

   1. Для параметра **gatewayName** укажите значение **adftutorialgateway**.    
   2. В **connectionString** сделайте следующее:    

      1. В поле **servername** введите имя сервера, на котором размещены базы данных SQL Server.
      2. В поле **databasename** введите имя базы данных.
      3. На панели инструментов нажмите кнопку **Зашифровать**. Отобразится приложение "Диспетчер учетных данных".

         ![Приложение "Диспетчер учетных данных"](./media/data-factory-move-data-between-onprem-and-cloud/credentials-manager-application.png)
      4. В диалоговом окне **Настройка учетных данных** введите имя пользователя и пароль, а затем нажмите кнопку **ОК**. Если подключение установлено успешно, зашифрованные учетные данные сохраняются в JSON, а диалоговое окно закрывается.
      5. Закройте пустую вкладку браузера, запустившую диалоговое окно, если она не закрылась автоматически, и вернитесь на вкладку портала Azure.

         В компьютере шлюза учетные данные **зашифрованы** с помощью сертификата, который принадлежит службе фабрики данных. Если вы хотите использовать вместо него сертификат, связанный со шлюзом управления данными, см. раздел, посвященный [безопасной настройке учетных данных](#set-credentials-and-security).    
   3. Нажмите кнопку **Развернуть** на панели команд, чтобы развернуть связанную службу SQL Server. В представлении в виде дерева отобразится связанная служба.

      ![Связанная служба SQL Server в представлении в виде дерева](./media/data-factory-move-data-between-onprem-and-cloud/sql-linked-service-in-tree-view.png)    

#### <a name="add-a-linked-service-for-an-azure-storage-account"></a>Добавление связанной службы для учетной записи хранения Azure
1. В **редакторе фабрики данных** щелкните **Создать хранилище данных** на панели команд и выберите **Служба хранилища Azure**.
2. В поле **Имя учетной записи**введите имя учетной записи хранения Azure.
3. В поле **Ключ учетной записи**введите ключ учетной записи хранения Azure.
4. Щелкните **Развернуть**, чтобы развернуть службу **AzureStorageLinkedService**.

## <a name="create-datasets"></a>Создание наборов данных
На этом этапе вы создадите наборы входных и выходных данных, которые представляют собой входные и выходные данные для операции копирования (из локальной базы данных SQL Server в хранилище BLOB-объектов Azure). Прежде чем создавать наборы данных, необходимо сделать следующее (подробное описание шагов приводится далее):

* В базе данных SQL Server, добавленной в фабрику данных в качестве связанной службы, создайте таблицу с именем **emp** и вставьте в нее пару записей в качестве примера.
* В учетной записи хранения BLOB-объектов Azure, которую вы добавили в качестве связанной службы в фабрику данных, создайте контейнер BLOB-объектов с именем **adftutorial** .

### <a name="prepare-on-premises-sql-server-for-the-tutorial"></a>Подготовка локальной связанной службы SQL Server для учебника
1. В базе данных SQL Server, которую вы указали для локальных связанных служб (**SqlServerLinkedService**), используйте следующий сценарий SQL, чтобы создать в базе данных таблицу **emp**.

    ```SQL   
    CREATE TABLE dbo.emp
    (
        ID int IDENTITY(1,1) NOT NULL,
        FirstName varchar(50),
        LastName varchar(50),
        CONSTRAINT PK_emp PRIMARY KEY (ID)
    )
    GO
    ```
2. Вставьте несколько образцов в таблицу:

    ```SQL
    INSERT INTO emp VALUES ('John', 'Doe')
    INSERT INTO emp VALUES ('Jane', 'Doe')
    ```

### <a name="create-input-dataset"></a>Создание входного набора данных

1. В **редакторе фабрики данных** нажмите кнопку **... More** (...Дополнительно), на панели команд щелкните **Новый набор данных** и выберите **Таблица SQL Server**.
2. Замените сценарий JSON в области справа на следующий текст:

    ```JSON   
    {        
        "name": "EmpOnPremSQLTable",
        "properties": {
            "type": "SqlServerTable",
            "linkedServiceName": "SqlServerLinkedService",
            "typeProperties": {
                "tableName": "emp"
            },
            "external": true,
            "availability": {
                "frequency": "Hour",
                "interval": 1
            },
            "policy": {
                "externalData": {
                    "retryInterval": "00:01:00",
                    "retryTimeout": "00:10:00",
                    "maximumRetry": 3
                }
            }
        }
    }     
    ```     
   Обратите внимание на следующие моменты.

   * **type** имеет значение **SqlServerTable**.
   * **tablename** имеет значение **emp**.
   * Для **linkedServiceName** задано значение **SqlServerLinkedService** (вы создали эту связанную службу ранее в этом руководстве).
   * Если входной набор данных не создается другим конвейером фабрики данных Azure, для параметра **external** следует задать значение **true**. Это означает, что входные данные создаются вне службы фабрики данных Azure. При необходимости можно указать любые внешние политики данных с помощью **externalData** в разделе **Policy**.    

   Дополнительные сведения о свойствах файлов JSON см. в статье [Перемещение данных в базу данных SQL Server и обратно на локальных компьютерах и виртуальных машинах Azure IaaS с помощью фабрики данных Azure](data-factory-sqlserver-connector.md).
3. Нажмите кнопку **Развернуть** на панели команд, чтобы развернуть набор данных.  

### <a name="create-output-dataset"></a>Создание выходного набора данных

1. В **редакторе фабрики данных** щелкните на панели команд **Создать набор данных** и выберите **Хранилище BLOB-объектов Azure**.
2. Замените сценарий JSON в области справа на следующий текст:

    ```JSON   
    {
        "name": "OutputBlobTable",
        "properties": {
            "type": "AzureBlob",
            "linkedServiceName": "AzureStorageLinkedService",
            "typeProperties": {
                "folderPath": "adftutorial/outfromonpremdf",
                "format": {
                    "type": "TextFormat",
                    "columnDelimiter": ","
                }
            },
            "availability": {
                "frequency": "Hour",
                "interval": 1
            }
        }
     }
    ```   
   Обратите внимание на следующие моменты.

   * **type** имеет значение **AzureBlob**.
   * **linkedServiceName** имеет значение **AzureStorageLinkedService** (вы создали эту связанную службу на шаге 2).
   * **folderPath** имеет значение **adftutorial/outfromonpremdf**, где outfromonpremdf — это папка в контейнере adftutorial. Если контейнер **adftutorial** еще не существует, создайте его.
   * Параметр **availability** имеет значение **hourly** (параметру **frequency** присваивается значение **hour**, а параметру **interval** — значение **1**).  Служба фабрики данных каждый час создает срез выходных данных в таблице **emp** в базе данных SQL Azure.

   Если не указать **fileName** для **выходной таблицы**, то созданные в **folderPath** файлы получают имена в следующем формате: Data<Guid>.txt (например: Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt.).

   Чтобы динамически установить параметры **folderPath** и **fileName** на основе времени **SliceStart**, используйте свойство partitionedBy. В следующем примере folderPath использует год, месяц и день из SliceStart (время начала обработки среза), а в fileName используется время (часы) из SliceStart. Например, если срез выполняется для временной отметки 2014-10-20T08:00:00, folderName получает значение wikidatagateway/wikisampledataout/2014/10/20, а fileName – 08.csv.

    ```JSON
    "folderPath": "wikidatagateway/wikisampledataout/{Year}/{Month}/{Day}",
    "fileName": "{Hour}.csv",
    "partitionedBy":
    [

        { "name": "Year", "value": { "type": "DateTime", "date": "SliceStart", "format": "yyyy" } },
        { "name": "Month", "value": { "type": "DateTime", "date": "SliceStart", "format": "MM" } },
        { "name": "Day", "value": { "type": "DateTime", "date": "SliceStart", "format": "dd" } },
        { "name": "Hour", "value": { "type": "DateTime", "date": "SliceStart", "format": "hh" } }
    ],
    ```

    Дополнительные сведения о свойствах файлов JSON см. в статье [Перемещение данных в базу данных SQL Server и обратно на локальных компьютерах и виртуальных машинах Azure IaaS с помощью фабрики данных Azure](data-factory-azure-blob-connector.md).
3. Нажмите кнопку **Развернуть** на панели команд, чтобы развернуть набор данных. Оба набора данных должны отображаться в представлении в виде дерева.  

## <a name="create-pipeline"></a>Создание конвейера
На этом шаге вы создадите **конвейер** одним **действием копирования**, для выполнения которого **EmpOnPremSQLTable** будет использоваться как входные данные, а **OutputBlobTable** — как выходные данные.

1. В редакторе фабрики данных нажмите кнопку **... Дополнительно** и **Новый конвейер**.
2. Замените сценарий JSON в области справа на следующий текст:    

    ```JSON   
     {
         "name": "ADFTutorialPipelineOnPrem",
         "properties": {
         "description": "This pipeline has one Copy activity that copies data from an on-prem SQL to Azure blob",
         "activities": [
           {
             "name": "CopyFromSQLtoBlob",
             "description": "Copy data from on-prem SQL server to blob",
             "type": "Copy",
             "inputs": [
               {
                 "name": "EmpOnPremSQLTable"
               }
             ],
             "outputs": [
               {
                 "name": "OutputBlobTable"
               }
             ],
             "typeProperties": {
               "source": {
                 "type": "SqlSource",
                 "sqlReaderQuery": "select * from emp"
               },
               "sink": {
                 "type": "BlobSink"
               }
             },
             "Policy": {
               "concurrency": 1,
               "executionPriorityOrder": "NewestFirst",
               "style": "StartOfInterval",
               "retry": 0,
               "timeout": "01:00:00"
             }
           }
         ],
         "start": "2016-07-05T00:00:00Z",
         "end": "2016-07-06T00:00:00Z",
         "isPaused": false
       }
     }
    ```   
   > [!IMPORTANT]
   > Замените значение свойства **start** текущей датой, а значение свойства **end** — датой следующего дня.
   >
   >

   Обратите внимание на следующие моменты.

   * В разделе действий есть только действие, для параметра **type** которого задано значение **Copy**.
   * Для параметра действия **input** установлено значение **EmpOnPremSQLTable**, а для **output** — **OutputBlobTable**.
   * В разделе **typeProperties** в качестве **типа источника** указано значение **SqlSink**, а в качестве **типа приемника** — **BlobSource**.
   * Для свойства **sqlReaderQuery** типа **SqlSource** задан тип SQL-запроса `select * from emp`.

   Даты начала и окончания должны быть в [формате ISO](http://en.wikipedia.org/wiki/ISO_8601). Например, 2014-10-14T16:32:41Z. Время **окончания** указывать не обязательно, однако в этом примере мы будем его использовать.

   Если не указать значение свойства **end**, оно вычисляется по формуле "**время начала + 48 часов**". Чтобы запустить конвейер в течение неопределенного срока, укажите значение **9/9/9999** в качестве значения свойства **end**.

   Вы определяете интервал времени, в рамках которого выполняются срезы данных на основе свойств **доступности**, определенных для каждого набора данных Azure.

   В этом примере получено 24 среза данных, так как они создаются каждый час.        
3. Нажмите кнопку **Развернуть** на панели команд, чтобы развернуть набор данных (таблица представляет собой прямоугольный набор данных). Убедитесь, что конвейер отображается в представлении в виде дерева в узле **Конвейеры**.  
4. Нажмите кнопку **X** дважды, чтобы закрыть все страницы и вернуться к странице **Фабрика данных** для **ADFTutorialOnPremDF**.

**Поздравляем!** Вы успешно создали фабрику данных Azure, связанные службы, наборы данных и конвейер, а также выполнили планирование конвейера.

#### <a name="view-the-data-factory-in-a-diagram-view"></a>Просмотр фабрики данных в представлении схемы
1. На **портале Azure** щелкните элемент **Схема** на домашней странице фабрики данных **ADFTutorialOnPremDF**. :

    ![Ссылка на схему](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDiagramLink.png)
2. Вы должны увидеть схему, аналогичную приведенной ниже:

    ![Представление схемы](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDiagramView.png)

    Можно увеличивать и уменьшать масштаб, выбирать 100 %-ный масштаб или масштаб по размеру, автоматически размещать конвейеры и наборы данных, а также отображать сведения из журнала обращений и преобразований (выделение восходящих и нисходящих элементов для выбранных элементов).  Дважды щелкните объект (входной или выходной набор данных либо конвейер), чтобы просмотреть его свойства.

## <a name="monitor-pipeline"></a>Отслеживание конвейера
На этом шаге используется портал Azure для мониторинга фабрики данных Azure. Вы также можете использовать командлеты PowerShell для мониторинга наборов данных и конвейеров. Дополнительные сведения о мониторинге см. в статье [Мониторинг конвейеров фабрики данных Azure и управление ими](data-factory-monitor-manage-pipelines.md).

1. На схеме дважды щелкните **EmpOnPremSQLTable**.  

    ![Срезы EmpOnPremSQLTable](./media/data-factory-move-data-between-onprem-and-cloud/OnPremSQLTableSlicesBlade.png)
2. Обратите внимание, что для всех срезов данных выше установлено состояние **Готово**, так как в качестве значения продолжительности выполнения конвейера (от времени начала до времени окончания) указано значение в прошлом. Это также результат того, что вы вставили данные в базу данных SQL Server, и они находились там все время. Убедитесь, что в разделе **Проблемные срезы** в нижней части окна не показаны срезы. Чтобы просмотреть все срезы, щелкните **Подробности** под списком срезов.
3. Затем на странице **Наборы данных** щелкните **OutputBlobTable**.

    ![Срезы OputputBlobTable](./media/data-factory-move-data-between-onprem-and-cloud/OutputBlobTableSlicesBlade.png)
4. Щелкните любой срез данных в списке, чтобы отобразить страницу **Срез данных**. Для этого среза отобразится раздел "Запуски операции". Как правило, в этом разделе отображается только одно действие.  

    ![Колонка среза данных](./media/data-factory-move-data-between-onprem-and-cloud/DataSlice.png)

    Если срез не находится в состоянии **Готов**, вы можете увидеть восходящие срезы, которые не находятся в состоянии готовности и блокируют выполнение текущего среза в списке **Неготовые восходящие срезы**.
5. Щелкните **выполненное действие** в нижней части списка, чтобы просмотреть **дополнительные сведения о выполнении операции**.

   ![Страница "Подробности о выполнении операции"](./media/data-factory-move-data-between-onprem-and-cloud/ActivityRunDetailsBlade.png)

   Отобразятся такие сведения, как пропускная способность, продолжительность и шлюз, использованный для передачи данных.
6. Закройте все страницы, щелкая значок **X**, пока
7. не вернетесь к домашней странице **ADFTutorialOnPremDF**.
8. (Необязательно.) Щелкните **Конвейеры**, а затем — **ADFTutorialOnPremDF** и просмотрите параметры входных таблиц (**Использовано**) или выходных наборов данных (**Выполнено**).
9. Используйте инструменты, такие как [обозреватель хранилищ Microsoft](http://storageexplorer.com/), чтобы проверить, создается ли файл или большой двоичный объект каждый час.

   ![обозреватель хранилищ Azure](./media/data-factory-move-data-between-onprem-and-cloud/OnPremAzureStorageExplorer.png)

## <a name="next-steps"></a>Дополнительная информация
* Все дополнительные сведения о шлюзе управления данными см. в статье [Шлюз управления данными](data-factory-data-management-gateway.md).
* Чтобы узнать, как перемещать данные из исходного хранилища данных в приемник данных с помощью действия копирования, ознакомьтесь со статьей [Копирование данных из хранилища BLOB-объектов Azure в базу данных SQL с помощью фабрики данных](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md) .
