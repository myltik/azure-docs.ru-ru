---
title: Руководство по созданию конвейера фабрики данных Azure для копирования данных (портал Azure) | Документация Майкрософт
description: В этом руководстве вы будете использовать портал Azure, чтобы создать конвейер с действием копирования фабрики данных Azure для копирования данных из хранилища BLOB-объектов Azure в базу данных SQL Azure.
services: data-factory
documentationcenter: ''
author: linda33wj
manager: ''
editor: ''
ms.assetid: d9317652-0170-4fd3-b9b2-37711272162b
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: get-started-article
ms.date: 01/22/2018
ms.author: jingwang
robots: noindex
ms.openlocfilehash: c4e8de6b229fe027e43a2b13f2a7ad54b7c3eaf2
ms.sourcegitcommit: 9cdd83256b82e664bd36991d78f87ea1e56827cd
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/16/2018
---
# <a name="tutorial-use-azure-portal-to-create-a-data-factory-pipeline-to-copy-data"></a>Руководство по создания конвейера фабрики данных для копирования данных с помощью портала Azure 
> [!div class="op_single_selector"]
> * [Обзор и предварительные требования](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md)
> * [Мастер копирования](data-factory-copy-data-wizard-tutorial.md)
> * [портал Azure](data-factory-copy-activity-tutorial-using-azure-portal.md)
> * [Visual Studio](data-factory-copy-activity-tutorial-using-visual-studio.md)
> * [PowerShell](data-factory-copy-activity-tutorial-using-powershell.md)
> * [Шаблон Azure Resource Manager](data-factory-copy-activity-tutorial-using-azure-resource-manager-template.md)
> * [REST API](data-factory-copy-activity-tutorial-using-rest-api.md)
> * [API для .NET](data-factory-copy-activity-tutorial-using-dotnet-api.md)
> 

> [!NOTE]
> Статья относится к версии 1 фабрики данных, которая является общедоступной версией. Если вы используете версию 2 службы фабрики данных, которая находится на этапе предварительной версии, прочитайте [руководство по действиям копирования в версии 2](../quickstart-create-data-factory-dot-net.md). 

В этом руководстве показано, как создать фабрику данных c конвейером, который копирует данные из хранилища BLOB-объектов Azure в базу данных SQL Azure, с помощью [портала Azure](https://portal.azure.com). Если вы еще не работали с фабрикой данных Azure, перед выполнением действий, описанных в этом руководстве, ознакомьтесь со статьей [Введение в фабрику данных Azure](data-factory-introduction.md).   

В этом руководстве описывается создание конвейера с одним действием — действием копирования. Действие копирования копирует данные из поддерживаемого хранилища данных в поддерживаемое хранилище данных-приемник. Список хранилищ данных, которые поддерживаются в качестве источников и приемников, см. в разделе [Поддерживаемые хранилища данных и форматы](data-factory-data-movement-activities.md#supported-data-stores-and-formats). Это действие выполняется с помощью глобально доступной службы, обеспечивающей безопасное, надежное и масштабируемое копирование данных между разными хранилищами. Дополнительные сведения о действии копирования см. в статье [Перемещение данных с помощью действия копирования](data-factory-data-movement-activities.md).

Конвейер может содержать сразу несколько действий. Два действия можно объединить в цепочку (выполнить одно действие вслед за другим), настроив выходной набор данных одного действия как входной набор данных другого действия. Дополнительные сведения см. в разделе [Несколько действий в конвейере](data-factory-scheduling-and-execution.md#multiple-activities-in-a-pipeline). 

> [!NOTE] 
> В этом руководстве конвейер данных копирует данные из исходного хранилища данных в целевое. Инструкции по преобразованию данных с помощью фабрики данных Azure см. в [руководстве по созданию конвейера для преобразования данных с помощью кластера Hadoop](data-factory-build-your-first-pipeline.md).

## <a name="prerequisites"></a>предварительным требованиям
Прежде чем начать работу, выполните предварительные требования, перечисленные в [этом руководстве](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).

## <a name="steps"></a>Действия
Ниже приведены шаги, которые вы выполните в процессе работы с этим руководством.

1. Создайте **фабрику данных** Azure. На этом этапе вы создадите фабрику данных Azure с именем ADFTutorialDataFactory. 
2. Создайте в этой фабрике данных **связанные службы**. На этом этапе вы создадите две связанные службы — службу хранилища Azure и базу данных SQL Azure. 
    
    Связанная служба хранилища Azure связывает учетную запись хранения Azure с фабрикой данных. Вы создали контейнер и отправили данные в эту учетную запись хранения в ходе выполнения предварительных [требований](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).   

    Связанная служба SQL Azure связывает базу данных SQL Azure с фабрикой данных. В этой базе данных хранятся данные, скопированные из хранилища BLOB-объектов. Вы создали таблицу SQL в этой базе данных в ходе выполнения [предварительных требований](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).   
3. Создайте в фабрике данных входные и выходные **наборы данных**.  
    
    Связанная служба хранилища Azure указывает строку подключения, которую фабрика данных использует во время выполнения, чтобы подключиться к учетной записи хранения Azure. А входной набор данных больших двоичных объектов определяет контейнер и папку с входными данными.  

    Аналогичным образом связанная служба базы данных SQL Azure указывает строку подключения, которую служба фабрики данных использует во время выполнения, чтобы подключиться к базе данных SQL Azure. А выходной набор данных таблицы SQL определяет таблицу в базе данных, в которую копируются данные из хранилища BLOB-объектов.
4. Создайте **конвейер** в фабрике данных. На этом этапе вы создадите конвейер с действием копирования.   
    
    Действие копирования копирует данные из большого двоичного объекта из хранилища BLOB-объектов Azure в таблицу в базе данных SQL Azure. Его можно использовать, чтобы копировать данные из любого поддерживаемого источника в любое расположение. Список поддерживаемых хранилищ данных см. в [этом разделе](data-factory-data-movement-activities.md#supported-data-stores-and-formats). 
5. Выполните мониторинг конвейера. На этом этапе вы **отследите** срезы входных и выходных наборов данных с помощью портала Azure. 

## <a name="create-data-factory"></a>Создание фабрики данных
> [!IMPORTANT]
> Выполните [предварительные требования, необходимые для работы с этим руководством](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md), если вы еще этого не сделали.   

Фабрика данных может иметь один или несколько конвейеров. Конвейер может содержать одно или несколько действий. Это может быть, например, действие копирования данных из исходного хранилища в целевое или действие HDInsight Hive для выполнения скрипта Hive, преобразующего входные данные в выходные данные продукта. Начнем с создания фабрики данных.

1. Войдите на [портал Azure](https://portal.azure.com/), нажмите кнопку **Создать ресурс** в меню слева, откройте раздел **Данные и аналитика** и щелкните **Фабрика данных**. 
   
   ![Создать -> Фабрика данных](./media/data-factory-copy-activity-tutorial-using-azure-portal/NewDataFactoryMenu.png)    
2. В колонке **Создать фабрику данных** выполните следующие действия.
   
   1. Введите **ADFTutorialDataFactory** в поле **Имя**. 
      
         ![Создать колонку "Фабрика данных"](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-new-data-factory.png)
      
       Имя фабрики данных Azure должно быть **глобально уникальным**. При возникновении указанной ниже ошибки измените имя фабрики данных (например, на ваше_имя_ADFTutorialDataFactory) и попробуйте создать фабрику данных снова. Ознакомьтесь с разделом [Фабрика данных — правила именования](data-factory-naming-rules.md) , чтобы узнать о правилах именования артефактов фабрики данных.
      
           Data factory name “ADFTutorialDataFactory” is not available  
      
       ![Имя фабрики данных недоступно](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-data-factory-not-available.png)
   2. Выберите **подписку** Azure, в рамках которой вы хотите создать фабрику данных. 
   3. Для **группы ресурсов** выполните одно из следующих действий.
      
      - Выберите **Использовать существующую**и укажите существующую группу ресурсов в раскрывающемся списке. 
      - Выберите **Создать новую**и укажите имя группы ресурсов.   
         
          Некоторые действия, описанные в этом руководстве, предполагают, что для группы ресурсов используется имя **ADFTutorialResourceGroup**. Сведения о группах ресурсов см. в статье, где описывается [использование групп ресурсов для управления ресурсами Azure](../../azure-resource-manager/resource-group-overview.md).  
   4. Укажите **расположение** фабрики данных. В раскрывающемся списке отображаются только те регионы, которые поддерживаются службой фабрики данных.
   5. Кроме того, установите флажок **Закрепить на панели мониторинга**.     
   6. Нажмите кнопку **Создать**.
      
      > [!IMPORTANT]
      > Создавать экземпляры фабрики данных может пользователь с ролью [Участник фабрики данных](../../role-based-access-control/built-in-roles.md#data-factory-contributor) на уровне подписки или группы ресурсов.
      > 
      > В будущем имя фабрики данных может быть зарегистрировано в качестве DNS-имени и, следовательно, стать отображаемым.                
      > 
      > 
3. На панели мониторинга вы увидите приведенный ниже элемент с состоянием **Deploying data factory** (Развертывание фабрики данных). 

    ![Элемент Deploying data factory (Развертывание фабрики данных)](media/data-factory-copy-activity-tutorial-using-azure-portal/deploying-data-factory.png)
4. Когда экземпляр будет создан, вы увидите колонку **Фабрика данных** , как показано на рисунке ниже.
   
   ![Домашняя страница фабрики данных](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-data-factory-home-page.png)

## <a name="create-linked-services"></a>Создание связанных служб
Связанная служба в фабрике данных связывает хранилища данных и службы вычислений с фабрикой данных. В этом руководстве не используются службы вычислений, например Azure HDInsight или Azure Data Lake Analytics. Вы используете два хранилища данных — служба хранилища Azure (источник) и база данных SQL Azure (конечное хранилище). 

Поэтому нужно создать две связанные службы: служба хранилища Azure с именем AzureStorageLinkedService и база данных SQL Azure с именем AzureSqlLinkedService.  

Связанная служба хранилища Azure связывает учетную запись хранения Azure с фабрикой данных. В этой учетной записи хранения вы создали контейнер и отправили в нее данные в ходе выполнения предварительных [требований](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).   

Связанная служба SQL Azure связывает базу данных SQL Azure с фабрикой данных. В этой базе данных хранятся данные, скопированные из хранилища BLOB-объектов. Вы создали пустую таблицу в этой базе данных в ходе выполнения [предварительных требований](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).  

### <a name="create-azure-storage-linked-service"></a>Создание связанной службы хранения Azure
На этом шаге вы свяжете учетную запись хранения Azure с фабрикой данных. В этом разделе вы укажете имя и ключ вашей учетной записи хранения Azure.  

1. В колонке **Фабрика данных** щелкните элемент **Создать и развернуть**.
   
   ![Плитка "Создание и развертывание"](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-author-deploy-tile.png) 
2. Вы увидите **редактор фабрики данных**, как показано на следующем рисунке: 

    ![Редактор фабрики данных](./media/data-factory-copy-activity-tutorial-using-azure-portal/data-factory-editor.png)
3. В редакторе нажмите кнопку **Новое хранилище данных** на панели инструментов и выберите в раскрывающемся меню пункт **Служба хранилища Azure**. На панели справа должен появиться шаблон JSON для создания связанной службы хранилища Azure. 
   
    ![Кнопка "Создать хранилище данных" в редакторе](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-editor-newdatastore-button.png)    
3. Замените `<accountname>` и `<accountkey>` значениями имени и ключа учетной записи хранения Azure. 
   
    ![Хранилище больших двоичных объектов JSON в редакторе](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-editor-blob-storage-json.png)    
4. На панели инструментов щелкните **Развернуть** . Теперь в иерархическом представлении должен отобразиться развернутый экземпляр **AzureStorageLinkedService** . 
   
    ![Развертывание хранилища больших двоичных объектов в редакторе](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-editor-blob-storage-deploy.png)

    Дополнительные сведения о свойствах JSON см. в статье о [соединителе хранилища BLOB-объектов Azure](data-factory-azure-blob-connector.md#linked-service-properties).

### <a name="create-a-linked-service-for-the-azure-sql-database"></a>Создание связанной службы для базы данных SQL Azure.
На этом шаге вы свяжете базу данных SQL Azure с фабрикой данных. В этом разделе вы укажете имя сервера SQL Azure, имя базы данных, имя пользователя и пароль. 

1. В **редакторе фабрики данных** на панели инструментов щелкните **Новое хранилище данных** и выберите в раскрывающемся меню пункт **База данных SQL Azure**. В правой панели должен появиться шаблон JSON для создания связанной службы SQL Azure.
2. Замените `<servername>`, `<databasename>`, `<username>@<servername>` и `<password>` именем своего сервера SQL Azure, именем учетной записи пользователя и паролем. 
3. На панели инструментов щелкните **Развернуть**, чтобы создать и развернуть экземпляр **AzureSqlLinkedService**.
4. Экземпляр **AzureSqlLinkedService** должен отображаться в иерархическом представлении в разделе **Связанные службы**.  

    Дополнительные сведения об этих свойствах JSON см. в статье о [соединителе базы данных SQL Azure](data-factory-azure-sql-connector.md#linked-service-properties).

## <a name="create-datasets"></a>Создание наборов данных
На предыдущем шаге вы создали связанные службы, связывающие учетную запись хранения Azure и базу данных SQL Azure с фабрикой данных. На этом этапе вы определите два набора данных (InputDataset и OutputDataset), представляющие входные и выходные данные в хранилищах данных, на которые ссылаются службы AzureStorageLinkedService и AzureSqlLinkedService соответственно.

Связанная служба хранилища Azure указывает строку подключения, которую фабрика данных использует во время выполнения, чтобы подключиться к учетной записи хранения Azure. А входной набор данных больших двоичных объектов (InputDataset) определяет контейнер и папку с входными данными.  

Аналогичным образом связанная служба базы данных SQL Azure указывает строку подключения, которую служба фабрики данных использует во время выполнения, чтобы подключиться к базе данных SQL Azure. А выходной набор данных таблицы SQL (OututDataset) определяет таблицу в базе данных, в которую копируются данные из хранилища BLOB-объектов. 

### <a name="create-input-dataset"></a>Создание входного набора данных
На этом этапе вы создадите набор данных с именем InputDataset. Он указывает на файл большого двоичного объекта (emp.txt) в корневой папке контейнера больших двоичных объектов в службе хранилища Azure, которая представлена связанной службой AzureStorageLinkedService. Если не указать значение fileName (или пропустить его), данные из всех больших двоичных объектов в папке входных данных копируются в целевое расположение. В этом руководстве вы укажете значение параметра fileName. 

1. В **редакторе** фабрики данных щелкните **... Дополнительно**, **Новый набор данных**, а затем в раскрывающемся меню выберите пункт **Хранилище BLOB-объектов**. 
   
    ![Меню "Новый набор данных"](./media/data-factory-copy-activity-tutorial-using-azure-portal/new-dataset-menu.png)
2. Замените JSON на правой панели следующим фрагментом JSON: 
   
    ```json
    {
      "name": "InputDataset",
      "properties": {
        "structure": [
          {
            "name": "FirstName",
            "type": "String"
          },
          {
            "name": "LastName",
            "type": "String"
          }
        ],
        "type": "AzureBlob",
        "linkedServiceName": "AzureStorageLinkedService",
        "typeProperties": {
          "folderPath": "adftutorial/",
          "fileName": "emp.txt",
          "format": {
            "type": "TextFormat",
            "columnDelimiter": ","
          }
        },
        "external": true,
        "availability": {
          "frequency": "Hour",
          "interval": 1
        }
      }
    }
    ```   

    В следующей таблице приведены описания свойств JSON, используемых в этом фрагменте кода.

    | Свойство | ОПИСАНИЕ |
    |:--- |:--- |
    | Тип | Для свойства типа задано значение **AzureBlob**, так как данные хранятся в хранилище BLOB-объектов Azure. |
    | linkedServiceName | Ссылается на созданную ранее службу **AzureStorageLinkedService**. |
    | folderPath | Определяет **контейнер** больших двоичных объектов и **папку**, которая содержит входные большие двоичные объекты. В этом руководстве adftutorial — это контейнер больших двоичных объектов, а созданная папка является корневой. | 
    | fileName | Это необязательное свойство. Если это свойство не указано, выбираются все файлы из папки folderPath. В этом руководстве для свойства fileName указывается значение **emp.txt**, чтобы обрабатывался только этот файл. |
    | format -> type |Входной файл имеет текстовый формат, поэтому укажите значение **TextFormat**. |
    | columnDelimiter | Столбцы во входном файле разделяются **запятыми (`,`)**. |
    | frequency и interval | Для свойства frequency задано значение **Hour**, а для свойства interval — значение **1**. Это означает, что срезы входных данных будут создаваться **каждый час**. Иными словами, служба фабрики данных будет искать входные данные в корневой папке указанного контейнера BLOB-объектов (**adftutorial**) каждый час. Поиск данных осуществляется в пределах времени начала и времени окончания для конвейера, но не перед этим периодом или после него.  |
    | external | Это свойство имеет значение **true**, если этот конвейер не создает данные. В этом руководстве входные данные находятся в файле emp.txt, который не создается этим конвейером, поэтому мы присвоим этому свойству значение true. |

    Дополнительные сведения об этих свойствах JSON см. в [этом разделе](data-factory-azure-blob-connector.md#dataset-properties).      
3. На панели инструментов щелкните **Развернуть**, чтобы создать и развернуть набор данных **InputDataset**. Набор данных **InputDataset** должен отображаться в иерархическом представлении.

### <a name="create-output-dataset"></a>Создание выходного набора данных
База данных SQL Azure указывает строку подключения, которую служба фабрики данных использует во время выполнения, чтобы подключиться к базе данных SQL Azure. Выходной набор данных таблицы SQL (OututDataset), который вы создаете на этом шаге, определяет таблицу в базе данных, в которую копируются данные из хранилища BLOB-объектов.

1. В **редакторе** фабрики данных щелкните **... Дополнительно**, **Новый набор данных**, а затем в раскрывающемся меню выберите пункт **Azure SQL**. 
2. Замените JSON на правой панели следующим фрагментом JSON:

    ```json   
    {
      "name": "OutputDataset",
      "properties": {
        "structure": [
          {
            "name": "FirstName",
            "type": "String"
          },
          {
            "name": "LastName",
            "type": "String"
          }
        ],
        "type": "AzureSqlTable",
        "linkedServiceName": "AzureSqlLinkedService",
        "typeProperties": {
          "tableName": "emp"
        },
        "availability": {
          "frequency": "Hour",
          "interval": 1
        }
      }
    }
    ```     

    В следующей таблице приведены описания свойств JSON, используемых в этом фрагменте кода.

    | Свойство | ОПИСАНИЕ |
    |:--- |:--- |
    | Тип | Свойство type имеет значение **AzureSqlTable**, так как данные копируются в таблицу в базе данных SQL Azure. |
    | linkedServiceName | Ссылается на созданную ранее службу **AzureSqlLinkedService**. |
    | tableName | Указывает **таблицу**, в которую копируются данные. | 
    | frequency и interval | Для свойства frequency задано значение **Hour**, а для interval — **1**. Это означает, что срезы выходных данных создаются **каждый час** в пределах времени начала и времени окончания для конвейера, но не перед этим периодом или после него.  |

    В таблице emp в базе данных есть три столбца: **ID**, **FirstName** и **LastName**. ID — это столбец для идентификаторов, поэтому здесь вам нужно указать только значения **FirstName** и **LastName**.

    Дополнительные сведения об этих свойствах JSON см. в [этом разделе](data-factory-azure-sql-connector.md#dataset-properties).
3. На панели инструментов щелкните **Развернуть**, чтобы создать и развернуть набор данных **OutputDatase**. Набор данных **OutputDataset** должен отображаться в иерархическом представлении в разделе **Наборы данных**. 

## <a name="create-pipeline"></a>Создание конвейера
На этом этапе вы создадите конвейер с **действием копирования**, которое использует **InputDataset** в качестве входных данных и **OutputDataset** в качестве выходных.

Сейчас на основе этого набора настраивается расписание. В этом руководстве выходной набор данных создает срез раз в час. Для конвейера настроено время начала и время окончания с разницей в сутки. Таким образом, конвейер создает 24 среза для выходного набора данных. 

1. В **редакторе** фабрики данных щелкните **... Дополнительно** и **Новый конвейер**. Кроме того, можно щелкнуть правой кнопкой мыши **Конвейеры** в древовидном представлении и выбрать **Создать конвейер**.
2. Замените JSON на правой панели следующим фрагментом JSON: 

    ```json   
    {
      "name": "ADFTutorialPipeline",
      "properties": {
        "description": "Copy data from a blob to Azure SQL table",
        "activities": [
          {
            "name": "CopyFromBlobToSQL",
            "type": "Copy",
            "inputs": [
              {
                "name": "InputDataset"
              }
            ],
            "outputs": [
              {
                "name": "OutputDataset"
              }
            ],
            "typeProperties": {
              "source": {
                "type": "BlobSource"
              },
              "sink": {
                "type": "SqlSink",
                "writeBatchSize": 10000,
                "writeBatchTimeout": "60:00:00"
              }
            },
            "Policy": {
              "concurrency": 1,
              "executionPriorityOrder": "NewestFirst",
              "retry": 0,
              "timeout": "01:00:00"
            }
          }
        ],
        "start": "2017-05-11T00:00:00Z",
        "end": "2017-05-12T00:00:00Z"
      }
    } 
    ```   
    
    Обратите внимание на следующие моменты.
   
    - В разделе действий доступно только одно действие, параметр **type** которого имеет значение **Copy**. Дополнительные сведения о действии копирования см. в статье [Перемещение данных с помощью действия копирования](data-factory-data-movement-activities.md). В решениях фабрики данных можно также использовать [действия преобразования данных](data-factory-data-transformation-activities.md).
    - Для этого действия параметру input присвоено значение **InputDataset**, а параметру output — значение **OutputDataset**. 
    - В разделе **typeProperties** в качестве типа источника указано **BlobSource**, а в качестве типа приемника — **SqlSink**. Список хранилищ данных, поддерживаемых действием копирования в качестве источников и приемников, см. в разделе [Поддерживаемые хранилища данных и форматы](data-factory-data-movement-activities.md#supported-data-stores-and-formats). Чтобы узнать, как использовать конкретное хранилище данных в качестве источника или приемника, щелкните ссылку в таблице.
    - Даты начала и окончания должны быть в [формате ISO](http://en.wikipedia.org/wiki/ISO_8601). Например, 2016-10-14T16:32:41Z. Время **окончания** указывать не обязательно, однако в этом примере мы будем его использовать. Если не указать значение свойства **end**, оно вычисляется по формуле "**время начала + 48 часов**". Чтобы запустить конвейер в течение неопределенного срока, укажите значение **9999-09-09** в качестве значения свойства **end**.
     
    В примере выше получено 24 среза данных, так как они создаются каждый час.

    Описание свойств JSON в определении конвейера см. в статье [Конвейеры и действия в фабрике данных Azure](data-factory-create-pipelines.md). Описание свойств JSON в определении действия копирования см. в статье [Перемещение данных с помощью действия копирования](data-factory-data-movement-activities.md). Описание свойств JSON, поддерживаемых BlobSource, см. в статье о [соединителе больших двоичных объектов Azure](data-factory-azure-blob-connector.md). Описание свойств JSON, поддерживаемых SqlSink, см. в статье о [соединителе базы данных SQL Azure](data-factory-azure-sql-connector.md).
3. Щелкните **Развернуть** на панели инструментов, чтобы создать и развернуть конвейер **ADFTutorialPipeline**. Убедитесь, что конвейер отображается в иерархической структуре. 
4. Теперь закройте колонку **Редактор**, щелкнув **X**. Щелкните **X** снова, чтобы отобразить домашнюю страницу **фабрики данных** для экземпляра **ADFTutorialDataFactory**.

**Поздравляем!** Фабрика данных Azure с конвейером, который копирует данные из хранилища BLOB-объектов Azure в базу данных SQL Azure, успешно создана. 


## <a name="monitor-pipeline"></a>Отслеживание конвейера
На этом шаге используется портал Azure для мониторинга фабрики данных Azure.    

### <a name="monitor-pipeline-using-monitor--manage-app"></a>Мониторинг конвейера с использованием приложения по мониторингу и управлению
Ниже приведены действия по мониторингу конвейеров в фабрике данных с помощью приложения "Мониторинг и управление". 

1. Щелкните плитку **Monitor & Manage** (Мониторинг и управление) на домашней странице фабрики данных.
   
    ![Плитка Monitor & Manage (Мониторинг и управление)](./media/data-factory-copy-activity-tutorial-using-azure-portal/monitor-manage-tile.png) 
2. **Приложение "Мониторинг и управление"** должно появиться на отдельной вкладке. 

    > [!NOTE]
    > Если веб-браузер завис на действии "Авторизация...", сделайте следующее: снимите флажок **Block third-party cookies and site data** (Блокировать сторонние файлы cookie и данные сайта) или создайте исключение для адреса **login.microsoftonline.com**, а затем попробуйте открыть приложение еще раз.

    ![Приложение по мониторингу и управлению](./media/data-factory-copy-activity-tutorial-using-azure-portal/monitor-and-manage-app.png)
3. Измените значения параметров **Время начала** и **Время окончания**, чтобы они включали соответствующие значения (2017-05-11 и 2017-05-12) для конвейера, и щелкните **Применить**.       
3. В списке в центре страницы отображаются **окна действий**, связанные с каждым часом в промежутке между временем начала и окончания конвейера. 
4. Чтобы просмотреть сведения об окне действия, выберите его в списке **Activity Windows** (Окна действий). 
    ![Сведения об окне действия](./media/data-factory-copy-activity-tutorial-using-azure-portal/activity-window-details.png)

    В обозревателе окон действий справа вы увидите, что все срезы вплоть до текущего времени в формате UTC (20:12) обработаны (отображаются зеленым цветом). Срезы, полученные в промежутке с 20:00 до 21:00, с 21:00 до 22:00, с 22:00 до 23:00, с 23:00 до 00:00, еще не обработаны.

    В разделе **Попытки** в области справа содержатся сведения о выполнении действия для среза данных. Если при выполнении произошла ошибка, в этом разделе отображаются сведения о ней. Например, если входная папка или контейнер еще не созданы и произошел сбой обработки среза, вы увидите соответствующее сообщение об ошибке.

    ![Попытки выполнения действия](./media/data-factory-copy-activity-tutorial-using-azure-portal/activity-run-attempts.png) 
4. Запустите **SQL Server Management Studio**, подключитесь к базе данных SQL Azure и убедитесь, что строки вставляются в таблицу **emp** в базе данных.
    
    ![результаты SQL-запроса](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-sql-query-results.png)

Дополнительные сведения об использовании этого приложения см. в статье [Мониторинг конвейеров фабрики данных Azure и управление ими с помощью нового приложения по мониторингу и управлению](data-factory-monitor-manage-app.md).

### <a name="monitor-pipeline-using-diagram-view"></a>Мониторинг конвейера с использованием представления схемы
Конвейеры данных также можно отслеживать, используя представление схемы.  

1. В колонке **Фабрика данных** щелкните **Схема**.
   
    ![Колонка "Фабрика данных" — плитка схемы](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-datafactoryblade-diagramtile.png)
2. Вы должны увидеть схему, аналогичную приведенной ниже: 
   
    ![Представление схемы](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-diagram-blade.png)  
5. В представлении схемы дважды щелкните входной набор данных **InputDataset**, чтобы просмотреть его срезы.  
   
    ![Наборы данных с выбранным элементом InputDataset](./media/data-factory-copy-activity-tutorial-using-azure-portal/DataSetsWithInputDatasetFromBlobSelected.png)   
5. Щелкните ссылку **Еще**, чтобы отобразить все срезы данных. Вы увидите срезы, созданные в течение 24 часов в промежутке между временем начала и окончания конвейера. 
   
    ![Все срезы входных данных](./media/data-factory-copy-activity-tutorial-using-azure-portal/all-input-slices.png)  
   
    Обратите внимание, что срезы данных вплоть до текущего времени (в формате UTC) находятся в состоянии **Готово**, так как файл **emp.txt** все это время находится в контейнере больших двоичных объектов **adftutorial\input**. Срезы для будущих периодов пока что не находятся в состоянии "Готово". Убедитесь, что срезы не отображаются в разделе **Срезы, в которых недавно произошел сбой** в нижней части.
6. Закрывайте колонки, пока не появится представление схемы, или прокрутите окно влево. Затем дважды щелкните **OutputDataset**. 
8. Щелкните ссылку **Еще** в колонке **Таблица** набора данных **OutputDataset**, чтобы увидеть все его срезы.

    ![Колонка срезов данных](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-dataslices-blade.png) 
9. Обратите внимание, что срезы данных вплоть до текущего времени (в формате UTC) перешли из состояния **Ожидает выполнения** в состояние **Выполняется** ==> **Готово**. По умолчанию срезы, полученные в прошлом (до текущего времени), обрабатываются по порядку от последних до самых старых. Например, если сейчас 20:12 (UTC), сначала обрабатывается срез, полученный в промежутке с 19:00 до 20:00, а затем срез, полученный в промежутке с 18:00 до 19:00. По умолчанию срез, полученный в промежутке с 20:00 до 21:00, обрабатывается в конце временного интервала, то есть после 21:00.  
10. Щелкните любой срез данных в списке, чтобы отобразить колонку **Срез данных** . Фрагмент данных, связанный с окном действия, называется срезом. Срез может быть одним файлом или несколькими файлами.  
    
     ![Колонка среза данных](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-dataslice-blade.png)
    
     Если срез не находится в состоянии **Готов**, вы можете увидеть восходящие срезы, которые не находятся в состоянии готовности и блокируют выполнение текущего среза в списке **Неготовые восходящие срезы**.
11. В колонке **СРЕЗ ДАННЫХ** в списке в нижней части окна отображаются все выполненные действия. Щелкните **выполняемое действие**, чтобы просмотреть колонку **Подробности о выполнении операции**. 
    
    ![Сведения о выполнении действия](./media/data-factory-copy-activity-tutorial-using-azure-portal/ActivityRunDetails.png)

    В этой колонке приведены сведения о времени выполнения операции копирования, пропускной способности, объеме записанных и прочитанных данных, времени начала и окончания выполнения и т. д.  
12. С помощью кнопки **X** закройте все колонки, чтобы вернуться к начальной колонке **ADFTutorialDataFactory**.
13. Чтобы открыть колонки, используемые на предыдущих этапах, щелкните элемент **Наборы данных** или **Конвейеры**. 
14. Запустите **SQL Server Management Studio**, подключитесь к базе данных SQL Azure и убедитесь, что строки вставляются в таблицу **emp** в базе данных.
    
    ![результаты SQL-запроса](./media/data-factory-copy-activity-tutorial-using-azure-portal/getstarted-sql-query-results.png)


## <a name="summary"></a>Сводка
В этом учебнике вы создали фабрику данных Azure для копирования данных из большого двоичного объекта Azure в базу данных SQL Azure. Вы использовали портал Azure для создания фабрики данных, связанных служб, наборов данных и конвейера. Вот обобщенные действия, которые вы выполнили в этом руководстве:  

1. Создание **фабрики данных Azure**.
2. Создание **связанных служб**.
   1. **Служба хранилища Azure** — связанная служба для связи с учетной записью хранения Azure, которая содержит входные данные.     
   2. **SQL Azure** — связанная служба для связи с базой данных SQL Azure, которая содержит выходные данные. 
3. Создание **наборов данных** , которые описывают входные и выходные данные для конвейеров.
4. Создание **конвейера** с **BlobSource** в качестве источника и **SqlSink** в качестве приемника с помощью **действия копирования**.  

## <a name="next-steps"></a>Дополнительная информация
В этом руководстве в ходе операции копирования вы использовали хранилище BLOB-объектов Azure как исходное хранилище данных, а базу данных SQL Azure — как целевое хранилище данных. В следующей таблице приведен список хранилищ данных, которые поддерживаются в качестве источников и целевых расположений для действия копирования. 

[!INCLUDE [data-factory-supported-data-stores](../../../includes/data-factory-supported-data-stores.md)]

Чтобы получить дополнительные сведения о том, как скопировать данные в хранилище данных или из него, щелкните ссылку для хранилища данных в таблице.