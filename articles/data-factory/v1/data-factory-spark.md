---
title: Вызов программ Spark из фабрики данных Azure | Документация Майкрософт
description: Узнайте, как вызывать программы Spark из фабрики данных Azure с помощью действия MapReduce.
services: data-factory
documentationcenter: ''
author: sharonlo101
manager: ''
editor: ''
ms.assetid: fd98931c-cab5-4d66-97cb-4c947861255c
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: conceptual
ms.date: 01/10/2018
ms.author: shlo
robots: noindex
ms.openlocfilehash: d22829217209b7d0b1b5690d6a864b58bf102e3c
ms.sourcegitcommit: 266fe4c2216c0420e415d733cd3abbf94994533d
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 06/01/2018
ms.locfileid: "34622284"
---
# <a name="invoke-spark-programs-from-azure-data-factory-pipelines"></a>Вызов программ Spark из конвейеров фабрики данных Azure

> [!div class="op_single_selector" title1="Transformation Activities"]
> * [Действие Hive](data-factory-hive-activity.md)
> * [Действие Pig](data-factory-pig-activity.md)
> * [Действие MapReduce](data-factory-map-reduce.md)
> * [Действие потоковой передачи Hadoop](data-factory-hadoop-streaming-activity.md)
> * [Действие Spark](data-factory-spark.md)
> * [Создание прогнозирующих конвейеров с помощью машинного обучения Azure и фабрики данных Azure](data-factory-azure-ml-batch-execution-activity.md)
> * [Действие "Обновить ресурс" в службе машинного обучения](data-factory-azure-ml-update-resource-activity.md)
> * [Действие хранимой процедуры](data-factory-stored-proc-activity.md)
> * [Действие U-SQL в Data Lake Analytics](data-factory-usql-activity.md)
> * [Настраиваемое действие .NET](data-factory-use-custom-activities.md)

> [!NOTE]
> Статья относится к версии 1 фабрики данных Azure, которая является общедоступной. Если вы используете версию 2 службы фабрики данных, которая находится на этапе предварительной версии, см. статью о [преобразовании данных с помощью действия Apache Spark в фабрике данных Azure версии 2](../transform-data-using-spark.md).

## <a name="introduction"></a>Введение
Действие Spark — это одно из [действий преобразования данных](data-factory-data-transformation-activities.md), которое поддерживает фабрика данных. Это действие запускает указанную программу Spark в кластере Spark в Azure HDInsight. 

> [!IMPORTANT]
> - Действие Spark не поддерживает кластеры HDInsight Spark, использующие Azure Data Lake Store в качестве основного хранилища.
> - Оно поддерживает только имеющиеся (собственные) кластеры HDInsight Spark. Связанная служба HDInsight по запросу не поддерживается.

## <a name="walkthrough-create-a-pipeline-with-a-spark-activity"></a>Пошаговое руководство по созданию конвейера с действием Spark
Ниже приведены стандартные действия, необходимые для создания конвейера фабрики данных с действием Spark. 

* Создадите фабрику данных.
* Создайте связанную службу хранилища Azure для связи хранилища, которое связано с кластером HDInsight Spark, с фабрикой данных.
* Создайте связанную службу HDInsight, чтобы связать кластер Spark в HDInsight с фабрикой данных.
* Создайте набор данных, который ссылается на связанную службу хранилища. Затем определите выходной набор данных для действия, даже если выходные данные не выдаются. 
* Создайте конвейер с действием Spark, который ссылается на созданную связанную службу HDInsight. Конфигурация действия выполняется на основе набора данных, созданного на предыдущем шаге в качестве выходного набора данных. На основе этого набора настраивается расписание (ежечасно, ежедневно). Исходя из сказанного выше, вы должны определить выходной набор данных, даже если действие не выдает выходные данные.

### <a name="prerequisites"></a>предварительным требованиям
1. Создайте учетную запись хранения общего назначения, следуя указаниям в [этом разделе](../../storage/common/storage-create-storage-account.md#create-a-storage-account).

2. Создайте кластер Spark в HDInsight, следуя инструкциям в [этом руководстве](../../hdinsight/spark/apache-spark-jupyter-spark-sql.md). Свяжите учетную запись хранения, созданную на шаге 1, с этим кластером.

3. Скачайте и просмотрите файл сценария Python **test.py**, расположенный по адресу [https://adftutorialfiles.blob.core.windows.net/sparktutorial/test.py](https://adftutorialfiles.blob.core.windows.net/sparktutorial/test.py).

4. Отправьте файл **test.py** в папку **pyFiles** контейнера **adfspark** в хранилище BLOB-объектов. Создайте контейнер и папку, если их нет.

### <a name="create-a-data-factory"></a>Создание фабрики данных
Чтобы создать фабрику данных, сделайте следующее:

1. Войдите на [портале Azure](https://portal.azure.com/).

2. Выберите **Создать** > **Данные и аналитика** > **Фабрика данных**.

3. В колонке **Новая фабрика данных** в поле **Имя** введите **SparkDF**.

   > [!IMPORTANT]
   > Имя фабрики данных Azure должно быть глобально уникальным. Если появится сообщение об ошибке "Имя SparkDF фабрики данных недоступно", измените имя фабрики данных. Например, используйте "ваше_имя_SparkDFdate" и снова создайте фабрику данных. Дополнительные сведения о правилах именования в фабрике данных Azure см. в [этой статье](data-factory-naming-rules.md).

4. В разделе **Подписка** выберите подписку Azure, в рамках которой необходимо создать фабрику данных.

5. Выберите имеющуюся группу ресурсов Azure или создайте новую.

6. Установите флажок **Закрепить на панели мониторинга**.

7. Нажмите кнопку **Создать**.

   > [!IMPORTANT]
   > Создавать экземпляры фабрики данных может пользователь с ролью [Участник Data Factory](../../role-based-access-control/built-in-roles.md#data-factory-contributor) на уровне подписки или группы ресурсов.

8. Созданная фабрика данных появится на панели мониторинга портала Azure.

9. После успешного создания фабрики данных ее содержимое отобразится на странице **Фабрика данных**. Если страница **Фабрика данных** не отображается, щелкните плитку вашей фабрики данных на панели мониторинга.

    ![Колонка "Фабрика данных"](./media/data-factory-spark/data-factory-blade.png)

### <a name="create-linked-services"></a>Создание связанных служб
На этом шаге создайте две связанные службы. Одна служба связывает кластер Spark с фабрикой данных, а другая связывает хранилище с фабрикой данных. 

#### <a name="create-a-storage-linked-service"></a>Создание связанной службы хранилища
На этом шаге вы свяжете учетную запись хранения с фабрикой данных. Набор данных, который вы создадите на следующем шаге этого пошагового руководства, относится к этой связанной службе, как и связанная служба HDInsight, которую вы определите на следующем шаге. 

1. В колонке **Фабрика данных** щелкните **Создать и развернуть**. Отобразится редактор фабрики данных.

2. Выберите **Новое хранилище данных** и **Служба хранилища Azure**.

   ![Новое хранилище данных](./media/data-factory-spark/new-data-store-azure-storage-menu.png)

3. Скрипт JSON, используемый для создания связанной службы хранилища, отобразится в редакторе.

   ![AzureStorageLinkedService](./media/data-factory-build-your-first-pipeline-using-editor/azure-storage-linked-service.png)

4. Замените **имя учетной записи** и **ключ учетной записи** значениями имени и ключа учетной записи хранения. Сведения о получении, просмотре, копировании и повторном создании ключей доступа к хранилищу см. в разделе [Управление учетной записью хранения](../../storage/common/storage-create-storage-account.md#manage-your-storage-account).

5. Чтобы развернуть связанную службу, выберите **Развертывание** на панели команд. После успешного развертывания связанной службы окно Draft-1 исчезнет. Экземпляр **AzureStorageLinkedService** отобразится в представлении в виде дерева слева.

#### <a name="create-an-hdinsight-linked-service"></a>Создание связанной службы HDInsight
На этом шаге вы создадите связанную службу HDInsight, чтобы связать кластер HDInsight Spark с фабрикой данных. В этом примере кластер HDInsight используется для выполнения программы Spark, указанной в действии Spark конвейера. 

1. В редакторе фабрики данных выберите **More** (Дополнительно) > **Новое вычисление** > **Кластер HDInsight**.

    ![Создание связанной службы HDInsight](media/data-factory-spark/new-hdinsight-linked-service.png)

2. Вставьте следующий фрагмент в окно Draft-1. В редакторе JSON выполните следующие действия.

    a. Укажите URI для кластера HDInsight Spark. Например, `https://<sparkclustername>.azurehdinsight.net/`.

    Б. Укажите имя пользователя, имеющего доступ к кластеру Spark.

    c. Укажите пароль для пользователя.

    d. Укажите связанную службу хранилища, которая связана с кластером HDInsight Spark. (В этом примере это AzureStorageLinkedService.)

    ```json
    {
        "name": "HDInsightLinkedService",
        "properties": {
            "type": "HDInsight",
            "typeProperties": {
                "clusterUri": "https://<sparkclustername>.azurehdinsight.net/",
                "userName": "admin",
                "password": "**********",
                "linkedServiceName": "AzureStorageLinkedService"
            }
        }
    }
    ```

    > [!IMPORTANT]
    > - Действие Spark не поддерживает кластеры HDInsight Spark, использующие Azure Data Lake Store в качестве основного хранилища.
    > - Оно поддерживает только имеющиеся (собственные) кластеры HDInsight Spark. Связанная служба HDInsight по запросу не поддерживается.

    Подробные сведения о связанной службе HDInsight см. в разделе [Связанная служба Azure HDInsight](data-factory-compute-linked-services.md#azure-hdinsight-linked-service).

3. Чтобы развернуть связанную службу, выберите **Развертывание** на панели команд. 

### <a name="create-the-output-dataset"></a>Создание выходного набора данных
На основе этого набора настраивается расписание (ежечасно, ежедневно). Исходя из сказанного выше, вы должны определить выходной набор данных для действия Spark в конвейере, даже если действие не создает выходные данные. Указание входного набора данных для действия необязательно.

1. В редакторе фабрики данных выберите **More** (Дополнительно) > **Новый набор данных** > **Хранилище BLOB-объектов Azure**.

2. Вставьте следующий фрагмент в окно Draft-1. Этот фрагмент кода JSON определяет набор данных с именем **OutputDataset**. Кроме того, нужно указать, что результаты будут храниться в контейнере больших двоичных объектов **adfspark** и в папке **pyFiles/output**. Как упоминалось ранее, это фиктивный набор данных. В этом примере программа Spark не создает никаких выходных данных. В разделе **availability** указывается частота, с которой ежедневно будет создаваться выходной набор данных. 

    ```json
    {
        "name": "OutputDataset",
        "properties": {
            "type": "AzureBlob",
            "linkedServiceName": "AzureStorageLinkedService",
            "typeProperties": {
                "fileName": "sparkoutput.txt",
                "folderPath": "adfspark/pyFiles/output",
                "format": {
                    "type": "TextFormat",
                    "columnDelimiter": "\t"
                }
            },
            "availability": {
                "frequency": "Day",
                "interval": 1
            }
        }
    }
    ```
3. Чтобы развернуть набор данных, щелкните **Развертывание** на панели команд.


### <a name="create-a-pipeline"></a>Создание конвейера
На этом шаге создается конвейер с действием HDInsightSpark. Сейчас расписание активируется с помощью выходного набора данных, поэтому его необходимо создать, даже если действие не создает никаких выходных данных. Если действие не принимает никаких входных данных, входной набор данных можно не создавать. Таким образом, в этом примере входной набор данных не указывается.

1. В редакторе фабрики данных выберите **More** (Дополнительно) > **Новый конвейер**.

2. Замените скрипт в окне Draft-1 следующим скриптом:

    ```json
    {
        "name": "SparkPipeline",
        "properties": {
            "activities": [
                {
                    "type": "HDInsightSpark",
                    "typeProperties": {
                        "rootPath": "adfspark\\pyFiles",
                        "entryFilePath": "test.py",
                        "getDebugInfo": "Always"
                    },
                    "outputs": [
                        {
                            "name": "OutputDataset"
                        }
                    ],
                    "name": "MySparkActivity",
                    "linkedServiceName": "HDInsightLinkedService"
                }
            ],
            "start": "2017-02-05T00:00:00Z",
            "end": "2017-02-06T00:00:00Z"
        }
    }
    ```
    Обратите внимание на следующие моменты.

    a. Свойству **type** присваивается значение **HDInsightSpark**.

    Б. Свойству **rootPath** присваивается значение **adfspark\\pyFiles**, где adfspark — контейнер больших двоичных объектов, а pyFiles — папка с файлами в этом контейнере. В этом примере хранилище BLOB-объектов связано с кластером Spark. Файл можно отправить в другую учетную запись хранения. Чтобы сделать это, создайте связанную службу хранилища, которая свяжет эту учетную запись хранения с фабрикой данных. Затем укажите имя связанной службы в качестве значения свойства **sparkJobLinkedService**. Сведения об этом и других свойствах, поддерживаемых действием Spark, см. в разделе [Свойства действия Spark](#spark-activity-properties).

    c. Свойству **entryFilePath** присваивается значение **test.py**, которое является файлом Python.

    d. Свойству **getDebugInfo** присваивается значение **Always**. Так файлы журналов будут создаваться постоянно (успешные и неудачные события).

    > [!IMPORTANT]
    > Пока вы не устраните неполадки, советуем не устанавливать для этого свойства значение `Always` в рабочей среде.

    д. В разделе **outputs** содержится один выходной набор данных. Вы должны определить выходной набор данных, даже если программа Spark не выдает выходные данные. На основе этого набора настраивается расписание конвейера (ежечасно, ежедневно). 

    Сведения о свойствах, поддерживаемых действием Spark, см. в разделе [Свойства действия Spark](#spark-activity-properties).

3. Чтобы развернуть конвейер, щелкните **Развертывание** на панели команд.

### <a name="monitor-a-pipeline"></a>Мониторинг конвейера
1. В колонке **Фабрика данных** выберите **Мониторинг и управление**, чтобы запустить мониторинг приложения на другой вкладке.

    ![Плитка Monitor & Manage (Мониторинг и управление)](media/data-factory-spark/monitor-and-manage-tile.png)

2. Вверху укажите для фильтра **Start time** (Время начала) дату **01.02.2017** и щелкните **Применить**.

3. Отобразится только одно окно действия, так как между временем начала (01.02.2017) и окончания (02.02.2017) конвейера всего один день. Убедитесь, что срез данных находится в состоянии **Готово**.

    ![Мониторинг конвейера](media/data-factory-spark/monitor-and-manage-app.png)

4. Выберите выполнение действия в списке **Activity windows** (Окна действий), чтобы просмотреть сведения о нем. В случае ошибки в области справа будут отображаться детали.

### <a name="verify-the-results"></a>Проверка результатов

1. Запустите записную книжку Jupyter для кластера HDInsight Spark, перейдя на [этот веб-сайт](https://CLUSTERNAME.azurehdinsight.net/jupyter). Можно также сначала открыть панель мониторинга для кластера HDInsight Spark, а затем запустить записную книжку Jupyter.

2. Щелкните **New** (Создать) > **PySpark**, чтобы создать записную книжку.

    ![Новая записная книжка Jupyter](media/data-factory-spark/jupyter-new-book.png)

3. Выполните следующую команду, скопировав и вставив текст и нажав SHIFT+ENTER в конце второй инструкции.

    ```sql
    %%sql

    SELECT buildingID, (targettemp - actualtemp) AS temp_diff, date FROM hvac WHERE date = \"6/1/13\"
    ```
4. Убедитесь, что вы видите данные из таблицы hvac. 

    ![Результаты запроса Jupyter](media/data-factory-spark/jupyter-notebook-results.png)

<!-- Removed bookmark #run-a-hive-query-using-spark-sql since it doesn't exist in the target article -->
Подробные инструкции см. в разделе о [выполнении SQL-запроса Spark](../../hdinsight/spark/apache-spark-jupyter-spark-sql.md). 

### <a name="troubleshooting"></a>Устранение неполадок
Так как вы задали для getDebugInfo значение **Always**, вы увидите вложенную папку log в папке pyFiles в контейнере больших двоичных объектов. В файле журнала в папке log содержится дополнительная информация. Этот файл журнала особенно полезен в случае возникновения ошибки. В рабочей среде вы можете настроить состояние ошибки **Failure**.

Для дальнейшего устранения неполадок сделайте следующее:


1. Перейдите на сайт `https://<CLUSTERNAME>.azurehdinsight.net/yarnui/hn/cluster`.

    ![Приложение пользовательского интерфейса YARN](media/data-factory-spark/yarnui-application.png)

2. Щелкните **Logs** (Журналы) для одной из попыток выполнения.

    ![Страница приложения](media/data-factory-spark/yarn-applications.png)

3. Отобразятся дополнительные сведения об ошибке на странице журнала.

    ![Описание ошибки в журнале](media/data-factory-spark/yarnui-application-error.png)

В следующих разделах приведены сведения о сущностях фабрики данных, необходимых для использования кластера Spark и действия Spark в фабрике данных.

## <a name="spark-activity-properties"></a>Свойства действия Spark
Ниже приведен пример определения JSON конвейера с действием Spark. 

```json
{
    "name": "SparkPipeline",
    "properties": {
        "activities": [
            {
                "type": "HDInsightSpark",
                "typeProperties": {
                    "rootPath": "adfspark\\pyFiles",
                    "entryFilePath": "test.py",
                    "arguments": [ "arg1", "arg2" ],
                    "sparkConfig": {
                        "spark.python.worker.memory": "512m"
                    }
                    "getDebugInfo": "Always"
                },
                "outputs": [
                    {
                        "name": "OutputDataset"
                    }
                ],
                "name": "MySparkActivity",
                "description": "This activity invokes the Spark program",
                "linkedServiceName": "HDInsightLinkedService"
            }
        ],
        "start": "2017-02-01T00:00:00Z",
        "end": "2017-02-02T00:00:00Z"
    }
}
```

В следующей таблице приведено описание свойств, используемых в определении JSON.

| Свойство | ОПИСАНИЕ | Обязательно |
| -------- | ----------- | -------- |
| name | Имя действия в конвейере. | Yes |
| description | Описание действия. | Нет  |
| Тип | Для этого свойства необходимо задать значение HDInsightSpark. | Yes |
| linkedServiceName | Имя связанной службы HDInsight, в которой выполняется программа Spark. | Yes |
| rootPath | Контейнер больших двоичных объектов и папка, содержащая файл Spark. Имя файла чувствительно к регистру. | Yes |
| entryFilePath | Относительный путь к корневой папке пакета и кода Spark. | Yes |
| className | Основной класс Java или Spark приложения. | Нет  |
| arguments | Список аргументов командной строки для программы Spark. | Нет  |
| proxyUser | Учетная запись пользователя для олицетворения, используемая для выполнения программы Spark. | Нет  |
| sparkConfig | Укажите значения для свойств конфигурации Spark, перечисленных в разделе [свойств приложения документа о конфигурации Spark](https://spark.apache.org/docs/latest/configuration.html#available-properties). | Нет  |
| getDebugInfo | Указывает, когда файлы журнала Spark копируются в хранилище, используемое кластером HDInsight или определенное sparkJobLinkedService. Допустимые значения: None, Always или Failure. По умолчанию используется None. | Нет  |
| sparkJobLinkedService | Связанная служба хранилища, в которой хранятся файл задания Spark, зависимости и журналы. Если значение этого свойства не указано, используется хранилище, связанное с кластером HDInsight. | Нет  |

## <a name="folder-structure"></a>Структура папок
Действие Spark не поддерживает встроенный сценарий, в отличие от действий Pig и Hive. Задания Spark также обеспечивают большую гибкость, чем задания Pig и Hive. Для заданий Spark можно указать несколько зависимостей, например пакеты JAR (размещаются в CLASSPATH Java), файлы Python (размещаются в PYTHONPATH) и другие файлы.

Создайте следующую структуру папок в хранилище BLOB-объектов, на которое ссылается связанная служба HDInsight. Затем передайте зависимые файлы в соответствующие вложенные папки в корневой папке, определенной значением **entryFilePath**. Например, передайте файлы Python во вложенную папку pyFiles, а JAR-файлы — во вложенную папку jars, расположенную в корневой папке. Во время выполнения служба фабрики данных ожидает в хранилище BLOB-объектов следующую структуру папок. 

| Путь | ОПИСАНИЕ | Обязательно | type |
| ---- | ----------- | -------- | ---- |
| . | Путь к корневому каталогу задания Spark в хранилище связанной службы. | Yes | Папка |
| &lt;Определяется пользователем&gt; | Путь к файлу записи задания Spark. | Yes | Файл |
| ./jars | Все файлы в этой папке передаются и помещаются в папку classpath Java для кластера. | Нет  | Папка |
| ./pyFiles | Все файлы в этой папке передаются и помещаются в папку PYTHONPATH для кластера. | Нет  | Папка |
| ./files | Все файлы в этой папке передаются и помещаются в рабочий каталог исполнителя. | Нет  | Папка |
| ./archives | Все файлы в этой папке не сжаты. | Нет  | Папка |
| ./logs | Папка, в которой хранятся журналы из кластера Spark.| Нет  | Папка |

Ниже приведен пример хранилища, содержащего два файла заданий Spark в хранилище BLOB-объектов, на которое ссылается связанная служба HDInsight.

```
SparkJob1
    main.jar
    files
        input1.txt
        input2.txt
    jars
        package1.jar
        package2.jar
    logs

SparkJob2
    main.py
    pyFiles
        scrip1.py
        script2.py
    logs
```
